[
 {
  "content": "Microsoftが開発しているGUIテキストエディタ。 Atom を継ぐもの。\nhttps://code.visualstudio.com\nKeyboard Shortcuts https://code.visualstudio.com/docs/getstarted/keybindings\nkey command description ⇧⌘p Command Palette これさえ覚えれば ⌥⌘p Open Git Projects GPMを使ってディレクトリを開く ⌘p Search files by name プロジェクト内の別ファイルを開く ⇧⌘f Find in Files プロジェクト内をファイル横断で検索 ⇧⌘e Show Explorer プロジェクト内のファイル一覧 ⇧⌘x Show Extensions 拡張一覧。インストールもここから ⌘/ Toggle Line Comment コメントアウトしたり外したり ^` Toggle Terminal ^⏎ Run Selected Text エディタからターミナルにテキストを送る(要設定) ⌥↓ Move line down ⇧⌥↓ Copy line down ⌥⌘↓ cursorColumnSelectDown 矩形(ブロック)選択 ⌘k⌘s Keyboard Shortcuts 一覧 ⌘k⌘r Keyboard Shortcuts Ref. 抜粋PDF 設定ファイルは ~/.config/Code/User/keybindings.json\n{ \u0026#34;key\u0026#34;: \u0026#34;ctrl+enter\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;workbench.action.terminal.runSelectedText\u0026#34;, \u0026#34;when\u0026#34;: \u0026#34;editorTextFocus \u0026amp;\u0026amp; resourceExtname != .ipynb\u0026#34; }, 環境設定 https://code.visualstudio.com/docs/getstarted/settings\nいつもの ⌘, キーで設定画面を起動。\n設定ファイルは ~/.config/Code/User/settings.json\n拡張パッケージ https://code.visualstudio.com/docs/editor/extension-marketplace\nR https://code.visualstudio.com/docs/languages/r\n人にはRStudioを薦めるけど個人的にはこっちのほうが使いやすい。\nlanguageserver をインストールする: install.packages(\u0026#34;languageserver\u0026#34;) R extension for VSCode をインストール。 R session watcher の設定。 VSCodeの設定に \u0026quot;r.sessionWatcher\u0026quot;: true を追加して再起動。 \u0026ldquo;Create R terminal\u0026rdquo; コマンドで専用コンソールを立ち上げる場合はこれだけ。 Integrated Terminalから手動でRを立ち上げるような場合、 設定ファイルを明示的に読み込む必要があるので .Rprofile に追記: if (interactive() \u0026amp;\u0026amp; Sys.getenv(\u0026#34;TERM_PROGRAM\u0026#34;) == \u0026#34;vscode\u0026#34;) { source(\u0026#34;~/.vscode-R/init.R\u0026#34;) } tmux が TERM_PROGRAM を上書きすることに注意。 ここまでやれば View() や help() などをVSCodeで表示できる。 以下はお好みで。\nVSCodeの設定 \u0026quot;r.rmarkdown.enableCodeLens\u0026quot;: false: CodeLensは邪魔。 r.session.viewers.viewColumn: interactive viewers をどこで表示するか。 Active: 既存editor group内の新規タブ。 Two, Beside: 新規editor groupを作って表示。2つの違いは不明。 左右ではなく上下に分割して表示させるには \u0026quot;workbench.editor.openSideBySideDirection\u0026quot;: \u0026quot;down\u0026quot; 。 See vscode-R#1126. Disable: ネイティブに任せる。 デフォルト: {plot: Two, browser: Active, viewer: Two, pageViewer: Active, view: Two, helpPanel: Two} Rの設定。 ほとんどの項目はVSCode側で設定できる。 それらをどうしてもRから上書きしたいときに使う。 図をどうやって描画するか: デフォルト: png() で書き出してVSCode内に表示。 raggを使うオプションが欲しいところだけど無い。 描画領域のサイズ変化に追従する機能なども無い。 httpgd を使ってVSCode内に表示。 install.packages(\u0026quot;httpgd\u0026quot;) した上で VSCodeの設定に \u0026quot;r.plot.useHttpgd\u0026quot;: true を追加。 SVG形式なので要素数が多くなるほど急激に重くなり、例えばdiamondsの散布図でも使い物にならない。 PNGも選べるようにunigdを開発中らしい。 外部ターミナルでRを起動するのと同じように独立XQuartzなどで表示。 上記 viewColumn を \u0026quot;plot\u0026quot;: \u0026quot;Disable\u0026quot; に設定する。 IPython のようにコンソール上での補完や色付けなどを強化したければ radian を入れる。 設定 はRコードの形で ~/.config/radian/profile に書く。 Python https://code.visualstudio.com/docs/languages/python\nMicrosoft公式パッケージが提供されていて、がっちりサポートされている感じ。\nPython とりあえずこれを入れれば下記の依存パッケージも自動で入るはず。 Pylance 補完や型チェックなどを担う language server. 中では Pyright が使われている。 警告を非表示にするには次のように書く: # pyright: reportUnusedVariable=false 設定項目: https://github.com/microsoft/pyright/blob/main/docs/configuration.md\nJupyter Jupyter notebook support Glossary IntelliSense Microsoft製の補完システム。 各種 language server から情報を受け取る。 CodeLens 補助的な情報を表示する機能。 ファイルの内容と区別しにくい形で挿入されてすごく邪魔なので確実に切る。 Integrated Terminal VSCode内で開けるターミナル。^` シェルを介さず直接 tmux を起動する設定も可能。 ",
  "href": "/dev/vscode.html",
  "tags": [
   "editor",
   "writing"
  ],
  "title": "VSCode",
  "type": "dev"
 },
 {
  "content": "https://github.com/gollum/gollum\n研究室内の連絡・情報共有は、フロー型の情報ならSlackで、ストック型の情報ならWikiで。 という方針になったので、学内ネットワークで閲覧編集可能な自前Wikiサーバーを立てる。 開発環境は macOS, 本番環境は Ubuntu 18.04 → 20.04 → 22.04 LTS.\nソフトウェア選定 pukiwiki 学生のときの研究室で使ってたので馴染み深い。 でも独自記法だしphpとか文字コードとか考えたくないので却下。 crowi Node.js + MongoDB で動くモダンな Markdown wiki。 生のファイルが見えないデータベースっぽいので管理が難しそう。 growi はこれをフォークしたもので、 機能もドキュメントも強化されてるし、 docker-compose とかですぐ使えるのも楽ちん。 日本語の人しか使わなそう\u0026hellip;? gitit pandoc + git で動くのでかなり手堅い感じ。 Haskell の勉強を兼ねていじくり回す時間があれば\u0026hellip; gollum Ruby + git で動く Markdown wiki。 GitHubやGitLabのWikiにも採用されているのでコミュニティが大きそう。 自前サーバーを管理できる人が抜けても内部データを簡単に再利用可能。 Rubyはよく知らないけど理解しなくても雰囲気でいじれそう。 Hugo 静的ウェブサイトを作る用途には最高だけどWiki機能は無い。 Netlify CMS でWiki-likeなガワを取り付けることは可能だけど、 編集内容のpush先がGitHubとかになるので、 それを学内サーバーに即時反映させるのが難しい。 gollumインストールとWiki新規作成 rbenvを設定してRubyを入れる。 MacならHomebrewでもいいけどLinuxではHomebrewを混ぜるとエラーになりがちなので避け、 管理者たちが出入りできるところに RBENV_ROOT を置く:\nsudo apt install autoconf patch build-essential rustc libssl-dev libyaml-dev libreadline6-dev zlib1g-dev libgmp-dev libncurses5-dev libffi-dev libgdbm6 libgdbm-dev libdb-dev uuid-dev export RBENV_ROOT=/home/local/.rbenv PATH=\u0026#34;${PATH}:${RBENV_ROOT}/bin\u0026#34; git clone https://github.com/rbenv/rbenv.git $RBENV_ROOT git clone https://github.com/rbenv/ruby-build.git ${RBENV_ROOT}/plugins/ruby-build rbenv init eval \u0026#34;$(rbenv init -)\u0026#34; rbenv install -l rbenv install 3.1.4 eval \u0026quot;$(rbenv init -)\u0026quot; はここでインストールした ruby や bundle にPATHを通すコマンド。 新しいシェルを起動するたびに実行する必要があるので .zshrc, .bashrc 等の設定ファイルに記述しておく。\nrbenv global 3.1.4 とするか、 次に作るWikiリポジトリ内で rbenv local 3.1.4 とすることで使用するRubyのバージョンを設定する。\nWiki用のリポジトリ(ここではlabwiki)を作成して空コミット:\ngit init labwiki cd labwiki/ git commit --allow-empty -m \u0026#34;:beer: Create repository\u0026#34; Gemfile を作成してコミット:\nsource \u0026#39;https://rubygems.org\u0026#39; gem \u0026#39;commonmarker\u0026#39; gem \u0026#39;gollum\u0026#39; gollum本体をいろいろいじくる場合は自分のフォークを使う:\ngem \u0026#39;gollum-rugged_adapter\u0026#39;, :github =\u0026gt; \u0026#39;heavywatal/rugged_adapter\u0026#39;, :branch =\u0026gt; \u0026#39;custom\u0026#39; gem \u0026#39;gollum-lib\u0026#39;, :github =\u0026gt; \u0026#39;heavywatal/gollum-lib\u0026#39;, :branch =\u0026gt; \u0026#39;custom\u0026#39; gem \u0026#39;gollum\u0026#39;, :github =\u0026gt; \u0026#39;heavywatal/gollum\u0026#39;, :branch =\u0026gt; \u0026#39;custom\u0026#39; 開発環境ではローカルのクローンを使うように設定:\nSRCDIR=${HOME}/fork git clone https://github.com/heavywatal/gollum.git ${SRCDIR}/gollum -b custom git clone https://github.com/heavywatal/gollum-lib.git ${SRCDIR}/gollum-lib -b custom git clone https://github.com/heavywatal/rugged_adapter.git ${SRCDIR}/rugged_adapter -b custom bundle config local.gollum ${SRCDIR}/gollum bundle config local.gollum-lib ${SRCDIR}/gollum-lib bundle config local.gollum-rugged_adapter ${SRCDIR}/rugged_adapter bundle install で gollum 及び依存パッケージをまとめてインストール。 --local を付けてこのプロジェクト専用にしてもよい。\nbundle exec gollum でとりあえず走らせる。 手元のコンピュータなら http://localhost:4567 で確認。\n設定 基本 config.rb を作成して bundle exec gollum -c config.rb のように指定して読ませる。\nrequire \u0026#39;gollum/app\u0026#39; wiki_options = { page_file_dir: \u0026#39;source\u0026#39;, css: true, mathjax: false, emoji: true } Precious::App.set(:wiki_options, wiki_options) 例えばこの場合、 ページのソースファイルはリポジトリのルートではなく source/ から読まれるようになる。\ncss: true により custom.css を読み込まれるようになるが、 残念ながらリポジトリルートではなく source/custom.css に置かなければならない。 また、ローカルファイルではなくコミット済みのものが読まれることに注意。\nなぜかここからは設定できずコマンドからのみ設定可能な項目もある。\ne.g., -b/--base-path, --allow-uploads page.\nポート番号なしでアクセスする デフォルトでは http://example.com:4567 のように4567番ポートのルートで動く。 これを80番ポートの/wiki以下で動くように調整して http://example.com/wiki/ のようにアクセスできるようにする。\nApacheの設定ファイルを新規作成:\nsudo vim /etc/apache2/sites-available/gollum-wiki.conf ProxyRequests Off \u0026lt;Proxy *\u0026gt; Order deny,allow Allow from all \u0026lt;/Proxy\u0026gt; \u0026lt;Location /wiki\u0026gt; ProxyPass http://localhost:4567/wiki ProxyPassReverse http://localhost:4567/wiki \u0026lt;/Location\u0026gt; その設定ファイルを有効化してApache再起動:\nsudo a2ensite gollum-wiki.conf sudo systemctl restart apache2 bundle exec gollum -b /wiki で起動。\nconfig.rb のほうで Precious::App に base_path: '/wiki' を渡してもなぜか効かない。\nBASIC認証でやんわりパスワードをかける config.rb にこんな感じで書くだけ:\nmodule Precious class App \u0026lt; Sinatra::Base use Rack::Auth::Basic, \u0026#39;Private Wiki\u0026#39; do |username, password| users = File.open(File.expand_path(\u0026#39;users.json\u0026#39;, __dir__)) do |file| JSON.parse(file.read, symbolize_names: true) end name = username.to_sym digested = Digest::SHA256.hexdigest(password) if users.key?(name) \u0026amp;\u0026amp; digested == users[name][:password] Precious::App.set(:author, users[name]) end end before do session[\u0026#39;gollum.author\u0026#39;] = settings.author end end end session['gollum.author'] にハッシュを渡しておくとコミッターに反映してもらえる。 ユーザー情報は別ファイル(ここではusers.json)に分離しといたほうが見通しがいい。\n{ \u0026#34;user1\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;First User\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;user1@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;0b14d501a594442a01c6859541bcb3e8164d183d32937b851835442f69d5c94e\u0026#34; }, \u0026#34;user2\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Second User\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;user2@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;6cf615d5bcaac778352a8f1f3360d23f02f34ec182e259897fd6ce485d7870d4\u0026#34; } } もっとちゃんとした認証システムにしたほうがいいのかもしれないけど、 大学のファイアウォール内なのでとりあえずこれくらいで\u0026hellip;\nパスワードのハッシュ値は sha256sum \u0026lt;(pbpaste) とか echo -n 'greatpassword' | sha256sum のようなコマンドで計算できる。\nMarkdownパーサー/レンダラを変更する https://github.com/gollum/gollum/wiki/Custom-rendering-gems\nMarkdownを読んでHTMLに変換するライブラリは github-markup を通して選択できるようになっている。 デフォルトでは kramdown が利用されるらしいが、 なるべくCommonMark/GFM準拠で高速なのが良いので、 commonmarker を使うことにする。 例によって config.rb に追記:\nmodule Gollum class Markup GitHub::Markup::Markdown::MARKDOWN_GEMS.clear GitHub::Markup::Markdown::MARKDOWN_GEMS[\u0026#39;commonmarker\u0026#39;] = proc do |content| exts = %i[ table tasklist strikethrough autolink ] parse_opts = %i[ UNSAFE SMART ] render_opts = %i[ UNSAFE GITHUB_PRE_LANG ] doc = CommonMarker.render_doc(content, parse_opts, exts) doc.to_html(render_opts) end end end Gollum::Markup.formats.select! { |k, _| k == :markdown } ほかにどんなのが利用可能かはこちらを参照: https://github.com/github/markup/blob/master/lib/github/markup/markdown.rb\nsystemd で自動的に開始 sudo vim /etc/systemd/system/gollum.service [Unit] Description=Gollum wiki server After=network.target [Service] Type=simple User=YOURNAME WorkingDirectory=/path/to/your/labwiki ExecStart=bundle exec gollum -c config.rb -b /wiki --allow-uploads dir Restart=on-abort StandardOutput=file:/var/log/gollum.log StandardError=file:/var/log/gollum.log [Install] WantedBy=multi-user.target sudo systemctl start gollum.service sudo systemctl enable gollum.service ",
  "href": "/misc/gollum.html",
  "tags": [
   "writing"
  ],
  "title": "Gollum",
  "type": "misc"
 },
 {
  "content": "2019-10-30 東北大学 生命科学研究科 進化ゲノミクス分野 牧野研\n前半スライド\nEnvironment / 環境 OS 新しめのUNIX系OSが好ましい。\nmacOS 10.14 Mojave以降 Command Line Tools Gitは標準装備のやつで大丈夫 Linux Ubuntu 16.04以降 or CentOS 7以降 build-essential とか \u0026ldquo;Development Tools\u0026rdquo; 的なもの 普通にaptやyumで入るGitが古いv1系だったら、どうにかしてv2以降を入れる Windows 10 (1803以降)\nGit本体およびGit Bashを公式から入れる。 インストール途中でいろいろ選択肢あるけどとりあえずデフォルトでよいのでは。\nRStudioのターミナルのシェルとしてGit Bashを設定。 \u0026ldquo;Tools \u0026gt; Global Options \u0026gt; Terminal \u0026gt; Shell\u0026rdquo;\nRStudioで新しいTerminalを立ち上げて最低限のコマンド操作に慣れる。 e.g., pwd, ls, cd, mkdir\nGit Bashにおける HOME がRStudioにおけるそれと異なることに注意。\n### Git Bash echo $HOME # /c/Users/watal echo ~ # /c/Users/watal ### R normalizePath(\u0026#34;~\u0026#34;) # \u0026#34;C:\\\\Users\\\\watal\\\\Documents\u0026#34; UNIX派が仕方なくWindowsを使う場合は MSYS2の pacman で環境を構築するのが良さそう\u0026hellip;?\nCommon 適当なテキストエディタ(開発環境)を入れておく。 初期状態でもGit/GitHubとの連携機能が付いていて、 変更箇所を色付けしてくれたりコマンド入力を肩代わりしてくれたりするのが便利。\nAtom: GitHub製 VSCode: Microsoft製 RStudio: RStudio製 GitHubに個人アカウントを作る。\nGitの初期設定をターミナルから行う:\ngit --version # 2.0以上であることを確認 git config --global user.name \u0026#34;Watal M. Iwasaki\u0026#34; git config --global user.email \u0026#34;heavywatalあmail.com\u0026#34; git config --global push.default simple cat ~/.gitconfig [user] name = Watal M. Iwasaki email = heavywatalあmail.com [push] default = simple Enable macOS Keychain to skip password authentication:\ngit config --global credential.helper osxkeychain SSH (任意) GitHubとの通信に2つの方式がある。\nHTTPS: 設定不要で高速だが、操作によってパスワード入力が必要 SSH: 一旦ちゃんと設定すればパスワードなしで快適 ダウンロード操作(clone/fetch/pull)は高速なHTTPSで、\nアップロード操作(push)はパスワード無しのSSHで、というのが楽ちん。\nSSH公開鍵を作ってローカルマシンとGitHubに登録する。\n設定ファイル ~/.gitconfig に pushinsteadof の設定を追加:\n[url \u0026#34;git@github.com:\u0026#34;] pushinsteadof = https://github.com/ Essential commands / 基本操作 Fetch existing repositories: clone GitHub上の適当なリポジトリをひとつ選ぶ。 (e.g., https://github.com/heavywatal/tumopp)\n右の方の緑の \u0026ldquo;Clone or download\u0026rdquo; ボタンを押す。\nSSHではなくHTTPSを選択し、URLをコピー。\nターミナルにコマンドを入力:\ngit clone https://github.com/heavywatal/tumopp.git\n中身を眺めてみる:\ncd tumopp/ ls -al git log git remote -v 最新版のスナップショットだけでなく、 履歴もごっそり複製するので、 このあとはオフラインでもいろいろ操作できる。\nCreate new repositories: init GitHubの右上の \u0026ldquo;+\u0026rdquo; から \u0026ldquo;New repository\u0026rdquo; を選択。\nRepository name を例えば helloworld として \u0026ldquo;Create repository\u0026rdquo; を押す。 いくつかのファイル (README.md, LICENSE, .gitignore) をここで作ることもできるけど、今回はとりあえず空っぽのリポジトリを作る。\n手元のマシンにローカルリポジトリを作る:\nmkdir helloworld cd helloworld/ git init ls -al リポジトリの本体 .git/ が作成されたことを確認。\n空っぽのコミットを作る:\ngit status git commit --allow-empty -m \u0026#34;:beer: Create repository\u0026#34; git status git log 事あるごとに git status や git log を確認すると安心。\n先程作ったリモートリポジトリを紐付けて、pushしてみる:\ngit remote -v git remote add origin https://github.com/YOUR_NAME/helloworld.git git remote -v git push -u origin master git status GitHubで履歴を閲覧し、 git log と同じになってることを確認。\nExport local changes to a remote server: push 上で作ったリポジトリに、適当なファイルを追加:\necho \u0026#39;# Hello, world\u0026#39; \u0026gt; README.md cat README.md git status 作ったファイルをstaging areaに追加:\ngit add README.md git status git diff --staged この変更をcommit:\ngit commit -m \u0026#34;:memo: Create README.md\u0026#34; git status git log git show リモートにpush:\ngit push git status git log Import changes from a remote server: fetch 上のリポジトリでそのまま git fetch してみる。 ローカルとリモートは同じ状態なので当然何も起こらない。\n練習のためGitHub上で LICENSE ファイルを作成する。\nGitHub上のリポジトリのトップページを開き \u0026ldquo;Create new file\u0026rdquo; ボタンを押す。 ファイル名に LICENSE と入力。 右に現れる \u0026ldquo;Choose a license template\u0026rdquo; というボタンを押す。 とりあえず \u0026ldquo;MIT License\u0026rdquo; を選択。 YearとNameを適当に埋めて \u0026ldquo;Review and submit\u0026rdquo;。 \u0026ldquo;Commit directly to the master branch\u0026rdquo; を選択して \u0026ldquo;Commit new file\u0026rdquo; その変更をローカルリポジトリに取り寄せる:\ngit fetch git status git log --all ls -al リポジトリ内部 .git/ の origin/master は更新されたが、 working directoryにはまだ反映されていない。\norigin/master の内容を手元のファイルに反映する:\ngit merge git status git log git show ls -al git fetch と git merge を一気にやってくれる git pull というコマンドもあり、 普段の一人作業ではよく使う。\nOther commands diff 差分を表示:\n# HEAD vs working (staging前のファイルが対象) git diff # HEAD vs index (staging済みcommit前のファイルが対象) git diff --staged # HEAD vs working+index (commit前の全ファイルが対象) git diff HEAD # 特定コミットの変更点 (diffじゃない...) git show [revision] rm, clean tracking対象から外して忘れさせる(手元のファイルはそのまま):\ngit rm --cached \u0026lt;file\u0026gt; .gitignore で無視されてるuntrackedファイルを消す:\ngit clean -fdX 無視されていないuntrackedファイルも消したい場合は小文字の -fdx (危険)。\nreset git reset \u0026lt;DESTINATION\u0026gt; は HEAD の位置を戻す処理で、 オプションによってindexとworing treeもそこに合わせるように変更される。 --soft なら HEAD 移動のみ。 --mixed なら移動した HEAD にindexも合わせる。 --hard なら移動した HEAD にindexとworiking treeも合わせる。 直前の動作を取り消す用途に絞って使うのが無難:\n# commit直後、それを取り消す (indexとworkingはそのまま) git reset --soft HEAD^ # add直後、それを取り消す (workingとHEADはそのまま) git reset --mixed HEAD # 変更したファイルをHEADの状態に戻す (DANGEROUS!) git reset --hard HEAD # reset直後、それを取り消す git reset --hard ORIG_HEAD # divergedになってしまった手元のbranchを破棄 (DANGEROUS!) git reset --hard origin/master 直前のcommitをちょっと修正したいだけなら git commit --amend が簡単。 それより前のを修正するには git rebase -i HEAD~3 とかで戻ってrewordやedit。\nリモートにpush済みのものは改変しちゃダメ！\nCollaboration 基本的に、自分のリモートリポジトリにpushできるのは自分だけ。 コラボレータを設定して、権限を与えることも可能。 (\u0026ldquo;Settings \u0026gt; Collaborators\u0026rdquo;) でも権限を持つ人が増えすぎると競合・衝突など管理リスクも増える。 権限を持たない人はforkからPull Requestを送り、 権限を持つ少数の人がそれをレビューしてmergeするスタイルが安全。 Pull Request (PR) 他人のリポジトリに貢献するためのGitHubの機能。\ne.g., https://github.com/Rdatatable/data.table/pull/2807\n貢献したいリポジトリをForkして自分のGitHubアカウントに追加。 Forkした自分のリポジトリを手元にclone。 PR用のブランチを作って、そこでソースコードを編集。 コミットして、ひとまず自分のGitHubアカウントにpush。 GitHub上で大元のリポジトリにPRを送る。 取り込んでもらえたら、用済みのブランチを削除。 2人1組でPRとmergeを体験 🐸 KING: リポジトリの管理権限を持つ人 🐰 PAWN: 権限を持たず、PRを送る人 (できれば横に並んで相手の画面も見えるように)\n🐸 GitHubで新しいリポジトリを作成\n🐸 何かtypoを含む README.md を作ってpush\n🐰 相手のGitHubリポジトリでその README.md が見えることを確認\n🐰 右上のForkボタンで自分のGitHubリポジトリに取り込む\n🐰 forkした自分のリポジトリからローカルにclone:\ngit clone https://github.com/{PAWN}/PROJECT.git cd PROJECT/ 🐰 大元のリポジトリにupstreamという名前をつけておく:\ngit remote add upstream https://github.com/{KING}/PROJECT.git git remote -v ちなみに自分のリポジトリには自動的に origin という名前がついている。\n🐰 PR用のブランチを切って移動:\ngit checkout -b fix-typo 🐰 README.md をテキストエディタで編集して commit:\ngit diff git commit -a -m \u0026quot;:memo: Fix typo in README.md\u0026quot; Git連携機能のあるエディタを使っている場合、 そこからdiffやcommitをやってみてもよい。 コードの追加・変更・削除による色分けの便利さも体感しよう。\n🐰 この間にupstreamで更新が無いかどうか確認:\ngit fetch upstream もしあったら、それをデフォルトブランチ(master)越しに取り込む:\ngit checkout master git merge upstream/master git push origin/master git checkout fix-typo git rebase -i master 🐰 自分のリポジトリにpush:\ngit push origin fix-typo 🐰 GitHub上に出現する \u0026ldquo;Compare \u0026amp; pull request\u0026rdquo; ボタンを押す。\n🐰 差分を確認し、コメント欄を埋めて提出。\n🐸 受け取ったPRを確認。必要に応じて修正を要求したり、自分で修正したり。\n🐰 修正を求められたらそのブランチに続けてcommitしてまたpush。\n🐸 問題が無ければmergeする。\n🐸 自分のローカルリポジトリに pull (fetch+merge) する。\n🐰 無事マージされたら作業ブランチを消す。\nTips 習うより慣れる。 最初はコマンドが多くて難しそう・面倒くさそうに感じるけど、 だんだん意識しなくても使えるようになる。 git status やエラー文をちゃんと読む。 どうすればいいかだいたい書いてくれてるし、 そのままウェブ検索すればだいたい解決策が見つかる。 --force とか -f のような強制オプションは、 間違えると取り返しがつかなくなるので基本的に使わない。 コミットを簡潔に要約するメッセージを書く。 好ましいスタイルについては諸説あるけど、 とりあえず大文字で始まる命令形の一文を書くところから始めたらよいのでは。 コミットの内容に応じた分類で先頭に絵文字を入れるスタイルも人気になりつつある。 GitHubにpushされたら自動的に SlackやTwitterに投稿、 というような連携が可能。 RStudioでもディレクトリを\u0026quot;Project\u0026quot;として扱うことでGitを活用できる。 Glossary / 用語 https://help.github.com/articles/github-glossary/\nrepository commitの履歴を保持する拠点。 「ひとつのRパッケージ」とか「1冊の本の原稿」のような単位で作る。 git init で手元に新規作成するか、git clone でリモートから複製する。 commit git内部でroot treeのsnapshotを指すオブジェクト。 root treeのハッシュID、著者、コメントなどの情報を持つ。 動詞としては、staging areaの情報をひとつのcommitとしてリポジトリに登録することを指す。 tree git内部で1つのディレクトリを指すオブジェクトで、commitした時に作られる。 blobやファイル名などのメタデータに依存したハッシュIDを持ち、 その変化は親に伝播する。 blob git内部で1つのファイルを指すオブジェクトで、add時に作られる。 ファイル名などのメタデータは持たず、 ファイルの内容にのみ依存したハッシュIDを持つ。 origin remoteリポジトリの典型的なshortname。 clone時に自動的に追加され、 push先やfetch元を省略したときにデフォルトで使われる。 git remote -v で確認。 master デフォルトのブランチの典型的な名前。 HEAD, @ 現在checkoutしているbranch/commitを指すポインタ。 基本的にはmasterの最新commitを指していることが多い。 1つ前は HEAD^ か HEAD~、 2つ前は HEAD^^ か HEAD~~ か HEAD~2。 (HEAD^2 は merge で複数の親がある場合の2番目) zshのEXTENDED_GLOBが有効になってる場合は HEAD^ がパターン扱いされてエラーになるので、 HEAD\\^ のようにエスケープするか unsetopt NOMATCH しておいたほうがいい。\nFurther reading GitHub: 他人のGit活用事例が見放題。 GitHub Learning Lab: 公式ボットが手取り足取り教えてくれるらしい。 Pro Git book: Gitの公式？本。 Bookdown: R Markdownで本を書く。 ",
  "href": "/lectures/git2019makino.html",
  "tags": [
   "vcs",
   "writing"
  ],
  "title": "Git入門2019",
  "type": "lectures"
 },
 {
  "content": "2018-10-15 中央水産研究所\n事前準備 OS別 新しめのUNIX系OSが好ましい。\nmacOS 10.12 Sierra以降 Command Line Tools Gitは標準装備のやつで大丈夫 Linux Ubuntu 16.04以降 or CentOS 7以降 build-essential とか \u0026ldquo;Development Tools\u0026rdquo; 的なもの 普通にaptやyumで入るGitが古いv1系だったら、どうにかしてv2以降を入れる Windows 10 (1803以降)\nGit本体およびGit Bashを公式から入れる。 インストール途中でいろいろ選択肢あるけどとりあえずデフォルトでよいのでは。\nRStudioのターミナルのシェルとしてGit Bashを設定。 \u0026ldquo;Tools \u0026gt; Global Options \u0026gt; Terminal \u0026gt; Shell\u0026rdquo;\nRStudioで新しいTerminalを立ち上げて最低限のコマンド操作に慣れる。 e.g., pwd, ls, cd, mkdir\nGit Bashにおける HOME がRStudioにおけるそれと異なることに注意。\n### Git Bash echo $HOME # /c/Users/watal echo ~ # /c/Users/watal ### R normalizePath(\u0026#34;~\u0026#34;) # \u0026#34;C:\\\\Users\\\\watal\\\\Documents\u0026#34; UNIX派が仕方なくWindowsを使う場合は MSYS2の pacman で環境を構築するのが良さそう\u0026hellip;?\nOS共通 適当なテキストエディタ(開発環境)を入れておく。 初期状態でもGit/GitHubとの連携機能が付いていて、 変更箇所を色付けしてくれたりコマンド入力を肩代わりしてくれたりするのが便利。\nAtom: GitHub製 VSCode: Microsoft製 RStudio: RStudio製 GitHubに個人アカウントを作る。\nGitの初期設定をターミナルから行う:\ngit --version # 2.0以上であることを確認 git config --global user.name \u0026#34;Watal M. Iwasaki\u0026#34; git config --global user.email \u0026#34;heavywatalあmail.com\u0026#34; git config --global push.default simple cat ~/.gitconfig [user] name = Watal M. Iwasaki email = heavywatalあmail.com [push] default = simple SSHの設定 (任意) GitHubとの通信に2つの方式がある。\nHTTPS: 設定不要で高速だが、操作によってパスワード入力が必要 SSH: 一旦ちゃんと設定すればパスワードなしで快適 ダウンロード操作(clone/fetch/pull)は高速なHTTPSで、\nアップロード操作(push)はパスワード無しのSSHで、というのが楽ちん。\nSSH公開鍵を作ってローカルマシンとGitHubに登録する。\n設定ファイル ~/.gitconfig に pushinsteadof の設定を追加:\n[url \u0026#34;git@github.com:\u0026#34;] pushinsteadof = https://github.com/ 以下、本編。\nGitが無い世界の風景 バージョン違いのファイルがフォルダを埋め尽くす。\n% ls analysis.R analysis2.R analysis-20180129.R analysis-20180129改良版.R analysis-20180210.R analysis-20180210バグ？.R analysis佐藤edit.R analysis佐藤edit田中.R analysis完全版.R analysis最終.R analysis最終改.R analysis決定版！.R analysis真・最終.R plot.R plot2.R plot最終.R plot論文.R ファイルの中にも、いつか使うかもしれなくて消せないコードがたくさん。\nジャンクコードまみれになって、開発効率は低下、バグ混入リスクは上昇。\nオンラインストレージやバックアップ機能では不十分 Dropbox とか Google Drive では、保存のたびに履歴が残る。 Time Machine では、一定時間間隔で履歴が残る。 でも、バージョン管理や共同作業のためのツールじゃないから\u0026hellip;\nいつまでも履歴を保持してもらえるとは限らない。\n(ここでDropboxの履歴を例示しようと思ったらエラーで使えなかった) オフラインだったりバッテリー駆動だったりすると保存漏れが起きる。 いつのバージョンに戻したらいいのか、日時以外の手掛かりが無い。 ファイル変更の競合・衝突に対処しにくい。 Git and GitHub いつでも好きなところに戻れる安心感 履歴を残すタイミングは任意 = 手動。 バージョン(リビジョン)ごとにメッセージを残せる。 差分を簡単に見られる。 複数マシンや複数人での並列作業にも使える オフラインでも作業できる。 ブランチを作ることで競合・衝突の影響を抑えられる。 もし競合・衝突が起きてもうまく対処する機能がある。 課題を管理する機能もある。 読み方はギット、ギットハブ。(ちなみに画像のGIFはジフ) e.g., https://github.com/tidyverse/stringr/commits/master\n両者はどういう関係？ Git は分散型バージョン管理システムとして最も広く使われるオープンソース・ソフトウェア。 手元のコンピュータ上でこれを操作して、変更履歴を記録・閲覧したり送受信したりする。\nGitHub はGitをより便利に使うためのオンラインサービスであり、それを運営する会社の名前でもある。 個人的なリポジトリ置き場としてはもちろんのこと、 多人数で共有・協力してプロジェクトを進めるプラットフォームとしても使える。\n類似ツール Version Control System (VCS) Git git Mercurial hg その他 svn, cvs, rcs など。 Hosting Service GitHub: 公開リポジトリは無料。情報も連携も豊富。 Bitbucket: 非公開リポジトリも無料。 GitLab: 非公開リポジトリも無料。ローカル版もあり。 Gitea: ローカル版のみ。 その他 SourceForge, Google Code など。 VCSは基本的にGit一択。\nホスティングサービスは、使い方や予算などに応じて選択。\nGitHubの使いみち 基本: プレーンテキストのバージョン管理\nプログラムのソースコード: e.g., ggplot2, rstan 論文や本の原稿、サプリ: e.g., R4DS, Advanced R programming Issues: バグ報告、機能要望、課題の列挙などに使われる。 タグを付けたり、特定の人にassignすることもできる。\ne.g., https://github.com/gohugoio/hugo/issues, https://github.com/nlohmann/json/issues\nProjects: プロジェクトのタスク管理のためのツール。 もちろんissueとも連携可能。\ne.g., https://github.com/r-lib/pillar/projects/1\nWiki: チーム内のちょっとした情報共有などに。 でもできればそういう文書もちゃんとGitで管理したほうがいい。\ne.g., https://github.com/gnab/remark/wiki\nGitHub Pages: リポジトリの内容をウェブサイトとして公開できる。\ne.g., https://kazutan.github.io/kazutanR/\n構造と操作の概要 手元の変更を外に伝える 📁 working directory (working tree) 手元のファイルの変更はまだリポジトリに登録されていない ↓ git add staging area (index) 次のコミットに含めるファイルをマークする段階 ↓ git commit local repository 変更履歴が .git/ 内に記録されている ↓ git push remote repository GitHubなど別マシンのリポジトリに反映 外部の変更を手元に取り込む remote repository ↓ git fetch local repository 変更が .git/ に取り込まれたが、見えてるファイルには反映されてない ↓ git checkout or git merge 📁 working directory 手元のファイルが最新版に同期されている 用語 https://help.github.com/articles/github-glossary/\nrepository commitの履歴を保持する拠点。 「ひとつのRパッケージ」とか「1冊の本の原稿」のような単位で作る。 git init で手元に新規作成するか、git clone でリモートから複製する。 commit git内部でroot treeのsnapshotを指すオブジェクト。 root treeのハッシュID、著者、コメントなどの情報を持つ。 動詞としては、staging areaの情報をひとつのcommitとしてリポジトリに登録することを指す。 tree git内部で1つのディレクトリを指すオブジェクトで、commitした時に作られる。 blobやファイル名などのメタデータに依存したハッシュIDを持ち、 その変化は親に伝播する。 blob git内部で1つのファイルを指すオブジェクトで、add時に作られる。 ファイル名などのメタデータは持たず、 ファイルの内容にのみ依存したハッシュIDを持つ。 origin remoteリポジトリの典型的なshortname。 clone時に自動的に追加され、 push先やfetch元を省略したときにデフォルトで使われる。 git remote -v で確認。 master デフォルトのブランチの典型的な名前。 HEAD, @ 現在checkoutしているbranch/commitを指すポインタ。 基本的にはmasterの最新commitを指していることが多い。 1つ前は HEAD^ か HEAD~、 2つ前は HEAD^^ か HEAD~~ か HEAD~2。 (HEAD^2 は merge で複数の親がある場合の2番目) zshのEXTENDED_GLOBが有効になってる場合は HEAD^ がパターン扱いされてエラーになるので、 HEAD\\^ のようにエスケープするか unsetopt NOMATCH しておいたほうがいい。\n基本操作の実践 既存のリポジトリを取ってくる clone GitHub上の適当なリポジトリをひとつ選ぶ。 (e.g., https://github.com/heavywatal/clippson)\n右の方の緑の \u0026ldquo;Clone or download\u0026rdquo; ボタンを押す。\nSSHではなくHTTPSを選択し、URLをコピー。\nターミナルにコマンドを入力:\ngit clone https://github.com/heavywatal/clippson.git\n中身を眺めてみる:\ncd clippson/ ls -al git log git remote -v 最新版のスナップショットだけでなく、 履歴もごっそり複製するので、 このあとはオフラインでもいろいろ操作できる。\n新しいリポジトリを作る init GitHubの右上の \u0026ldquo;+\u0026rdquo; から \u0026ldquo;New repository\u0026rdquo; を選択。\nRepository name を例えば helloworld として \u0026ldquo;Create repository\u0026rdquo; を押す。 いくつかのファイル (README.md, LICENSE, .gitignore) をここで作ることもできるけど、今回はとりあえず空っぽのリポジトリを作る。\n手元のマシンにローカルリポジトリを作る:\nmkdir helloworld cd helloworld/ git init ls -al リポジトリの本体 .git/ が作成されたことを確認。\n空っぽのコミットを作る:\ngit status git commit --allow-empty -m \u0026#34;:beer: Create repository\u0026#34; git status git log 事あるごとに git status や git log を確認すると安心。\n先程作ったリモートリポジトリを紐付けて、pushしてみる:\ngit remote -v git remote add origin https://github.com/YOUR_NAME/helloworld.git git remote -v git push -u origin master git status GitHubで履歴を閲覧し、 git log と同じになってることを確認。\n手元の変更をリモートに push 上で作ったリポジトリに、適当なファイルを追加:\necho \u0026#34;# Hello, world!\u0026#34; \u0026gt; README.md cat README.md git status 作ったファイルをstaging areaに追加:\ngit add README.md git status git diff --staged この変更をcommit:\ngit commit -m \u0026#34;:memo: Create README.md\u0026#34; git status git log git show リモートにpush:\ngit push git status git log リモートの変更を手元に fetch 上のリポジトリでそのまま git fetch してみる。 ローカルとリモートは同じ状態なので当然何も起こらない。\n練習のためGitHub上で LICENSE ファイルを作成する。\nGitHub上のリポジトリのトップページを開き \u0026ldquo;Create new file\u0026rdquo; ボタンを押す。 ファイル名に LICENSE と入力。 右に現れる \u0026ldquo;Choose a license template\u0026rdquo; というボタンを押す。 とりあえず \u0026ldquo;MIT License\u0026rdquo; を選択。 YearとNameを適当に埋めて \u0026ldquo;Review and submit\u0026rdquo;。 \u0026ldquo;Commit directly to the master branch\u0026rdquo; を選択して \u0026ldquo;Commit new file\u0026rdquo; その変更をローカルリポジトリに取り寄せる:\ngit fetch git status git log --all ls -al リポジトリ内部 .git/ の origin/master は更新されたが、 working directoryにはまだ反映されていない。\norigin/master の内容を手元のファイルに反映する:\ngit merge git status git log git show ls -al git fetch と git merge を一気にやってくれる git pull というコマンドもあり、 普段の一人作業ではよく使う。\nその他よく使うコマンド diff 差分を表示:\n# HEAD vs working (staging前のファイルが対象) git diff # HEAD vs index (staging済みcommit前のファイルが対象) git diff --staged # HEAD vs working+index (commit前の全ファイルが対象) git diff HEAD # 特定コミットの変更点 (diffじゃない...) git show [revision] rm, clean tracking対象から外して忘れさせる(手元のファイルはそのまま):\ngit rm --cached \u0026lt;file\u0026gt; .gitignore で無視されてるuntrackedファイルを消す:\ngit clean -fdX 無視されていないuntrackedファイルも消したい場合は小文字の -fdx (危険)。\nreset git reset \u0026lt;DESTINATION\u0026gt; は HEAD の位置を戻す処理で、 オプションによってindexとworing treeもそこに合わせるように変更される。 --soft なら HEAD 移動のみ。 --mixed なら移動した HEAD にindexも合わせる。 --hard なら移動した HEAD にindexとworiking treeも合わせる。 直前の動作を取り消す用途に絞って使うのが無難:\n# commit直後、それを取り消す (indexとworkingはそのまま) git reset --soft HEAD^ # add直後、それを取り消す (workingとHEADはそのまま) git reset --mixed HEAD # 変更したファイルをHEADの状態に戻す (DANGEROUS!) git reset --hard HEAD # reset直後、それを取り消す git reset --hard ORIG_HEAD # divergedになってしまった手元のbranchを破棄 (DANGEROUS!) git reset --hard origin/master 直前のcommitをちょっと修正したいだけなら git commit --amend が簡単。 それより前のを修正するには git rebase -i HEAD~3 とかで戻ってrewordやedit。\nリモートにpush済みのものは改変しちゃダメ！\nチーム作業 基本的に、自分のリモートリポジトリにpushできるのは自分だけ。 コラボレータを設定して、権限を与えることも可能。 (\u0026ldquo;Settings \u0026gt; Collaborators\u0026rdquo;) でも権限を持つ人が増えすぎると競合・衝突など管理リスクも増える。 権限を持たない人はforkからPull Requestを送り、 権限を持つ少数の人がそれをレビューしてmergeするスタイルが安全。 Pull Request (PR) 他人のリポジトリに貢献するためのGitHubの機能。\ne.g., https://github.com/Rdatatable/data.table/pull/2807\n貢献したいリポジトリをForkして自分のGitHubアカウントに追加。 Forkした自分のリポジトリを手元にclone。 PR用のブランチを作って、そこでソースコードを編集。 コミットして、ひとまず自分のGitHubアカウントにpush。 GitHub上で大元のリポジトリにPRを送る。 取り込んでもらえたら、用済みのブランチを削除。 2人1組でPRとmergeを体験 🐸 カエル: リポジトリの管理権限を持つ人 🐰 ウサギ: 権限を持たず、PRを送る人 (できれば横に並んで相手の画面も見えるように)\n🐸 GitHubで新しいリポジトリを作成\n🐸 何かtypoを含む README.md を作ってpush\n🐰 相手のGitHubリポジトリでその README.md が見えることを確認\n🐰 右上のForkボタンで自分のGitHubリポジトリに取り込む\n🐰 forkした自分のリポジトリからローカルにclone:\ngit clone https://github.com/{PAWN}/PROJECT.git cd PROJECT/ 🐰 大元のリポジトリにupstreamという名前をつけておく:\ngit remote add upstream https://github.com/{KING}/PROJECT.git git remote -v ちなみに自分のリポジトリには自動的に origin という名前がついている。\n🐰 PR用のブランチを切って移動:\ngit checkout -b fix-typo 🐰 README.md をテキストエディタで編集して commit:\ngit diff git commit -a -m \u0026quot;:memo: Fix typo in README.md\u0026quot; Git連携機能のあるエディタを使っている場合、 そこからdiffやcommitをやってみてもよい。 コードの追加・変更・削除による色分けの便利さも体感しよう。\n🐰 この間にupstreamで更新が無いかどうか確認:\ngit fetch upstream もしあったら、それをデフォルトブランチ(master)越しに取り込む:\ngit checkout master git merge upstream/master git push origin/master git checkout fix-typo git rebase -i master 🐰 自分のリポジトリにpush:\ngit push origin fix-typo 🐰 GitHub上に出現する \u0026ldquo;Compare \u0026amp; pull request\u0026rdquo; ボタンを押す。\n🐰 差分を確認し、コメント欄を埋めて提出。\n🐸 受け取ったPRを確認。必要に応じて修正を要求したり、自分で修正したり。\n🐰 修正を求められたらそのブランチに続けてcommitしてまたpush。\n🐸 問題が無ければmergeする。\n🐸 自分のローカルリポジトリに pull (fetch+merge) する。\n🐰 無事マージされたら作業ブランチを消す。\nTips 習うより慣れる。 最初はコマンドが多くて難しそう・面倒くさそうに感じるけど、 だんだん意識しなくても使えるようになる。 git status やエラー文をちゃんと読む。 どうすればいいかだいたい書いてくれてるし、 そのままウェブ検索すればだいたい解決策が見つかる。 --force とか -f のような強制オプションは、 間違えると取り返しがつかなくなるので基本的に使わない。 コミットを簡潔に要約するメッセージを書く。 好ましいスタイルについては諸説あるけど、 とりあえず大文字で始まる命令形の一文を書くところから始めたらよいのでは。 コミットの内容に応じた分類で先頭に絵文字を入れるスタイルも人気になりつつある。 GitHubにpushされたら自動的に SlackやTwitterに投稿、 というような連携が可能。 RStudioでもディレクトリを\u0026quot;Project\u0026quot;として扱うことでGitを活用できる。 Further reading GitHub: 他人のGit活用事例が見放題。 GitHub Learning Lab: 公式ボットが手取り足取り教えてくれるらしい。 Pro Git book: Gitの公式？本。 Bookdown: R Markdownで本を書く。 ",
  "href": "/lectures/git2018nrifs.html",
  "tags": [
   "vcs",
   "writing"
  ],
  "title": "Git入門2018",
  "type": "lectures"
 },
 {
  "content": "https://supcom.hgc.jp/\n環境整備 ハードウェア ソフトウェア RHEL 7.6 /usr/bin/cmake 2.8.12 /usr/bin/cmake3 3.13.4 /usr/bin/gcc 4.8.5 /usr/bin/python 2.7.15 /usr/bin/python3 3.4.9 /usr/local/package/boost/1.67.0/ /usr/local/package/gcc/7.3.0/ /usr/local/package/python/3.6.5/ /usr/local/package/r/3.5.0/ tumopp on R https://heavywatal.github.io/rtumopp/\nShrokane5 で提供されている R-3.5 は /usr/local/package/gcc/7.3.0でビルドされているため C++14のライブラリも問題なく使える。\n普通に cmake とするとバージョン2.8.12のほうが参照されてしまうので、 優先的にパスの通ってるところに ln -s /usr/bin/cmake3 ~/local/bin/cmake などとしてバージョン3.8以上が使えるようにする。\nRを起動し、パッケージをインストール:\ninstall.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#34;heavywatal/rtumopp\u0026#34;) 最新版にアップデートするのもこのコマンド。\nパッケージを読み込んで実行:\nlibrary(tumopp) result = tumopp(\u0026#34;-D3 -Chex -N256\u0026#34;) command-line https://heavywatal.github.io/tumopp/\nShrokane5 (RHEL 7) では問題なくHomebrewを使える。\nbrew install heavywatal/tap/tumopp exec $SHELL -l tumopp -D3 -Chex -N256 tumopp -h ",
  "href": "/bio/shirokane.html",
  "tags": [
   "job"
  ],
  "title": "SHIROKANE",
  "type": "bio"
 },
 {
  "content": "概要 Global Interpreter Lock (GIL) の制約により、 1つのPythonインタープリタでは同時に1つのスレッドしかコードを実行できない。 したがってCPUバウンドなピュアPythonコードを threading でマルチスレッド化しても速くならない。 subprocess による外部プログラム実行やI/OなどGIL外の処理を待つ場合には有効。\n一方 multiprocessing は新しいインタプリタを os.fork() で立ち上げるので、 CPUバウンドなPythonコードもGILに邪魔されず並列処理できる。 ただし通信のため関数や返り値がpicklableでなければならない。\nそれらの低級ライブラリを使いやすくまとめたのが concurrent.futures (since 3.2) なので、とりあえずこれを使えばよい。 新しい asyncio (since 3.4) は勝手が違いすぎてとっつきにくい。\n並列化対象の関数の例:\nimport time import random def target_func(x): time.sleep(random.uniform(0, 1)) return x + 1 concurrent.futures import os import concurrent.futures as confu # 呼び出し順に拾う with confu.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor: futures = [executor.submit(target_func, x) for x in range(8)] (done, notdone) = confu.wait(futures) for future in futures: print(future.result()) # 終わったやつから拾う with confu.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor: futures = [executor.submit(target_func, x) for x in range(8)] for future in confu.as_completed(futures): print(future.result()) デフォルトの max_workers=None では 5 * os.cpu_count() になるらしい。\nmultiprocessing 由来の ProcessPoolExecutor も使い方は同じ。 こちらは os.cpu_count() がデフォルト。\nthreading target を指定して作った Thread インスタンスで start() するのが基本。\nimport threading threads = [] for i in range(8): th = threading.Thread(target=target_func, args=[i]) th.start() threads.append(th) for th in threads: th.join() 返り値を得たい場合などはクラスを継承していじる必要がある。 その場合は必ず run() メソッドをoverrideする。\nclass Worker(threading.Thread): def __init__(self, target, name=None, args=(), kwargs={}): threading.Thread.__init__(self, None, target, name, args, kwargs) self._return = None def run(self): self._return = self._target(*self._args, **self._kwargs) def get(self, timeout=None): self.join(timeout) return self._return threads = [] for i in range(8): th = Worker(target=target_func, args=[i]) th.start() threads.append(th) for th in threads: print(th.get()) スレッド数の上限値を設けたい場合は threading.Semaphore でうまくロックしてやる必要がある。\nmultiprocessing threading.Thread とほぼ同じインターフェイスの Process クラスも用意されているが、 Pool を使ったほうが楽チン。\nimport multiprocessing as mp with mp.Pool(processes=mp.cpu_count()) as pool: results = [pool.apply_async(target_func, [x]) for x in range(8)] for res in results: print(res.get()) mp.cpu_count() このためだけに multiprocessing をimportするのは億劫だったが、 3.4で os.cpu_count() が追加された。\nHyper-Threading (HT)が有効な場合は論理コア数が返ってくることに注意。 CPUを100%使い続ける数値計算とかだとそんなに並列化しても早くならない。 物理コア数を取得したい場合は psutil の psutil.cpu_count(logical=False) を使う。 標準ライブラリではないが、広く使われてるらしい。\n",
  "href": "/python/concurrent.html",
  "tags": [
   "python",
   "concurrent"
  ],
  "title": "concurrent.futures",
  "type": "python"
 },
 {
  "content": "数あるMCMCアルゴリズムの中でも効率的なHMC(Hybrid/Hamiltonian Monte Carlo)を用いてベイズ推定を行うツール。 RやPythonなどいろんなインターフェイスで利用可能。 RStan, PyStan が長らく使われてきたが、 CmdStanR, CmdStanPy への移行が進んできている。\nhttps://mc-stan.org/\nインストール RやPythonのパッケージを入れてから、それ越しにCmdStan本体を入れる。\ninstall.packages(\u0026#34;cmdstanr\u0026#34;, repos = \u0026#34;https://mc-stan.org/r-packages/\u0026#34;) library(cmdstanr) check_cmdstan_toolchain() install_cmdstan() cmdstan_path() cmdstan_version() %pip3 install cmdstanpy import cmdstanpy cmdstanpy.install_cmdstan() cmdstanpy.cmdstan_path() cmdstanpy.cmdstan_version() cmdstanpy.show_versions() 基本的な流れ cmdstanrを読み込む library(cmdstanr) 名前付きlistとしてデータを用意する。 e.g., 平均10のポアソン乱数。 sample_size = 1000L mydata = list(N = sample_size, x = rpois(sample_size, 10)) Stan言語でモデルを記述する。 RStanには文字列で渡せたがCmdStanPy, CmdStanRは別ファイル必須。 e.g., 与えられたデータがポアソン分布から取れてきたとすると、 その平均はどれくらいだったか？ data { int\u0026lt;lower=0\u0026gt; N; array[N] int\u0026lt;lower=0\u0026gt; x; } parameters { real\u0026lt;lower=0\u0026gt; lambda; } model { x ~ poisson(lambda); } モデルをC++に変換してコンパイルする。 中間ファイルは *.hpp。 model = cmdstan_model(\u0026#34;model.stan\u0026#34;) https://mc-stan.org/cmdstanr/reference/cmdstan_model.html コンパイル済みモデルを使ってMCMCサンプリング fit = model$sample(mydata) https://mc-stan.org/cmdstanr/reference/model-method-sample.html 結果を見てみる print(fit) fit$summary() fit$cmdstan_summary() fit$cmdstan_diagnose() fit$sampler_diagnostics() fit$diagnostic_summary() fit$metadata() https://mc-stan.org/cmdstanr/reference/CmdStanMCMC.html MCMCサンプルを使う。 draws_df = fit$draws(format = \u0026#34;df\u0026#34;) draws = fit$draws() params = names(model$variables()$parameters) bayesplot::mcmc_acf_bar(draws, pars = params) bayesplot::mcmc_trace(draws, pars = params) bayesplot::mcmc_hist(draws, pars = params) bayesplot::mcmc_combo(draws, pars = params) rhat = bayesplot::rhat(fit) neff = bayesplot::neff_ratio(fit) bayesplot::mcmc_rhat(rhat) bayesplot::mcmc_neff(neff) https://mc-stan.org/cmdstanr/reference/fit-method-draws.html Stan文法 https://mc-stan.org/documentation/\nブロック コード内に登場できるブロックは7種類で、省略可能だが順番はこの通りでなければならない。\nfunctions {...} 関数を定義できる。 data {...} Rから受け取る定数の宣言。 transformed data {...} 定数の宣言と代入。 決め打ちのハイパーパラメータとか。 決定論的な変換のみ可能。 parameters {...} サンプリングされる変数の宣言。 transformed parameters {...} モデルで使いやすい形に値を変換しておくとか。 ここに書いた変数もサンプリングされる。 model {...} 唯一の必須ブロック。 サンプルされないローカル変数を宣言してもよいが、制約をかけることはできない。 generated quantities {...} normal_rng()などによる乱数生成が許される唯一のブロック。 観察値の確信区間とかを モデリング あるパラメータにおけるlog probabilityと近傍での傾きを計算し、 それらを元に次の値にジャンプする、という操作が繰り返される。 modelブロック内で暗黙的に定義されている target 変数に対して += 演算子で対数確率をどんどん加算していく。 (昔は隠れ変数lp__やincrement_log_prob()などを使ってた。)\nサンプリング文(sampling statement)はそれを簡単に記述するためのショートカット。 名前とは裏腹に、確率分布からのサンプリングが行われるわけではないので紛らわしい。 例えば以下の表現はほぼ等価。 (定数の扱い方がうまいとかでサンプリング文のほうが効率的らしいけど)\nx ~ normal(0.0, 1.0); target += normal_lpdf(x | 0.0, 1.0); target += -0.5 * square(x); 確率分布としての正規化はうまいことやっといてくれるから気にしなくていいらしい (が、T[,]によるtruncated distributionではこうやって調整する、 とかいう記述もあるので、そのへんはまだよく分からない)。\n\\[\\begin{aligned} \\log p(x) \u0026\\propto -\\frac {x^2} 2 \\\\ p(x) \u0026\\propto \\exp \\left(- \\frac {x^2} 2 \\right) \\end{aligned}\\] 名のある確率分布はだいたい関数として用意されている。 形のバリエーションとしては:\n確率密度関数: *_lpdf(y | ...), *_lpmf(y | ...) 累積分布関数: *_cdf(y | ...), *_lcdf(y | ...) 相補累積分布関数: *_lccdf(y | ...) 乱数生成: *_rng(...) (対数版のsuffixは昔は _cdf_log(), _ccdf_log() という形だった)\n型 スカラーは整数(int) or 実数(real)。 実数ベクトル(vector, row_vector)と実数行列(matrix)は Eigen::Vector や Eigen::Matrix で実装されているので効率的に行列演算を行える。 配列(array)は std::vector で実装されていて、 整数配列や行列配列など何でも作れるが、行列演算はできないので生のforループが必要。 int v[3] のように宣言する書き方は非推奨になった。 宣言時に上限下限を設定できる (constrained integer/real)。 bool型は無くて基本的に整数の1/0。分岐ではnon-zeroがtrue扱い。 int i; real x; array[42] int a; array[42] real y; int\u0026lt;lower=1,upper=6\u0026gt; dice; vector[3] v; row_vector[3] r; matrix[3, 3] m; x * v // vector[3] r * v // real v * r // matrix[3, 3] m * v // vector[3] m * m // matrix[3, 3] m[1] // row_vector[3] そのほかの特殊な制約つきの型\nsimplex: 合計が1になる非負実数ベクトル unit_vector: 二乗和が1になる実数ベクトル ordered, positive_ordered: 昇順実数ベクトル。降順にしたければ transformed parameters ブロックで。 cov_matrix, corr_matrix, cholesky_factor_cov, cholesky_factor_corr Tips 条件分岐するときはなるべくif文を避けて三項演算子やステップ関数を使うべし、 という言語が多いけどStanでは逆にif文を素直に書くほうが良いらしい。 if_else()では真値でも両方の引数が評価されちゃうし、 step() や int_step() からの掛け算は遅いのだとか。\n代入演算子は普通に = イコール。(昔は \u0026lt;- 矢印だった)\n対数尤度の値を確認したいときは print(\u0026quot;log_prob: \u0026quot;, target())\n正規分布のsigmaやポアソン分布のlambdaの値域をちゃんと real\u0026lt;lower=0\u0026gt; に絞っているのに Scale parameter is 0, but must be positive! と怒られることがある。 実害はないけどどうしても警告を消したい場合は 違うseedを使うとか step_size = 0.1 のように歩幅を狭めるとかで対処できる。\nlibrary(bayesplot) https://mc-stan.org/bayesplot/\nlibrary(posterior) https://mc-stan.org/posterior/\nlibrary(rstanarm) https://mc-stan.org/rstanarm/\nR標準のGLMのような使い心地でStanを動かせるようにするパッケージ。\nformulaでモデルを立てられる。 data.frameを渡せる。 パラメータ調整やコンパイルの済んだ部品を組むような形なので試行錯誤が早い。 library(brms) https://paul-buerkner.github.io/brms/\nrstanarmと同様にformula形式でStanを動かせるようにする。 相違点:\nstan-devチームの一員ではない。 コンパイル済みの部品を使わずStanコードを生成する。そのぶん遅いが柔軟。 CmdStanRをbackendとして使える。 関連書籍 ",
  "href": "/rstats/stan.html",
  "tags": [
   "r",
   "c++"
  ],
  "title": "Stan",
  "type": "rstats"
 },
 {
  "content": " forループやlistの処理などをより簡潔に書けるようにしてくれるパッケージ。 標準のapply系関数よりも覚えやすく読みやすい。 dplyr や tidyr と組み合わせて使う。 いまのところ並列化する機能はないので、 それに関してはforeach/parallelページを参照。\ntidyverse に含まれているので、 install.packages(\u0026quot;tidyverse\u0026quot;) で一括インストール、 library(tidyverse) で一括ロード。\nlist, vector操作 各要素に関数を適用するapply系関数 library(conflicted) library(tidyverse) v = list(1, 2L, \u0026#34;3\u0026#34;) check_class = function(x) {paste0(x, \u0026#34; is \u0026#34;, class(x))} # 自分でfor文を書くと結構大変 results = vector(\u0026#34;list\u0026#34;, length(v)) for (i in seq_along(v)) { results[[i]] = check_class(v[[i]]) } # 1行で簡潔に記述でき、意図も明確 results = lapply(v, check_class) results = v |\u0026gt; purrr::map(check_class) results = v |\u0026gt; purrr::map(function(x) {paste0(x, \u0026#34; is \u0026#34;, class(x))}) results = v |\u0026gt; purrr::map(\\(x) {paste0(x, \u0026#34; is \u0026#34;, class(x))}) results = v |\u0026gt; purrr::map(~ {paste0(.x, \u0026#34; is \u0026#34;, class(.x))}) purrr::map(.x, .f, ...) list/vector .x の各要素に関数 .f を適用した結果をlistに詰めて返す。 base::lapply()とほぼ同義。 .f にformulaや数値などを渡すと関数に変換した上で処理してくれる。(後述) purrr::map_vec(.x, .f, ..., .ptype = NULL) listではなくvectorを返すmap亜種。 型は推定してもらえるが .ptype = integer() のように指定も可能。 base::vapply() と違って型省略可能。 base::sapply() と違って常にvectorを返す。 関数名で型指定する purrr::map_lgl(), map_int(), map_dbl(), map_chr() もある。 purrr::map2(.x, .y, .f, ...) 2変数バージョン。 3変数以上渡したいときはlistかdata.frameにまとめて次のpmap()を使う。 ほかの map_*() 亜種にも同様に提供されている。 purrr::pmap(.l, .f, ...) listの中身をparallelに処理するmap。 関数 .f の引数はlistの要素名と一致させるか ... で受け流す必要がある。 e.g., pmap(list(a = 1:3, b = 4:6), function(a, b) {a * b}) 。 data.frameの正体はlist of columnsなのでそのまま.lとして渡せる。 ほかの map_*() 亜種にも同様に提供されている。 purrr::map_if(.x, .p, .f, ...) .pがTRUEになる要素のみ.f()を適用し、残りはそのまま出力。 .pはlogical vectorでもいいし、 .x[[i]]を受け取るpredicate関数でもよい。 番号か名前で選ぶにはpurrr::map_at(.x, .at, .f, ...) purrr::walk(.x, .f, ...) map()同様に関数を適用しつつ元の値をそのままinvisible返しする亜種。 purrr::lmap(.x, .f, ...) .x[[i]]ではなく.x[i]を参照する亜種。 purrr::imap(.x, .f, ...) 名前や整数インデックスを第二引数で受け取れる亜種。 iwalk() などの派生もある。 purrr::modify(.x, .f, ...) 入力と同じ型で出力する亜種。 つまりdata.frameを入れたらlistじゃなくてdata.frameが出てくる。 e.g., diamonds |\u0026gt; modify_if(is.numeric, round) purrr::modify_tree(x, ..., leaf = identity, is_node = NULL, pre = identity, post = identity) listを再帰的に巡りながら関数を適用する。 最末端のleaf/sinkノードだけに適用するなら leaf, 内側も含めて全ノードに適用するなら pre / post を使う (前者は下りながら leaf 適用前、後者は leaf 適用後に上りながら)。 list要素の抽出・変更 purrr::pluck(.x, ..., .default = NULL) オブジェクト .x 内の要素を引っ張り出す [[ の強力版。 ... には整数、文字列、関数、listで複数指定できる。 例えば accessor(x[[1]])$foo だと読む順が左右に振られるが、 pluck(x, 1, accessor, \u0026quot;foo\u0026quot;) だと左から右に読み流せる。 存在を問うだけの　pluck_exists() もある。 pluck()\u0026lt;- も提供されているので代入も可能。 この用途には purrr::modify_in(.x, .where, .f, ...) や purrr::assign_in(x, where, value) もある。 purrr::chuck(.x ,...) 存在しない場合にエラー終了してくれる pluck() 亜種。 purrr::list_assign(.x, ..., .is_node = NULL) listに対して dplyr::mutate() するような感じ。 変更対象の要素がlistとして存在していても普通に上書き。 purrr::list_modify(.x, ..., .is_node = NULL) list_assign() と似てるが、変更対象の既存listで言及されなかった要素を変更しない、という点で異なる e.g., list(a = list(x = 1)) |\u0026gt; list_modify(a = list(y = 2)) で要素 x がそのまま保持されて要素 a は長さ2になる。 purrr::list_merge(.x, ..., .is_node = NULL) list_modify() と似てるが、変更対象の既存要素に上書きせずappendする。 e.g., list(a = list(x = 1)) |\u0026gt; list_modify(a = list(x = 2)) で要素 x が長さ2になる。 purrr::keep(.x, .p, ...), discard(), compact() listやvectorの要素を .p に応じて取捨選択。 .p に関数を渡した場合の挙動は .x[.p(.x)] じゃなくて .x[map_lgl(.x, .p, ...)] となることに注意。 名前を指定する亜種 keep_at(), discard_at() もある。 purrr::some(.x, .p, ...), purrr::every(), purrr::none() .p(.x[[i]]) が {少なくともひとつ TRUE, すべて TRUE, すべて FALSE} なら TRUE を返す。 purrr::has_element(.x, .y) list .x は要素 .y を持っている。 some(.x, identical, .y) list変形・解体 purrr::list_flatten(x, ..., name_spec, name_repair) 階層性のあるlistを浅いほうから一段階解消する。 結果がすべて整数とかでも勝手にvector化せず、常にlistを返す。 unlist(x, recursive = FALSE) |\u0026gt; as.list() のようなもの。 purrr::list_simplify(x, ..., strict = TRUE, ptype = NULL) listを一段階解消して同じ長さのvectorにする。 入力と出力の対応が保たれるので dplyr::mutate() の中とかでも使いやすい。 入力listの要素はすべて互換性のある型かつ長さ1である必要がある。 ptype = integer() のように明示的に型指定できる。 型が合わなくてvector化できないようならlistでいいから出力して、というときは strict = FALSE purrr::list_c(x, ..., ptype = NULL) listを一段階解消して要素を連結し、vectorにする。 purrr::list_simplify() とは異なり、対応関係や長さは気にせずとにかく連結する。 unlist(x, recursive = FALSE) |\u0026gt; as.vector() を安全にしたようなもの。 ptype = integer() のように明示的に型指定できる。 purrr::list_rbind(x, ..., names_to = rlang::zap(), ptype = NULL) list of data.frames を rbind() して返す。 例えば、同じ形式のCSVファイルを一気に読んで結合、みたいなときに便利: files = fs::dir_ls(\u0026#34;path/to/data/\u0026#34;, glob = \u0026#34;*.csv\u0026#34;) combined_df = files |\u0026gt; purrr::map(readr::read_csv) |\u0026gt; purrr::list_rbind() purrr::list_cbind() もある。 purrr::list_transpose(x, ..., template = NULL, simplify = NA, ptype = NULL, default = NULL) 行列転置関数t()のlist版。 例えば、pair of lists \u0026lt;=\u0026gt; list of pairs。 data.frameをlistとして渡すとlist of rowsが得られる。 その他 purrr::reduce(.x, .f, ..., .init) 二変数関数を順々に適用して1つの値を返す。 C++でいうstd::accumulate()。 例えば reduce(1:3, `+`) の結果は6。 purrr::accumulate(.x, .f, ..., .init) 二変数関数を順々に適用し、過程も含めてvectorで返す。 C++でいうstd::partial_sum()。 例えば accumulate(1:3, `+`) の結果は 1 3 6 。 purrr::set_names(x, nm = x) 標準のsetNames(x = nm, nm)は第二引数のほうが省略不可という気持ち悪い定義だが、 この改良版ではその心配が解消されている。 長さや型のチェックもしてくれる。 関数 .f として渡せるもの apply/map系関数は、名前のついた関数だけでなく、その場で定義された無名関数も受け取れる。 ごく短い関数や一度しか使わない関数には名前をつけないほうが楽ちん。 R 4.1 からはバックスラッシュを使った短縮表記 \\() が便利。 purrrのmap系関数はチルダ ~ を使ったformulaを受け取って関数として処理してくれる。\n# named function ord = function(x) {strtoi(charToRaw(x), 16L)} map_int(letters, ord) # unnamed function map_int(letters, function(x) {strtoi(charToRaw(x), 16L)}) map_int(letters, \\(x) strtoi(charToRaw(x), 16L)) # formula map_int(letters, ~ strtoi(charToRaw(.x), 16L)) # integer/character li = list(lower = letters, upper = LETTERS) map_chr(li, 3L) map_chr(li, \\(x) x[[3L]]) formula内部では、第一引数を.xまたは.として、第二引数を.yとして参照する。 ..1, ..2, ..3 のような形で三つめ以降も参照できる。\npurrr::as_mapper(.f, ...) map() 内部で関数への変換機能を担っている関数。 formulaを受け取ると function(.x, .y, . = .x) のような関数に変換する。 数値や文字列を受け取ると [[ による抽出関数に変換する。 参照先が存在しない場合の値はmap関数の .default 引数で指定できる。 purrr::partial(...f, ..., .env, .lazy, .first) 引数を部分的に埋めてある関数を作る。C++でいう std::bind() deprecated/superceeded purrr::map_dfr(.x, .f, ..., .id = NULL), map_dfc() 入力と出力が一対一対応しないということでmapファミリーから外され、 purrr::map() |\u0026gt; purrr::list_rbind() に取って代わられた。 purrr::flatten(.x) purrr::list_flatten(), purrr::list_simplify(), purrr::list_c() に取って代わられた。 flatten_lgl(), flatten_int(), flatten_dbl(), flatten_chr(), flatten_dfr() も同様。 purrr::invoke(.f, .x = NULL, ..., .env = NULL) rlang::exec() に取って代わられた。 list .x の中身を引数として関数 .f を呼び出す。 関数に渡す引数があらかじめlistにまとまってるときに使うdo.call()の改良版。 purrr::invoke_map(.f, .x = list(NULL), ..., .env = NULL) purrr::map(.f, rlang::exec, ...) に取って代わられた。 関数listを順々に実行してlistで返す。 引数.xは同じ長さのlist of listsか、list of a listをリサイクル。 e.g., invoke_map(list(runif, rnorm), list(c(n = 3, 0, 1))) purrr::list_along(x) rep_along(x, list()) に取って代わられた。 x と同じ長さの空listを作る vector(\u0026quot;list\u0026quot;, length(x)) のショートカット。 purrr::cross2(.x, .y, .filter = NULL) tidyr::crossing() とか tidyr::expand() のほうが推奨。 listの各要素の組み合わせを作る。 .filter に渡した関数が TRUE となるものは除外される。 名前付きlistを渡す purrr::cross() や purrr::cross_df() もある。 purrr::transpose(.l) purrr::list_transpose() に取って代わられた。 関連書籍 ",
  "href": "/rstats/purrr.html",
  "tags": [
   "r",
   "tidyverse"
  ],
  "title": "purrr",
  "type": "rstats"
 },
 {
  "content": "2D/3D Cellular Automaton上の個体・細胞の分布を評価したい。 そのためには白黒の二値画像処理の手法が結構使える。\n図形 X それぞれのノード(画素)の在・不在情報の集合。 $x \\in X$ 構造要素 (Structuring Element: SE) さまざまな処理を施すために用いられる単位図形のようなもの。 例えば、原点とそのムーア近傍。 $b \\in B$ voxel 3D空間における単位。2Dでいうpixel。 基本処理 Translation 平行移動 \\[ X_b = \\{x + b \\mid x \\in X\\} \\] Dilation 膨張 \\[ X \\oplus B = \\bigcup_{b \\in B} X_b \\] X と B のMinkowski和。 X を B の範囲でずらしながらunionを取ったもの。 国土をX 、半径12海里の円をSEとした、領空みたいなイメージ。\nErosion 浸食 \\[ X \\ominus B = \\bigcap_{b \\in B} X_b \\] X と B のMinkowski差。 X を B の範囲でずらしながらintersectを取ったもの。 SEを消しゴムとして X の外周上を走らせ、削るイメージ。\nOpening \\[ X \\circ B = (X \\ominus B) \\oplus B \\] 浸食してから膨張する。 X からハミ出ないようにSEを滑らせた軌跡に相当する。 トゲの先端や X 外部のチリなど、SEより小さい構造が削られて小さくなる。 特定の形を持ったSEを使えば、それを含む領域だけを抽出するのにも使える。\n元画像との差分 $X - (X \\circ B)$ は Top Hat と呼ばれ、 トゲの先っちょや背景のノイズ成分が得られる。\nClosing \\[ X \\bullet B = (X \\oplus B) \\ominus B \\] 膨張してから浸食する。 X の外部をOpeningすることと同義。 X 内部のヒビやチリなど、SEより小さい構造が塗りつぶされ、大きくなる。\n元画像との差分 $(X \\bullet B) - X$ は Black Hat と呼ばれ、 X 内のヒビやトゲの根元らへんが得られる。\n応用 Pattern Spectrum, サイズ分布 小さいSEから順に大きくしながら Openingで削れた部分の面積を記録していく。 元画像の面積で割ったものはサイズ密度関数(size density function)と呼ばれる。 細かいギザギザを含む図形ほど小さいSEで削れる成分が多い。 要約統計量としてはモーメントやエントロピーが使える。\nMorphological gradient \\[ (X \\oplus B) - (X \\ominus B) \\] dilationとerosionの差。 エッジ検出法のひとつ。 X上の境界が欲しい場合は$X - (X \\ominus B)$。 背景側の境界が欲しい場合は$(X \\oplus B) - X$。\nノイズ除去 平滑化フィルタ SEを端から端まで動かしつつ、その中に含まれる画素の平均値を中央画素に適用していく。 Gaussian filterのように、遠いものほど軽くなるように重み付けをする場合もある。 いずれにせよ、エッジがボヤけてしまうのが問題。 Median filter 平均値ではなく中央値で置き換える。 エッジは保存されるが、ソートを伴うので計算量は多め。 ライブラリ 画像処理を施す\nscikit-image Pythonモジュール。 scipy.ndimage を更に拡張したもの。 numpy.array を使って表現されるので汎用関数の適用も容易。 OpenCV (Open Source Computer Vision) C++、Pythonなど。信頼と実績があるらしく、書籍やネット上の情報も多い。 CImg C++。ヘッダひとつincludeするだけ。 ドキュメントも良さげ。 imager R。新しめでドキュメントも充実。内部でCImgを利用。 CairoやX11に依存しているので、 Rもそれらしくビルドされてる必要がある。 mmand R。READMEは良さげ。内部はRcpp。 ほとんど他のライブラリに依存していないのでインストールしやすい。 Morpho R。ドキュメント不足。 ",
  "href": "/bio/mathmorph.html",
  "tags": [
   "math",
   "graph"
  ],
  "title": "数理形態学",
  "type": "bio"
 },
 {
  "content": "概要 ターミナル上で軽快に動作するテキストエディタ。 文字入力はインサートモード、それ以外はすべてノーマルモードで行う。 起動時はノーマルモード。\nオリジナルがvi、改良版がvim (Vi IMproved)。 ただし、shの正体がbashであるように、LinuxやMacに入ってるviの正体はvim。 vimのデフォルトは拡張を無効にしたvi互換モード。 つまり設定をいじらなければ起動コマンドはviでもvimでも同じ？\nターミナル上で設定ファイルを修正したり、 git commit でメッセージ入力したり、 といった軽い用途でしか私は使わない。 その用途でも人に紹介するなら後述のnanoから。 それ以上のテキスト編集はVSCodeで。 Neovimも気になってはいる。\nノーマルモード とりあえず覚えてないと死ぬやつ key action i インサートモードに移行 :w 保存 :q 終了 :q! 保存しないで強制終了 u 元に戻す x 1文字削除 (行は消せない) dd 行カット ctrl-c 中断 移動 key action hjkl ←↓↑→ b B 単語頭 w W 次の単語頭 0 行頭 ^ 行頭 (非空白) $ 行末 H 画面先頭 L 画面末尾 gg ファイル先頭 G ファイル末尾 ctrl-b 前ページ ctrl-u 半ページ上 ctrl-d 半ページ下 ctrl-f 次ページ 数字と組み合わせられる。 e.g., 3jで3行下\neは単語末じゃなくてそのひとつ前に移動するので気持ち悪い。\nカーソルの移動だけでなく、コピーなどの動作対象の指定にも使う。\nコピペ key action d カット y コピー P ペースト カットとコピーは対象を指定する必要がある。\nd3l # 右に3文字カット; 3dl でもいい diw # カーソル位置の単語をカット yy # 1行まるまるコピー pはカーソルの右に挿入される謎仕様なので却下。 Pも挿入後のカーソル位置が気持ち悪いけど仕方ない。\n範囲選択を見ながら操作したい場合は下記のヴィジュアルモードを使う。\nインサートモード -- INSERT -- キー入力しかしない。 カーソルキーで移動はできるが、基本的にはしないつもりで。\nノーマルモードへの戻り方 esc: 標準だが遠すぎるので却下。 ctrl-[: それなりに押しやすく、escと同じ挙動。 ctrl-c: 最も押しやすいし覚えやすい。 インサートモードでのあらゆる動作を中断して戻るので注意。 ヴィジュアルモード -- VISUAL -- v で始まる範囲選択モード。 Emacsでいうctrl-space。\nshift-v で行単位選択。 ctrl-v で矩形選択。\nGNU nano https://www.nano-editor.org/\n機能は控えめだが操作が平易なテキストエディタ。 ターミナル初心者に勧めやすいし、軽微なサーバー作業でも頼りになる。\n画面の下の方に保存や終了のコマンドが書いてあり、 emacsやviと比べて迷いにくい。 vimと同じかそれ以上に、OS標準装備として利用可能な場合が多い。 Installation Linux にも Mac にも最初からインストールされている。 それが古くてどうしても気に入らない場合は Homebrew とかで新しいのを入れる:\nbrew install nano nano -V Configuration 各種設定は ~/.nanorc ファイルで。 とはいえ、フルカスタムしたところで機能は高が知れているし、 動かなくなったら困るので最低限の設定にとどめる。\n",
  "href": "/dev/vi.html",
  "tags": [
   "editor"
  ],
  "title": "vi",
  "type": "dev"
 },
 {
  "content": "Githubが開発したオープンソースのGUIテキストエディタ。 ChromiumとNode.js(を用いたElectronフレームワーク)でできており、 どのOSでも同じように動作する。 Visual Studio Code や Codespaces の台頭により2022年に開発終了。\nhttps://atom.io https://flight-manual.atom.io/ brew install --cask atom Tips 検索に頼る とりあえずコマンドパレット cmd-shift-p で呼び出し、やりたいことを打ち込んでみる 文字列検索 cmd-f find-and-replace:show cmd-shift-f project-find:show (プロジェクト内をファイル横断で) 関数定義やセクションタイトルを検索 cmd-r or alt . symbols-view:toggle-file-symbols shift-cmd-r symbols-view:toggle-project-symbols ファイル検索 (プロジェクト内) cmd-t fuzzy-finder:toggle-file-finder その他 矩形(ブロック)選択 ctrl-shift-down / ctrl-shift-up MacではデフォルトでMission Controlに割り当てられてしまっているので システム環境設定からそれを解除しておく。 選択範囲を掴んで移動 ctrl-cmd-down コメントアウト、解除 cmd / editor:toggle-line-comments 閉じタグを挿入 cmd-alt . bracket-matcher:close-tag 小文字から大文字へ cmd-k-u editor:upper-case 大文字から小文字へ cmd-k-l editor:lower-case Tree view key command cmd \\ tree-view:toggle ctrl-0 tree-view:toggle-focus m tree-view:move d tree-view:duplicate a tree-view:add-file shift-a tree-view:add-folder 矢印キーはそのものでもEmacs/Vim系でも想像通りの挙動\nプロジェクト内のファイルを開きたいだけなら cmd-t でインクリメントサーチする癖をつけるほうが早い。\n環境設定 https://flight-manual.atom.io/using-atom/sections/basic-customization/ https://github.com/heavywatal/dotfiles/tree/master/.atom いつもの cmd , キーで設定画面を起動。\n設定ファイルは ~/.atom/ 以下に置かれる。 設定画面から\u0026quot;Open Config Folder\u0026quot;ボタンを押すとAtom内でそれらを開くことができる。 変更は即時反映される。\nconfig.cson Core Settings keymap.cson Keybindings デバッグしたいときは cmd . でKey Binding Resolverを起動するとよい。 snippets.cson 定型句に名前を付けておいて簡単に呼び出せるようにする。 https://flight-manual.atom.io/using-atom/sections/snippets/ styles.less エディタ本体も含めていろんな部分をCSS的にスタイル設定可能。 cmd-alt-i でWeb Inspectorを起動させればあらゆる要素を調べることができる。 カーソル位置のスコープを知りたいだけなら cmd-alt-p が簡便。\nパッケージ https://atom.io/packages https://atom.io/users/heavywatal/stars 環境設定のInstallメニューからインストールし、 Packagesメニューで管理する。\napm コマンドを利用してもよい。\napm install pigments apm uninstall pigments apm upgrade 開発版を使う ~/.atom/packages/ か ~/.atom/dev/packages/ にリポジトリを置けばよい。 後者は開発モードで起動した場合のみ読み込まれる。 別のところに置いといて apm link [--dev] path/to/local/repo でシムリンクを張る方法もある。\n",
  "href": "/dev/atom.html",
  "tags": [
   "editor",
   "writing"
  ],
  "title": "Atom",
  "type": "dev"
 },
 {
  "content": " 岩嵜 航 東北大学 生命科学研究科\n進化ゲノミクス分野 牧野研究室 特任助教 Watal M. Iwasaki, PhD Project Assistant Professor in Laboratory of Evolutionary Genomics,\nGraduate School of Life Sciences, Tohoku University Address: Biology bldg., Tohoku University, Aramaki Aoba 6-3, Sendai, 980-8578, Japan 980-8578 仙台市青葉区荒巻字青葉6-3 東北大学 理学部生物棟 Phone: +81-22-795-3543 Email: heavywatalあgmail.com Find me on Education 2013-09 Doctor of Life Sciences, Graduate School of Life Sciences, Tohoku University. (Prof. Masakado Kawata)\n\u0026ldquo;Evolution of Diversity and Complexity by Cryptic Variations in Gene Regulatory Networks\u0026rdquo; 2008-03 Bachelor of Science, Biological Institute, Faculty of Science, Tohoku University. (Prof. Masakado Kawata) Academic Positions 2019-09 – Current Project Assistant Professor in Makino Laboratory, Tohoku University. 2013-10 – 2019-08 Postdoctoral fellow in Innan Laboratory, SOKENDAI (The Graduate University for Advanced Studies). 2010-07 – 2013-03 JSPS Research Fellow (DC1) in Kawata Laboratory, Tohoku University. Publications Articles\nWatal M. Iwasaki*, T. E. Kijima* (equally contributed), Hideki Innan. (2020) Mol. Biol. Evol. 37(2) 355\u0026ndash;364; Population Genetics and Molecular Evolution of DNA Sequences in Transposable Elements. II. Accumulation of Variation and Evolution of a New Subfamily. Jeffrey A. Fawcett, Fumio Sato, Takahiro Sakamoto, Watal M. Iwasaki, Teruaki Tozaki, Hideki Innan. (2019) PLoS ONE 14(7): e0218407; Genome-wide SNP analysis of Japanese Thoroughbred racehorses Atsushi Niida, Watal M. Iwasaki, Hideki Innan. (2018) Mol. Biol. Evol. 35(6) 1316\u0026ndash;1321; Neutral Theory in Cancer Cell Population Genetics. Yusuke Okamoto, Watal M. Iwasaki, Kazuto Kugou, Kazuki Takahashi, Arisa Oda, Koichi Sato, Wataru Kobayashi, Hidehiko Kawai, Ryo Sakasai, Akifumi Takaori-Kondo, Takashi Yamamoto, Masato T. Kanemaki, Masato Taoka, Toshiaki Isobe, Hitoshi Kurumizaka, Hideki Innan, Kunihiro Ohta, Masamichi Ishiai, Minoru Takata. (2018) Nucleic Acids Research 46(6) 2932\u0026ndash;2944; Replication stress induces accumulation of FANCD2 at central region of large fragile genes. Satoshi Tamate*, Watal M. Iwasaki* (equally contributed), Kenneth L. Krysko, Brian Composano, Hideaki Mori, Ryo Funayama, Keiko Nakayama, Takashi Makino and Masakado Kawata. (2017) Sci. Rep. 7(18008); Inferring evolutionary responses of Anolis carolinensis introduced into the Ogasawara archipelago using whole genome sequence data. Watal M. Iwasaki and Hideki Innan. (2017) PLOS ONE 12(9): e0184229; Simulation Framework for Generating Intratumor Heterogeneity Patterns in a Cancer Cell Population. Takahiro Yamanoi and Watal M. Iwasaki. (2015) Evolution: Education and Outreach 8:14; Origami Bird Simulator: a teaching resource linking natural selection and speciation. Hajime Wakasa, Antonio Cádiz, Lázaro M. Echenique-Díaz, Watal M. Iwasaki, Namiko Kamiyama, Yuki Nishimura, Hitoshi Yokoyama, Koji Tamura, and Masakado Kawata. (2015) J. Exp. Zool. B Mol. Dev. Evol. 324 5: 410\u0026ndash;423; Developmental stages for the divergence of relative limb length between a twig and a trunk-ground Anolis lizard species. Tezuka, A., S. Kasagi, C. van Oosterhout, M. McMullan, Watal M. Iwasaki, D. Kasai, M. Yamamichi, H. Innan, S. Kawamura, and M. Kawata. (2014) Heredity 113: 381\u0026ndash;389; Divergent selection on opsin gene variation in guppy (Poecilia reticulata) populations of Trinidad and Tobago. Watal M. Iwasaki, Masaki E. Tsuda, Masakado Kawata. (2013) BMC Evol. Biol. 13:91; Genetic and environmental factors affecting cryptic variations in gene regulatory networks. Book Chapters\nAtsushi Niida and Watal M. Iwasaki \u0026ldquo;Agent-Based Modeling and Analysis of Cancer Evolution\u0026rdquo; in Simulation Modelling (working title; 2021-10 online-first) doi:10.5772/intechopen.100140, IntechOpen 印南秀樹、岩嵜航、新井田厚司 「がんゲノミクスと集団遺伝学の融合」 BIO Clinica 33:6 (2018-06) 小川誠司(編) 北隆館 岩嵜航、印南秀樹 「第3章 ゲノム進化メカニズムと情報学的解析」 ゲノムを司るインターメア \u0026mdash; 非コードDNAの新たな展開 (2015-11) 小林武彦(編) 化学同人 岩嵜航、木村幹子 「第6章 SRを高めるための教育、メディアとは」 社会的責任学入門—環境危機時代に適応する7つの教養 (2011-06) 東北大学生態適応GCOEチームPEM 東北大学出版会 Presentations Invited\nWatal M. Iwasaki and Hideki Innan \u0026ldquo;Comprehensive Framework for Evolutionary Simulation of Intratumor Heterogeneity\u0026rdquo; International Workshop for Systems Genetics 2018 (IWSG2018) (2018-11) Tokyo 岩嵜航「隠蔽変異を介して相互に促進される生命システムの複雑化と多様化」 第6回Evo-Devo青年の会 \u0026mdash; 新奇性の生まれるとき (2013-07) 東京大学三崎臨海実習所 岩嵜航・津田真樹・河田雅圭「環境と発生システムの相互作用が制限または増進する進化可能性」(自由集会「esj60w」嶋田正和・三浦徹) 日本生態学会第60回大会 (2013-03) 静岡 Watal M. Iwasaki, Masaki E. Tsuda, Masakado Kawata \u0026ldquo;Revealing Genetic and Environmental Factors that Affect Cryptic Variations in Gene Regulatory Networks by Individual-Based Model\u0026rdquo; RCIS International Conference (2012-11) Okayama University, Japan 岩嵜航・津田真樹・河田雅圭「遺伝子制御ネットワークが隠蔽する新奇形質」 個体群生態学会第28回大会 (2012-10) 東邦大学 Organizing\n岩嵜航「新奇形質獲得への裏口：ランダムな前適応と遺伝的同化」 日本進化学会第11回年会 (2009-09) 北海道大学 Oral\nWatal M. Iwasaki, Hideki Innan. \u0026ldquo;Evolution of Genetic Diversity within Tumors\u0026rdquo; 日本進化学会 第21回大会 (2019-08) 北海道大学 岩嵜航・印南秀樹「tumopp: 腫瘍内不均一性の進化シミュレーション」 日本遺伝学会 第89回大会 (2017-09) 岡山大学 岩嵜航・河田雅圭「隠蔽変異を介して相互に促進される生命システムの複雑化と多様化」 日本進化学会第15回年会 (2013-08) 筑波大学 岩嵜航・津田真樹・河田雅圭「遺伝子制御ネットワークの隠れた変異がもたらす表現型可塑性の多型」 日本進化学会第11回年会 (2009-09) 北海道大学 Poster\nWatal M. Iwasaki, Hideki Innan \u0026ldquo;Simulation of Intratumor Heterogeneity and its Medical Implication\u0026rdquo; POB-093, SMBE2018 (2018-07) Yokohama, Japan 岩嵜航・津田真樹・河田雅圭「遺伝子制御ネットワークの進化と隠蔽変異の蓄積に対する生息環境の効果」 日本進化学会第14回年会 (2012-08) 首都大学東京 岩嵜航・津田真樹・河田雅圭「Evolutionary Model of Cryptic Variations in Gene Regulatory Networks」 日本進化学会第13回年会 (2011-07) 京都大学 Watal M. Iwasaki, Masaki Tsuda, Masakado Kawata \u0026ldquo;Evolutionary Model of Cryptic Variations in Gene Regulatory Networks\u0026rdquo; Annual Meeting of Society for Molecular Biology and Evolution (SMBE2011) (2011-07) Kyoto University, Japan 岩嵜航・津田真樹・河田雅圭「環境変化で出現する新奇形質：遺伝子制御ネットワークの隠蔽変異が適応進化を促進」 日本生態学会第58回大会 (2011-03) 札幌 岩嵜航・津田真樹・河田雅圭「隠蔽変異を介した表現型多様性創出における選択圧の影響：遺伝子制御ネットワークの個体ベースモデル」 日本進化学会第12回年会 (2010-08) 東京工業大学 岩嵜航・津田真樹・河田雅圭「隠れた変異の蓄積と顕在化に与える環境変動の影響：遺伝子制御ネットワークの個体ベースモデル」 日本生態学会第57回大会 (2010-03) 東京大学 Watal M. Iwasaki, Masaki Tsuda, Masakado Kawata \u0026ldquo;Cryptic polymorphisms induced by environments in henotypes produced by gene regulatory networks.\u0026rdquo; International Symposium on Complex Systems Biology (2009-10) The University of Tokyo, Japan 岩嵜航・津田真樹・河田雅圭「隠れた遺伝的変異と可塑性から進化する新規表現型」 日本生態学会第56回大会 (2009-03) 岩手県立大学 岩嵜航・津田真樹・河田雅圭「遺伝子制御ネットワークの進化による新規形質獲得機構」 日本進化学会第10回年会 (2008-08) 東京大学 Grants and Fellowships 2019-04 – 2022-03 科学研究費補助金 基盤研究C 分担 (代表者: 新井田厚司)\n超並列がん進化シミュレーションによる腫瘍内不均一性生成機構の解明 2010-07 – 2013-03 JSPS Research Fellow (DC1) Awards 環境機関コンソーシアム ポスター賞(サラヤ賞) (2012-03) 日本生態学会第58回大会 ポスター優秀賞 (2011-03) 日本生態学会第58回大会 エコカップ ほのぼのMVP (2011-03) 環境機関コンソーシアム アカデミア部門ポスター賞(トロン賞) (2010-03) 東北大学生命科学研究科 生命科学会長賞 (2010-02) Teaching / Outreach 「Hands-on Introduction to R」Tohoku University (2023-11) 「統計モデリング概論 DSHC 2023」 東京海上 Data Science Hill Climb (2023-08) 「統計モデリング入門」 岩手大学 連合農学研究科 (2023-06) 「Rを用いたデータ解析の基礎」東北大学 進化学実習 (2023-04) 「統計モデリング入門 東京医科歯科大 データサイエンス人材育成プログラム (2023-03) 「逆順で理解する R Markdown Presentation」 Tokyo.R #102 (2022-10) 「Rによるデータ前処理実習」 東京医科歯科大 データサイエンス人材育成プログラム (2022-09) 「統計モデリング概論 DSHC 2022」 東京海上 Data Science Hill Climb (2022-08) 「Rを用いたデータ解析の基礎」東北大学 進化学実習 (2022-04) 「インタビューでGO!!」水の森市民センター企画講座 (2021-11) 北仙台小学校、北仙台中学校 「Rによるデータ前処理実習」 東京医科歯科大 データ関連人材育成プログラム (2021-10) 「確率分布を理解する統計モデリング入門」 東京大学 マルチNGSオミクス解析研究会 (2021-09) 「Rにやらせて楽しよう — データの可視化と下ごしらえ」 北海道大学 生命科学院 特別講義 (2021-09) 「統計モデリング概論 DSHC 2021」 東京海上 Data Science Hill Climb (2021-06) 「Rによるデータ前処理実習」 東京医科歯科大 データ関連人材育成プログラム (2020-10) 「インタビューでGO!! — 子どもたちの育ちを支えよう」水の森市民センター企画講座 (2020-08) 北仙台小学校 「Hands-on Introduction to R 2020」 東北大ほか (2020-05) 「Rによるデータ前処理実習」 東京医科歯科大 データ関連人材育成プログラム (2019-12) 「Rでデータを可視化する」 Sendai.R #2 (2019-09) Sendai \u0026ldquo;Using external C++ libraries in Rcpp packages\u0026rdquo; Japan.R 2018 (2018-12) Tokyo \u0026ldquo;Writing an R package interface to C++ libraries with Rcpp\u0026rdquo; Tokyo.R #71 (2018-07) Tokyo 「Rにやらせて楽しよう \u0026mdash; データの可視化と下ごしらえ」アドバンス生命理学特論 (告知ポスター) (2018-05) 名古屋大 「それもRにやらせよう \u0026mdash; 整然データの下ごしらえ」(自由集会「データ解析で出会う統計的問題: R の新しい作図・作表」粕谷英一・久保拓弥) 日本生態学会第65回大会 (2018-03) 札幌 「架空生物オリガミバードを作って進化が起こるしくみを体験しよう」(山野井貴浩、岩嵜航) サイエンスデイ (2012-07) 東北大学 「進化の仕組みを学べる補助教材: 架空生物オリガミバード [シミュレータ実演]」(山野井貴浩、岩嵜航、武村政春、佐倉統) 東北大学生態適応GCOE環境機関コンソーシアム (2012-03) 仙台国際センター Teaching Assistant in Kawata Laboratory, Tohoku University (2009–2013). Statistical analysis and data visualization. Teaching Assistant in 自然科学総合実験, 東北大学 (2008). 新入生交流会サポーター in 生命科学研究科, 東北大学 (2010–2013). Software Development tumopp: Tumor growth simulator in C++/R. PLOS ONE 12(9): e0184229 tekkamaki: Individual-based simulator of pacific bluefin tuna tek2: Population Genetic Simulation Framework of Transposable Elements mkrange: Simulator used in Bridle et al. Evol. Appl. 2019;00:1–14 igraphlite: Lightweight R Interface to igraph Network Analysis Library pomber: Interactive visualization of population genomic analysis of fission yeast. (新学術領域研究「ゲノムを支える非コードDNA領域の機能」) driftr.js: Genetic Drift Simulator oribir: Origami Bird Simulator. Evolution: Education and Outreach 8:14 msutils: Utilities for coalescent simulation with ms sfmt-class: SFMT installer and wrapper class for C++ Hugo theme for non-blog websites Hugo theme for reveal.js slides Academic Societies 日本進化学会 / Society of Evolutionary Studies, Japan 日本生態学会 (退会中) / Ecological Society of Japan 日本遺伝学会 (退会中) / The Genetics Society of Japan 個体群生態学会 (退会中) / The Society of Population Ecology Peer Review Genes \u0026amp; Genetic Systems, Journal of Molecular Evolution, Journal of Theoretical Biology, Mathematical Biosciences, Molecular Biology and Evolution, PLOS ONE, Scientific Reports\n",
  "href": "/about.html",
  "tags": null,
  "title": "About",
  "type": "home"
 },
 {
  "content": "メニューバーから実行可能にする ~/Library/Scripts/ にスクリプトを置く Script Editor.app を起動 cmd, Preferences\u0026hellip; ✅ Show Script menu in menu bar Music Equalizer イコライザを設定するスクリプト。 band 1 が低音で band 10 が高音。 下記の設定は Equal-loudness contour(等ラウドネス曲線) を考慮して 小さい音量でもシャカシャカせずそれなりのバランスで聴こえるようにした例。 耳に届きやすい 4kHz (band 8) らへんを落とすのが肝。 機器や音量や好みによって調整し、別々のスクリプトとして保存しておくとよい:\ntell application \u0026#34;Music\u0026#34; tell EQ preset \u0026#34;Manual\u0026#34; set band 1 to 1.5 set band 2 to 0.7 set band 3 to 0.3 set band 4 to 0.1 set band 5 to -0.1 set band 6 to -0.5 set band 7 to -1 set band 8 to -2 set band 9 to 0.3 set band 10 to 0.7 set preamp to 0 end tell set current EQ preset to EQ preset \u0026#34;Manual\u0026#34; end tell Finder: Get file path tell application \u0026#34;Finder\u0026#34; set the clipboard to POSIX path of (the selection as alias) end tell osascript: スクリプトをターミナルで実行 スクリプトを置いたディレクトリにパスを通せれば楽なんだけど。できるのかな？:\nosascript ${HOME}/Library/Scripts/Applications/Bibdesk/exportOOXML.scpt Bibdesk からGroupとTemplateを指定してExport 出力先、グループ名、テンプレートを最初の３行で指定し、Exportしたいライブラリを開いた状態で実行:\nset theOutFile to POSIX file \u0026#34;/Users/Iwasaki/Presentations/20100118Thesis/items1.xml\u0026#34; set theGroup to \u0026#34;20100118 Thesis\u0026#34; set theTemplateName to \u0026#34;OOXML template\u0026#34; using terms from application \u0026#34;BibDesk\u0026#34; tell application \u0026#34;BibDesk\u0026#34; set theDoc to first document set thePubs to publication of static group theGroup of theDoc tell theDoc export for thePubs using template theTemplateName to theOutFile end tell end tell end using terms from ",
  "href": "/mac/applescript.html",
  "tags": [
   "mac"
  ],
  "title": "AppleScript",
  "type": "mac"
 },
 {
  "content": "https://apptainer.org/\nDocker のようなもの。 管理者権限なしで実行できるので一般ユーザーに使わせやすい。 ホスト機のファイル読み書きもデフォルトでやりやすい。 イメージはOCI形式ではなくSIF形式。 拡張子は .sif が標準的で .simg も無印も見かける。 SingularityだったものがLinux Foundationへの移管に伴って改名。 紛らわしいことにSylabs社がSingularity CEと称しているforkはとりあえず無視。\nAdmin https://apptainer.org/docs/admin/latest/\nInstallation https://apptainer.org/docs/admin/latest/installation.html\nLinux sudo add-apt-repository -y ppa:apptainer/ppa sudo apt update sudo apt install -y apptainer setuid版というのはかなり古いkernel向けっぽいのでとりあえず無視。\nMac kernelが違うのでネイティブには動かない。 今のところDocker Desktop的なものも無いので自分で仮想環境を用意する必要があるらしい。\nConfiguration https://apptainer.org/docs/admin/latest/configfiles.html\napptainer help apptainer version apptainer buildcfg apptainer remote list /etc/singularity や /usr/local/etc/singularity が残ってると怒られるので消す。 singularity コマンドは互換性のためのエイリアスとしてしばらく提供されるっぽいけど。\nUser https://apptainer.org/docs/user/latest/\nCLI https://apptainer.org/docs/user/latest/cli.html\npull apptainer pull [pull options...] [output file] \u0026lt;URI\u0026gt; SIFイメージをダウンロードする。 OCIイメージだったら build を呼んでSIFに変換する。 e.g., apptainer pull docker://alpine すると alpine_latest.sif ができる。 exec apptainer exec [exec options...] \u0026lt;container\u0026gt; \u0026lt;command\u0026gt; コマンドを実行する。 container引数にはSIFファイルへのパスを渡せる。 docker exec との違い: containerが先に走っている必要はない。 --rm も不要。 --mount 無しでも /home/$USER, /tmp, $PWD がbindされる。 -it 無しでも自然な入出力。 run container内のrunscriptを実行する。 runscriptは /apptainer (/singularity) に置かれたシェルスクリプトで、 exec と同様にコマンドを実行できるようになっていることが多そう。 shell container内でシェルを起動する。 exec や run で bash するのとどう違うか？ Repositories docker:// Docker向けregistryからpullしてSIFに変換。 まともなSIF registryが見当たらない現状では結局これが楽ちん。 shub:// Singularity Hub 2021-04 に凍結。 DataLad に引き継がれた既存イメージはpullできるらしい。 library:// Sylabs Singularity library Apptainerデフォルトではオフ。 ",
  "href": "/dev/apptainer.html",
  "tags": [
   "package"
  ],
  "title": "Apptainer",
  "type": "dev"
 },
 {
  "content": "環境に合わせて Makefile を作る仕組み。 以下の3つのツールを組み合わせて使う。\nhttps://www.gnu.org/software/autoconf/ https://www.gnu.org/software/automake/ https://www.gnu.org/software/libtool/ CMake のほうがモダンで高速。\nCommands autoscan 指定したディレクトリ(指定しなければカレント)のソースコードを読んで configure.ac の雛形となる configure.scan を作る。 既存の configure.ac もチェックするらしいが、その内容をすべて configure.scan に反映してくれるわけではなさそうなので そのまま上書きしてはダメっぽい。 aclocal configure.ac を読んで aclocal.m4 を作る automake Makefile.am と configure.ac から Makefile.in を作る autoconf configure.ac と aclocal.m4 から configure を作る autoreconf 上記のツールをいい感じに繰り返し呼び出して各種ファイルを更新 大まかな流れ configure.scan の雛形を自動生成し、 configure.ac に名前変更:\nautoscan mv configure.scan configure.ac configure.ac を適宜編集\nMakefile.am を作る\nその2つのファイルから、自動的にその他のファイルを生成:\nautoreconf --install できあがった configure ファイルを試してみる:\n./configure --help ./configure make configure.ac や Makefile.am を変更したら autoreconf で反映させる、を繰り返す\nconfigure.ac https://www.gnu.org/software/autoconf/manual/html_node/\nconfigure.ac の基本構造: https://www.gnu.org/software/autoconf/manual/html_node/Autoconf-Input-Layout.html 標準マクロ: https://www.gnu.org/software/autoconf/manual/html_node/Autoconf-Macro-Index.html M4マクロ: https://www.gnu.org/software/autoconf/manual/html_node/M4-Macro-Index.html Gitのタグをバージョン番号として取り込む:\nAC_INIT([MyApp], m4_esyscmd([git describe --tags | tr -d '\\n'])) Makefile.am https://www.gnu.org/software/automake/manual/html_node/\nマクロ: https://www.gnu.org/software/automake/manual/html_node/Macro-Index.html 変数: https://www.gnu.org/software/automake/manual/html_node/Variable-Index.html インストールするファイルと場所を指定する変数:\nbin_PROBRAMS = beer bin_SCRIPTS = beer.sh lib_LIBRARIES = libbeer.a include_HEADERS = beer.h ビルドするのに必要な情報をターゲットごとに指定する変数。 target_ARGNAME のような形をとる:\nbeer_SOURCES = main.cpp beer_CPPFLAGS = -DNDEBUG beer_CXXFLAGS = -O3 libbeer_a_SOURCES = lib.cpp Makefile 全体に関わる変数。 ただし上記のターゲット特異的変数に上書きされる:\nAM_CPPFLAGS = -Wall -Wextra AM_CXXFLAGS = -O2 ユーザーが指定する CPPFLAGS は beer_CPPFLAGS や AM_CPPFLAGS を上書きせず、 後ろに並べて使用される。\nhttps://www.gnu.org/software/automake/manual/html_node/Flag-Variables-Ordering.html\n",
  "href": "/dev/autotools.html",
  "tags": [
   "package"
  ],
  "title": "autoconf, automake",
  "type": "dev"
 },
 {
  "content": " https://www.bioconductor.org http://blog.hackingisbelieving.org/2010/09/r-package_06.html https://qiita.com/tags/bioconductor http://manuals.bioinformatics.ucr.edu/home/R_BioCondManual http://search.bioconductor.jp/ 基本操作 インストール https://www.bioconductor.org/install/\nBioconductor 3.8 から BiocManager を使う方法に変わった。 source(\u0026quot;*/biocLite.R\u0026quot;) や biocLite() はもう使わない。\nR本体をインストール\nRの中でコマンドを実行:\ninstall.packages(\u0026#34;BiocManager\u0026#34;) BiocManager::install(c(\u0026#34;Biostrings\u0026#34;, \u0026#34;GenomicRanges\u0026#34;, \u0026#34;rtracklayer\u0026#34;)) パッケージ管理 標準の関数ではなく BiocManager を使う:\n# バージョンなどを確認 BiocManager::valid() # インストール済みのものを更新 BiocManager::install() # 使いたいパッケージを入れる BiocManager::install(\u0026#34;edgeR\u0026#34;) BiocManager::install(\u0026#34;VariantAnnotation\u0026#34;) インストールしたものを使うときには普通と同じように読み込む:\nlibrary(edgeR) 一覧 https://www.bioconductor.org/packages/release/bioc/\nhttps://www.bioconductor.org/packages/release/data/annotation/\nhttps://www.bioconductor.org/packages/release/data/experiment/ 検索 https://www.bioconductor.org/packages/release/BiocViews.html 使い方を調べる\nhelp(package=\u0026#34;Biostrings\u0026#34;) browseVignettes(package=\u0026#34;Biostrings\u0026#34;) Biostrings https://www.bioconductor.org/packages/release/bioc/html/Biostrings.html\nクラス 配列 BString\nDNAString\nRNAString\nAAString 配列セット e.g. ある遺伝子の複数のcds BStringSet\nDNAStringSet\nRNAStringSet\nAAStringSet 配列セットリスト e.g. 遺伝子ごとの配列セット cdsBy(txdb, by=\u0026quot;gene\u0026quot;) BStringSetList\nDNAStringSetList\nRNAStringSetList\nAAStringSetList 多重整列 DNAMultipleAlignment\nRNAMultipleAlignment\nAAMultipleAlignment ペアワイズ整列 PairwiseAlignments このコンストラクタを直接呼び出してはいけない。 pairwiseAlignment() 関数を使うべし。 Views (x, start=NULL, end=NULL, width=NULL, names=NULL) 「ある配列のこの領域とこの領域」を表現したいとき、 配列を切り出すのではなく、その切り取り方のみを保持する。 IRanges みたいな感じ。 as.character() でその部分配列を文字列として取り出せる。 PDict (x, max.mismatch=NA, ...) 正規表現 TAA|TAG|TGA のように複数の配列で検索する場合にまとめておく。 matchPDict() とかで使う。 デモデータ\n.file = system.file(\u0026#34;extdata\u0026#34;, \u0026#34;someORF.fa\u0026#34;, package=\u0026#34;Biostrings\u0026#34;) bss = readDNAStringSet(.file) 関数 配列変換 reverse(bs)\ncomplement(bs)\nreverseComplement(bs) transcribe(dna) \u0026mdash; deprecated\n3\u0026rsquo;-鋳型鎖=アンチセンス鎖-5\u0026rsquo; を引数として相補的な 5\u0026rsquo;-mRNA-3\u0026rsquo; を返す。非推奨。\ntranslate(bs, genetic.code=GENETIC_CODE, if.fuzzy.codon=\u0026quot;error\u0026quot;)\n翻訳して AAString に変換。 引数はDNAでもRNAでもよい。 genetic.code は名前付き文字列ベクタで、 組み込みで使えるやつは getGeneticCode(\u0026quot;SGC2\u0026quot;) のように指定できる。 何がどの名前で使えるかは GENETIC_CODE_TABLE で確認。\ncodons(bs)\n3塩基ずつ切って見る Views オブジェクトを返す。\nsubseq(bs, start, end)\u0026lt;-\n部分置換\nxscat(...)\nBstring 版 paste0()\n頻度を数える alphabetFrequency(x, as.prob=FALSE, ...)\nletterFrequency(x, letters, OR=\u0026quot;|\u0026quot;, as.prob=FALSE, ...)\ndinucleotideFrequency(x, ...)\ntrinucleotideFrequency(x, ...)\noligonucleotideFrequency(x, width, ...)\nnucleotideFrequencyAt(set_or_views, at) コンセンサス consensusMatrix(set_or_views)\nconsensusString(set_or_views, ambiguityMap, threshold, ...) 配列ファイルを読む、書く (FASTA, FASTQ) readBStringSet(file, format=\u0026quot;fasta\u0026quot;, ...)\nreadDNAStringSet(file, format=\u0026quot;fasta\u0026quot;, ...)\nreadRNAStringSet(file, format=\u0026quot;fasta\u0026quot;, ...)\nreadAAStringSet(file, format=\u0026quot;fasta\u0026quot;, ...)\nwriteXStringSet(x, filepath, append=FALSE, compress=FALSE, format=\u0026quot;fasta\u0026quot;, ...) アラインメントを読む (FASTA, stockholm, clustal) readDNAMultipleAlignment()\nreadRNAMultipleAlignment()\nreadAAMultipleAlignment() 配列を読み込まずに情報だけを見る fasta.info(file, ...)\nfastq.geometry(file, ...) 検索 matchPattern(pattern, subject, ...)\nmatchPDict(PDict, subject, ...)\nmatchPWM(pwm, subject, min.score=\u0026quot;80%\u0026quot;, widh.score=FALSE, ...)\n対象は Bstring だけでなく Views でもよい。 例えば codons() の結果を対象とすれば読み枠限定の検索となる。 結果の返し方の違う vmatchXXX() と countXXX() もある。 GenomicRanges See \u0026ldquo;IRanges and GenomicRanges\u0026rdquo;.\nGenomicFeatures https://www.bioconductor.org/packages/release/bioc/html/GenomicFeatures.html\nhttps://qiita.com/yuifu/items/4bab5f713aa75bd18a84\nTranscriptDB からいろんな条件で絞り込み、 該当する区間を GRanges または GRangesList で返す。\n絞り込んで GRanges を返す transcripts(txdb, vals=NULL, columns)\nvals=list(tx_chrom=c(\u0026quot;chrI\u0026quot;, \u0026quot;chrV\u0026quot;)) のように指定する。 取りうる値: gene_id, tx_id, tx_name, tx_chrom, tx_strand, exon_id, exon_name, exon_chrom, exon_strand, cds_id, cds_name, cds_chrom, cds_strand, exon_rank\nexons(txdb, ...)\ncds(txdb, ...)\ngenes(txdb, ...)\npromoters(txdb, upstream=2000, downstream=200, ...)\ndisjointExons(txdb, aggregateGenes=FALSE, includeTranscripts=TRUE, ...)\nmicroRNAs(txdb)\ntRNAs(txdb)\nlibrary(FDb.UCSC.tRNAs) の情報を使ってtRNAコーディング部分を抽出 グループ毎に絞り込んで GRangesList を返す transcriptsBy(txdb, by, use.names=FALSE, ...)\nby は \u0026quot;gene\u0026quot;, \u0026quot;exon\u0026quot;, \u0026quot;cds\u0026quot;, \u0026quot;tx\u0026quot; のどれか。\nexonsBy(txdb, by, ...)\ncdsBy(txdb, by, ...)\nintronsByTranscript(txdb, ...)\nfiveUTRsByTranscript(txdb, ...)\nthreeUTRsByTranscript(txdb, ...) extractTranscriptSeqs(s, transcripts) DNAString または BSgenome から配列を抜き出す。 transcripts 引数は GRanges, GRangesList, TranscriptDb のどれか。 TranscriptDB の形式変換 makeTranscriptDbFromBiomart(biomart=\u0026quot;ensembl\u0026quot;, dataset=\u0026quot;hsapiens_gene_ensembl\u0026quot;, ...)\nmakeTranscriptDbFromUCSC(genome=\u0026quot;hg18\u0026quot;, table=\u0026quot;knownGene\u0026quot;, ...)\nmakeTranscriptDBFromGFF(file, format=c(\u0026quot;gff3\u0026quot;, \u0026quot;gtf\u0026quot;), ...)\nasBED(txdb)\nasGFF(txdb) TranscriptDB データのインストールと読み込み\nBiocManager::install(\u0026#34;TxDb.Scerevisiae.UCSC.sacCer3.sgdGene\u0026#34;) library(\u0026#34;TxDb.Scerevisiae.UCSC.sacCer3.sgdGene\u0026#34;) txdb = TxDb.Scerevisiae.UCSC.sacCer3.sgdGene BSgenome https://www.bioconductor.org/packages/release/bioc/html/BSgenome.html\nインストール、利用\nBiocManager::install(\u0026#34;BSgenome.Scerevisiae.UCSC.sacCer3\u0026#34;) library(\u0026#34;BSgenome.Scerevisiae.UCSC.sacCer3\u0026#34;) bsg = BSgenome.Scerevisiae.UCSC.sacCer3 クラス BSgenome @organism\n@species\n@provider\n@provider_version\n@release_date\n@release_name\n@source_url\n@seqinfo\n@user_seqnames\n@masks\n@single_sequences\n@multiple_sequences\n@pkgname ほか 染色体(DNAString)にアクセス\nbsg$chrI bsg[[\u0026#34;chrI\u0026#34;]] bsg[[1]] BSParams @X\n@FUN\n@exclude\n@simplify\n@maskList\n@motifList\n@userMask\n@invertUserMask new()() で作って bsapply()() に渡すらしい\nGenomeData\nGenomeDescription\n関数 アクセサー organism(bsg)\nspecies(bsg)\nprovider(bsg)\nproviderVersion(bsg)\nreleaseDate(bsg)\nreleaseName(bsg)\nbsgenomeName(bsg)\nseqlengths() などseqinfo系関数も使える 利用可能なデータを調べる available.genomes(splitNameParts=FALSE, type=getOption(\u0026quot;pkgType\u0026quot;))\ninstalled.genomes(splitNameParts=FALSE) getBSgenome(name, masked=FALSE) インストール済みデータから読み込み。 BSgenome.Scerevisiae.UCSC.sacCer3 あるいは sacCer3 のような名前で。 getSeq(bsg, names, start=NA, end=NA, width=NA, starnd=\u0026quot;+\u0026quot;, as.character=FALSE) BSgenome オブジェクトから配列を抜き出す。 names は配列名の文字列ベクタか GRanges か GRangesList データパッケージを作る https://www.bioconductor.org/packages/release/bioc/vignettes/BSgenome/inst/doc/BSgenomeForge.pdf\n染色体ごとのFASTAファイルを用意する e.g. Ensemblから *.chromosome.*.fa.gz をダウンロードして展開\n染色体名と拡張子だけを残したファイル名にする必要がある。 しかもgzipのままでは読めない残念な仕様なので展開しておく。 e.g. IV.fa\n既存の BSgenome のやつを参考に適当にseedファイルを作る:\nPackage: BSgenome.Scerevisiae.EF4.74.ensembl Title: BSgenome.Scerevisiae.EF4.74.ensembl Description: BSgenome.Scerevisiae.EF4.74.ensembl Version: 0.74.1 organism: Saccharomyces_cerevisiae species: Scerevisiae provider: ENSEMBL provider_version: release-74 release_date: 2013-11-25T21:47:17 release_name: EF4.74 source_url: ftp://ftp.ensembl.org/pub/release-74/fasta/Saccharomyces_cerevisiae/dna/ organism_biocview: Saccharomyces_cerevisiae BSgenomeObjname: BSgenome.Scerevisiae.EF4.74.ensembl seqnames: c(\u0026quot;I\u0026quot;, \u0026quot;II\u0026quot;, \u0026quot;III\u0026quot;, \u0026quot;IV\u0026quot;, \u0026quot;IX\u0026quot;, \u0026quot;Mito\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;VI\u0026quot;, \u0026quot;VII\u0026quot;, \u0026quot;VIII\u0026quot;, \u0026quot;X\u0026quot;, \u0026quot;XI\u0026quot;, \u0026quot;XII\u0026quot;, \u0026quot;XIII\u0026quot;, \u0026quot;XIV\u0026quot;, \u0026quot;XV\u0026quot;, \u0026quot;XVI\u0026quot;) circ_seqs: c(\u0026quot;Mito\u0026quot;) SrcDataFiles1: Saccharomyces_cerevisiae.EF4.74.dna.chromosome.I.fa.gz, Saccharomyces_cerevisiae.EF4.74.dna.chromosome.II.fa.gz, Saccharomyces_cerevisiae.EF4.74.dna.chromosome.III.fa.gz, Saccharomyces_cerevisiae.EF4.74.dna.chromosome.IV.fa.gz, Saccharomyces_cerevisiae.EF4.74.dna.chromosome.IX.fa.gz, Saccharomyces_cerevisiae.EF4.74.dna.chromosome.Mito.fa.gz, Saccharomyces_cerevisiae.EF4.74.dna.chromosome.V.fa.gz, Saccharomyces_cerevisiae.EF4.74.dna.chromosome.VI.fa.gz, Saccharomyces_cerevisiae.EF4.74.dna.chromosome.VII.fa.gz, Saccharomyces_cerevisiae.EF4.74.dna.chromosome.VIII.fa.gz, Saccharomyces_cerevisiae.EF4.74.dna.chromosome.X.fa.gz, Saccharomyces_cerevisiae.EF4.74.dna.chromosome.XI.fa.gz, Saccharomyces_cerevisiae.EF4.74.dna.chromosome.XII.fa.gz, Saccharomyces_cerevisiae.EF4.74.dna.chromosome.XIII.fa.gz, Saccharomyces_cerevisiae.EF4.74.dna.chromosome.XIV.fa.gz, Saccharomyces_cerevisiae.EF4.74.dna.chromosome.XV.fa.gz, Saccharomyces_cerevisiae.EF4.74.dna.chromosome.XVI.fa.gz from ftp://ftp.ensembl.org/pub/release-74/fasta/Saccharomyces_cerevisiae/dna/ PkgExamples: genome$I # same as genome[[\u0026quot;I\u0026quot;]] seqs_srcdir: /Users/watal/db/ensembl/release-74/fasta/saccharomyces_cerevisiae/dna パッケージ名に使える文字は結構限られてるので注意\nR で forgeBSgenomeDataPkg(seedfile) を実行\nできあがったパッケージディレクトリをビルドしてインストール:\nR CMD build BSgenome.Scerevisiae.EF4.74.ensembl R CMD INSTALL BSgenome.Scerevisiae.EF4.74.ensembl VariantAnnotation https://www.bioconductor.org/packages/release/bioc/html/VariantAnnotation.html\nクラス VCF, CollapsedVCF, ExpandedVCF @assays\n@colData\n@exptData\n@fixed DataFrame\n$REF: 参照配列の塩基 ref()\n$ALT: 変異配列の塩基 alt()\n$QUAL\n$FILTER\n@info DataFrame\n$TSA: SNV, deletion, insertion\n$VE: *_incl_consequences.vcf.gz の追加情報\n@rowData\n変異の位置情報 GRanges。 rowData() でアクセスすると @fixed の情報込みで表示される。 関数 readVcf(file, genome, param) 予めターミナルで tabix -h some_file.vcf.gz を実行して indexファイル some_file.vcf.gz.tbi を作っておく。 file には生のファイル名ではなく Rsamtools::TabixFile を渡す。 genome は sacCer3 みたいな名前か Seqinfo を指定。 param に GRanges などを入れると範囲限定で読み込む。 染色体の名前(seqlevels)が合ってないと怒られるので修正する。 writeVcf(obj, filename, index=FALSE) index=TRUE とすると bgzip 圧縮と tabix 生成を試みるが失敗する。 predictCoding(query, subject, seqSource, varAllele, ..., ignore.strand=FALSE) それぞれの変異がCDSやprotein上のどの位置でどういう変化を引き起こすか というのを計算して GRanges のmetadataに書き出す。 (引数の型判別がバグってるっぽいので vcf@rowData, txdb, bsgenome, alt(vcf) とする)\nquery : VCF あるいは rowData(vcf)\nsubject : TxDb オブジェクト\nseqSource : BSgenome オブジェクト\nvarAllele : 省略あるいは alt(vcf) データ読み込み、取得 rtracklayer\nbiomart\nGEOquery\nモチーフ検索 http://blog.hackingisbelieving.org/2012/02/dna-bioconductor.html\n/bio/motif\n作図 http://blog.hackingisbelieving.org/2012/02/rbioconductor.html\nhttp://blog.hackingisbelieving.org/2012/02/r.html\nhttps://qiita.com/wakuteka/items/a99d5fb9f24367f55461\n",
  "href": "/rstats/bioconductor.html",
  "tags": [
   "r",
   "bioconductor"
  ],
  "title": "Bioconductor",
  "type": "rstats"
 },
 {
  "content": "https://www.bioconductor.org/packages/release/bioc/html/biomaRt.html\nhttps://qiita.com/yuifu/items/a757629506c1cd98156b\nBioMartからプログラマチックにデータを取得するための Bioconductor 拡張。 ウェブアプリ MartView のGUIでひととおり慣れておくと良い。\nクラス Mart @biomart \u0026mdash; ensembl @host \u0026mdash; http://www.biomart.org:80/biomart/martservice\n@dataset \u0026mdash; scerevisiae_gene_ensembl\n@filters $name \u0026mdash; chromosome_name, biotype, \u0026hellip;\n$options \u0026mdash; [ncRNA,protein_coding,pseudogene,rRNA,snoRNA,snRNA,tRNA], \u0026hellip;\n$operation \u0026mdash; =, \u0026gt;=, only,excluded, \u0026hellip;\n$description\n$fullDescription\n\u0026hellip; @attributes $name \u0026mdash; ensembl_gene_id, entrezgene, \u0026hellip;\n$page \u0026mdash; feature_page, structure, homologs, snp, sequences\n$description\n$fullDescription\n\u0026hellip; 関数 listMarts(mart, host=\u0026quot;www.biomart.org\u0026quot;, ..., archive=FALSE, ...) 利用可能なマート列挙。 e.g. ensembl, fungi_mart_21, unimart listDatasets(mart, verbose=FALSE) マートで利用可能なデータセット列挙。 e.g. scerevisiae_gene_ensembl attributePages(mart) マートで利用可能なデータのカテゴリ分け(ページ)を列挙。 unique(mart@attributes$page) と同じ。 データを取得するときはこれらの間を跨がないようにアトリビュートを選ぶ。 listAttributes(mart, page, what=c(\u0026quot;name\u0026quot;, \u0026quot;desciption\u0026quot;)) マートで利用可能なアトリビュート列挙。 mart@attributes へのアクセサ。 listFilters(mart, what=c(\u0026quot;name\u0026quot;, \u0026quot;description\u0026quot;)) マートで利用可能なフィルター列挙。 mart@filters へのアクセサ。 useMart(martname, dataset, host=\u0026quot;www.biomart.org\u0026quot;, ..., archive=FALSE, ...) 利用するマート(とデータセット)を指定して Mart オブジェクトを作る。 データセットを決めずにも作れる。 e.g. mart = useMart(\u0026quot;ensembl\u0026quot;, \u0026quot;scerevisiae_gene_ensembl\u0026quot;) useDataset(dataset, mart, verbose=FALSE) データセットを決めた Mart オブジェクトを作る。 でもこれって useMart() にもできるので不要\u0026hellip;？ e.g. mart = useDataset(\u0026quot;scerevisiae_gene_ensembl\u0026quot;, mart) getBM(attributes, filters=\u0026quot;\u0026quot;, values=\u0026quot;\u0026quot;, mart, curl=NULL, checkFilters=TRUE, verbose=FALSE, uniqueRows=TRUE, bmHeader=FALSE) このパッケージのメイン関数。 フィルターは名前付きリストで渡す。 filter に渡すと AND 結合: getBM(c(\u0026quot;ensembl_gene_id\u0026quot;, \u0026quot;sgd_gene\u0026quot;), filters=list(chromosome_name=\u0026quot;IV\u0026quot;, biotype=\u0026quot;tRNA\u0026quot;), mart=ensembl) # 第四染色体上のtRNA遺伝子 values に渡すと OR 結合:\ngetBM(c(\u0026quot;ensembl_gene_id\u0026quot;, \u0026quot;sgd_gene\u0026quot;), values=list(chromosome_name=\u0026quot;IV\u0026quot;, biotype=\u0026quot;tRNA\u0026quot;), mart=ensembl) # 第四染色体上の遺伝子とtRNA遺伝子 このへんとか useDataset() らへんとか、 インターフェイスがあまり洗練されてない印象だなぁ\u0026hellip;。\ngetSequence(chromosome, start, end, id, type, seqType, upstream, downstream, mart) Sequencesページのデータをダウンロードすることに特化した getBM() ラッパー。 結果は2列(配列とID)の data.frame。\nseqType: gene_exon, transcript_exon, transcript_exon_intron, gene_exon_intron, cdna, coding, coding_transcript_flank, coding_gene_flank, transcript_flank, gene_flank, peptide, 3utr, 5utr getGene(id, type, mart) IDを指定して遺伝子情報をダウンロードすることに特化した getBM() ラッパー。 得られるdata.frameは9列: 指定したID, external_gene_id, description, chromosome_name, band, strand, start_position, end_position, ensembl_gene_id select(mart, keys, columns, keytype) getBM() を生のSQLっぽくした感じ。 keytype(フィルター)は1つしか使えない。 dplyr::select() と名前が衝突する。 columns(mart) mart@attributes$name と同じ keytypes(mart) mart@filters$name と同じ keys(mart, keytype) subset(mart@filters, name==keytype)$options をちゃんと要素ごとに切った文字列ベクタで。 exportFASTA(sequences, file)\n使用例 Ensembl マートとデータセットの決定\nlibrary(biomaRt) ensembl = useMart(\u0026#34;ensembl\u0026#34;, \u0026#34;scerevisiae_gene_ensembl\u0026#34;) ## あるいは listMarts() ensembl = useMart(\u0026#34;ensembl\u0026#34;) listDatasets(ensembl) ensembl = useDataset(\u0026#34;scerevisiae_gene_ensembl\u0026#34;, ensembl) どんなデータやフィルタが利用可能か調べる\nattributePages(ensembl) listAttributes(ensembl, \u0026#34;feature_page\u0026#34;) subset(ensembl@filters, select=c(name, description, type, operation)) フィルタの選択肢を調べる\n\u0026gt; subset(ensembl@filters, name==\u0026#34;biotype\u0026#34;)$options [1] \u0026#34;[ncRNA,protein_coding,pseudogene,rRNA,snoRNA,snRNA,tRNA]\u0026#34; \u0026gt; keys(ensembl, \u0026#34;biotype\u0026#34;) [1] \u0026#34;ncRNA\u0026#34; \u0026#34;protein_coding\u0026#34; \u0026#34;pseudogene\u0026#34; \u0026#34;rRNA\u0026#34; \u0026#34;snoRNA\u0026#34; \u0026#34;snRNA\u0026#34; \u0026#34;tRNA\u0026#34; \u0026gt; keys(ensembl, \u0026#34;go_evidence_code\u0026#34;) [1] \u0026#34;IBA\u0026#34; \u0026#34;IC\u0026#34; \u0026#34;IDA\u0026#34; \u0026#34;IEA\u0026#34; \u0026#34;IEP\u0026#34; \u0026#34;IGI\u0026#34; \u0026#34;IMP\u0026#34; \u0026#34;IPI\u0026#34; \u0026#34;ISA\u0026#34; \u0026#34;ISM\u0026#34; \u0026#34;ISS\u0026#34; \u0026#34;NAS\u0026#34; \u0026#34;ND\u0026#34; \u0026#34;TAS\u0026#34; 近いミラーを使う\n\u0026gt; listMarts(host=\u0026#34;asia.ensembl.org\u0026#34;) biomart version 1 ENSEMBL_MART_ENSEMBL Ensembl Genes 75 2 ENSEMBL_MART_SNP Ensembl Variation 75 3 ENSEMBL_MART_FUNCGEN Ensembl Regulation 75 4 ENSEMBL_MART_VEGA Vega 55 5 pride PRIDE (EBI UK) \u0026gt; ensembl = useMart(\u0026#34;ENSEMBL_MART_SNP\u0026#34;, \u0026#34;scerevisiae_snp\u0026#34;, host=\u0026#34;asia.ensembl.org\u0026#34;) UniProt https://www.uniprot.org/\nフィルタ列挙\n\u0026gt; unimart = useMart(\u0026#34;unimart\u0026#34;, \u0026#34;uniprot\u0026#34;) \u0026gt; subset(unimart@filters, select=c(name, description, type, operation)) name description type operation 1 superregnum_name Superregnum name list = 2 proteome_name Complete proteome list = 3 accession Accession text = 4 protein_name Protein text = 5 length_greater Length \u0026gt; text \u0026gt; 6 length_smaller Length \u0026lt; text \u0026lt; 7 protein_evidence Protein existence list = 8 embl_id EMBL IDs id_list =,in 9 arrayexpress_id ArrayExpress IDs id_list =,in 10 ensembl_id Ensembl IDs id_list =,in 11 pdbsum_id PDBSum IDs id_list =,in 12 intact_id IntAct IDs id_list =,in 13 interpro_id InterPro IDs id_list =,in 14 go_id Gene Ontology IDs id_list =,in 15 gene_name Gene name text = 16 entry_type Entry type list = 17 organelle organelle list = 18 plasmid_f Plasmid text = フィルタの選択肢\n\u0026gt; subset(unimart@filters, 3 \u0026lt; nchar(options) \u0026amp; nchar(options) \u0026lt; 120, select=c(name, options)) name options 1 superregnum_name [Eukaryota,Bacteria,Archaea,Viruses] 7 protein_evidence [1: Evidence at protein level,2: Evidence at transcript level,3: Inferred from homology,4: Predicted,5: Uncertain] 16 entry_type [Swiss-Prot,TrEMBL] \u0026ldquo;Complete proteome\u0026rdquo; の選択肢(すげえ長い)を抜き出す\nproteome_name = biomaRt::keys(unimart, \u0026#34;proteome_name\u0026#34;) grep(\u0026#34;Sac.* cer.*\u0026#34;, proteome_name, value=TRUE) アトリビュート列挙 (ただし embl_id 以降の項目はほとんど使えない)\n\u0026gt; listAttributes(unimart) name description 1 accession Accession 2 name Entry name 3 protein_name Protein name 4 gene_name Gene name 5 organism Organism 6 protein_evidence Protein existence 7 entry_type Status 8 go_id GO ID 9 go_name GO name 10 db2go_p__dm_primary_id GO ID(p) 11 db2go_p__dm_description GO name 12 db2go_f__dm_description GO name (F) 13 db2go_f__dm_primary_id GO ID (F) 14 db2go_c__dm_primary_id GO ID (C) 15 db2go_c__dm_description GO name (C) 16 embl_id EMBL IDs 17 ensembl_id Ensembl IDs 18 interpro_id InterPro IDs 19 pdbsum_id PDBSum IDs 20 pdb_id PDB IDs 21 arrayexpress ArrayExpress IDs 22 pride_id PRIDE IDs 23 interact_id IntAct IDs 24 comments Comments 25 ec_number Ec number 26 keyword Keyword 27 plasmid_name Plasmid name 28 organelle_name organelle name ",
  "href": "/rstats/biomart.html",
  "tags": [
   "r",
   "bioconductor"
  ],
  "title": "biomaRt",
  "type": "rstats"
 },
 {
  "content": "https://biopython.org https://biopython.org/DIST/docs/api/Bio-module.html\nSeqIO https://biopython.org/wiki/SeqIO https://biopython.org/DIST/docs/tutorial/Tutorial.html#sec51 https://biopython.org/DIST/docs/api/Bio.SeqIO-module.html read(file, format, alphabet=None) 1配列しか含まないファイルを読んで SeqRecord を返す parse(file, format, alphabet=None) 複数の配列を含むファイルを読んで SeqRecord のイテレータを返す index(filename, format, alphabet=None, key_function=None) id をキーとした辞書likeオブジェクトを返す write(sequences, file, format)\nto_dict(sequences, key_function=None)\nconvert(in_file, in_format, out_file, out_format, alphabet=None)\nfrom Bio import SeqIO with open('beer.fasta', 'r') as fin: for record in SeqIO.parse(fin, 'fasta'): print(record.id) print(record.seq) SeqRecord https://biopython.org/wiki/SeqRecord https://biopython.org/DIST/docs/tutorial/Tutorial.html#sec32 https://biopython.org/DIST/docs/api/Bio.SeqRecord.SeqRecord-class.html Seq とそのほかの情報をひとまとまりにしたクラス。\nid\nseq\nname\ndescription\ndbxrefs\nfeatures\nannotations\nletter_annotations\n\u0026gt;\u0026gt;\u0026gt; for record in SeqIO.parse(fin, 'fasta'): ... print(record) ... ID: gi|186972394|gb|EU490707.1| Name: gi|186972394|gb|EU490707.1| Description: gi|186972394|gb|EU490707.1| Selenipedium aequinoctiale maturase K (matK) gene, partial cds; chloroplast Number of features: 0 Seq('ATTTTTTACGAACCTGTGGAAATTTTTGGTTATGACAATAAATCTAGTTTAGTA...GAA', SingleLetterAlphabet()) ID: gi|186972391|gb|ACC99454.1| Name: gi|186972391|gb|ACC99454.1| Description: gi|186972391|gb|ACC99454.1| maturase K [Scaphosepalum rapax] Number of features: 0 Seq('IFYEPVEILGYDNKSSLVLVKRLITRMYQQKSLISSLNDSNQNEFWGHKNSFSS...EEE', SingleLetterAlphabet()) Seq https://biopython.org/wiki/Seq https://biopython.org/DIST/docs/tutorial/Tutorial.html#sec17 https://biopython.org/DIST/docs/api/Bio.Seq.Seq-class.html 塩基配列・アミノ酸配列のクラス。 標準 str とほとんど同じように扱えるほか、 相補鎖とかなんとかを簡単に扱えるメソッドが備わっている。\ncomplement(self) reverse_complement(self) transcribe(self) back_transcribe(self) translate(self, table=\u0026quot;Standard\u0026quot;, stop_symbol=\u0026quot;*\u0026quot;, to_stop=False, cds=False) ungap(self, gap=None) GenomeDiagram https://biopython.org/DIST/docs/tutorial/Tutorial.html#sec328 https://biopython.org/DIST/docs/GenomeDiagram/userguide.pdf\nEntrez https://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc90 esearch NCBI ESerch Utility\nhttps://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc93\n\u0026gt;\u0026gt;\u0026gt; from Bio import Entrez \u0026gt;\u0026gt;\u0026gt; handle = Entrez.esearch(db=\u0026#34;nucleotide\u0026#34;, retmax=10, term=\u0026#34;Opuntia\u0026#34;) \u0026gt;\u0026gt;\u0026gt; record = Entrez.read(handle) \u0026gt;\u0026gt;\u0026gt; for item in record.items(): ... print(item) ... (u\u0026#39;Count\u0026#39;, \u0026#39;390\u0026#39;) (u\u0026#39;RetMax\u0026#39;, \u0026#39;10\u0026#39;) (u\u0026#39;IdList\u0026#39;, [\u0026#39;257359511\u0026#39;, \u0026#39;283467266\u0026#39;, \u0026#39;246905625\u0026#39;, \u0026#39;246905624\u0026#39;, \u0026#39;246655205\u0026#39;, \u0026#39;246655204\u0026#39;, \u0026#39;240253899\u0026#39;, \u0026#39;240253897\u0026#39;, \u0026#39;240253576\u0026#39;, \u0026#39;240253574\u0026#39;]) (u\u0026#39;TranslationStack\u0026#39;, [{u\u0026#39;Count\u0026#39;: \u0026#39;200\u0026#39;, u\u0026#39;Field\u0026#39;: \u0026#39;Organism\u0026#39;, u\u0026#39;Term\u0026#39;: \u0026#39;\u0026#34;Opuntia\u0026#34;[Organism]\u0026#39;, u\u0026#39;Explode\u0026#39;: \u0026#39;Y\u0026#39;}, {u\u0026#39;Count\u0026#39;: \u0026#39;390\u0026#39;, u\u0026#39;Field\u0026#39;: \u0026#39;All Fields\u0026#39;, u\u0026#39;Term\u0026#39;: \u0026#39;Opuntia[All Fields]\u0026#39;, u\u0026#39;Explode\u0026#39;: \u0026#39;Y\u0026#39;}, \u0026#39;OR\u0026#39;, \u0026#39;GROUP\u0026#39;]) (u\u0026#39;TranslationSet\u0026#39;, [{u\u0026#39;To\u0026#39;: \u0026#39;\u0026#34;Opuntia\u0026#34;[Organism] OR Opuntia[All Fields]\u0026#39;, u\u0026#39;From\u0026#39;: \u0026#39;Opuntia\u0026#39;}]) (u\u0026#39;RetStart\u0026#39;, \u0026#39;0\u0026#39;) (u\u0026#39;QueryTranslation\u0026#39;, \u0026#39;\u0026#34;Opuntia\u0026#34;[Organism] OR Opuntia[All Fields]\u0026#39;) efetch NCBI EFetch Utility https://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc96 返り値は結果(XML)へのハンドル。:\nfrom Bio import Entrez handle = Entrez.efetch(db=\u0026quot;nucleotide\u0026quot;, id=\u0026quot;186972394,186972394\u0026quot;, rettype=\u0026quot;fasta\u0026quot;) record = SeqIO.parse(handle, \u0026quot;fasta\u0026quot;) for x in record: print(i) Installation pip で一発:\npip install biopython 書籍 ",
  "href": "/python/biopython.html",
  "tags": [
   "python"
  ],
  "title": "BioPython",
  "type": "python"
 },
 {
  "content": "Basic Local Alignment Search Tool\nhttps://blast.ncbi.nlm.nih.gov/doc/blast-help/downloadblastdata.html\nblast+ のインストール Homebrew を使って brew install blast するのが楽チン。 以下に紹介するのはそれ以外の茨の道。\nLinux, Mac (binary) https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ から最新版の ncbi-blast-*-x64-linux.tar.gz あるいは ncbi-blast-*-universal-macosx.tar.gz をダウンロードして展開:\nwget -O- ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ncbi-blast-2.2.30+-x64-linux.tar.gz | tar xz しかるべきところに移動してシムリンク:\nsudo mv ncbi-blast-2.2.30+ /usr/local/ sudo ln -s /usr/local/ncbi-blast-2.2.30+ /usr/local/ncbi-blast あとは /usr/local/ncbi-blast/bin にパスを通して使う。\nMac (source) ncbi-blast-*.dmg や ncbi-blast-*-universal-macosx.tar.gz は古いGCCでUniversalビルドされてるっぽいので、 自分のマシンに最適化されるようにビルドしてみる。 ホントに速いかどうかは試してない。\nhttps://www.ncbi.nlm.nih.gov/books/NBK279690/#CmdLineAppsManual.Installation\nBoost ライブラリをインストールする。このとき regex, spirit, system, filesystem, test, thread をビルド対象に含める。 cf. Boost\nftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST から最新版の ncbi-blast-*-src.tar.gz をダウンロード\n展開して移動:\ntar xzf ncbi-blast-2.2.30+-src.tar.gz cd ncbi-blast-2.2.30+-src/c++/ ncbi-blast-*-universal-macosx.tar.gz に入ってる ncbi_package_info を参考に configure して make:\n./configure --without-debug --with-strip --without-pcre --with-mt --with-flat-makefile --with-64 --with-ncbi-public --without-ccache --without-caution --without-makefile-auto-update --with-projects=scripts/projects/blast/project.lst --with-internal --prefix=/usr/local/ncbi-blast --with-boost=/usr/local/boost-gcc CC=gcc-4.9 CXX=g++-4.9 clang (および Xcode のニセ gcc) では configure が通らないので本物の gcc を Homebrew などでインストールしておく。\nそのまま make してもダメらしいので Makefile.mk の -std=gnu++11 を消す:\nsed -i.orig -e 's/-std=[a-z0-9+]\\+//' ReleaseMT/build/Makefile.mk ビルドしてインストール:\nmake sudo make install 共通設定 .zshenv 等でプログラムとデータベースのパスを設定:\nexport PATH=/usr/local/ncbi-blast/bin:${PATH} export BLASTDB=${HOME}/db/blast ~/.ncbirc でも BLASTDB を指定できるっぽいけど 普通にシェルの環境変数を使うほうがわかりやすいし、 makeblastdb の出力先として参照できるので便利。\n確認:\nrehash blastn -version ローカルデータベース構築 -subject オプションを使えばFASTAファイル同士でいきなり検索できるが、 何度もやるならデータベース化しておいたほうが効率いいはず。\nhttps://www.ncbi.nlm.nih.gov/books/NBK279684/#appendices.T.makeblastdb_application_opt\nデータベースの置き場所をひとつ決め (e.g. ~/db/blast)、 上記のように環境変数 BLASTDB を設定しておく。\nFASTA形式の配列データを用意する (e.g. ftp://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/ から yeast.aa.gz)\nmakeblastdb コマンドでBLASTデータベース形式に変換:\nmakeblastdb -in yeast.nt -out ${BLASTDB}/yeast.nt -dbtype nucl -parse_seqids -hash_index makeblastdb -in yeast.aa -out ${BLASTDB}/yeast.aa -dbtype prot -parse_seqids -hash_index 圧縮ファイルは直接読めないので、展開して標準入力に流し込む。 このとき -title と -out は必須の引数になる:\ngunzip -c mydata.fa.gz | makeblastdb -in - -title mydata -out ${BLASTDB}/mydata -dbtype nucl -hash_index -in (stdin)\n-dbtype (prot) prot or nucl -title (入力ファイル名) どういうときに使われるか不明。 標準入力を使う場合は必須オプション。 -parse_seqids 配列名が gi|129295 みたいな特定の形式になってる場合にうまいことやる -hash_index よくわからんけど検索のスピードアップに繋がるっぽいオプション -out (入力ファイル名) 出力先のベースネーム。 標準入力を使う場合は必須オプション。 -taxid\nプログラム実行 共通オプション https://www.ncbi.nlm.nih.gov/books/NBK279684/#appendices.T.options_common_to_all_blast\n-db\n-query (stdin)\n-out (stdout)\n-evalue (10.0) これを下回るE値を持つものだけ集める。 -max_target_seqs (500) アラインされた配列をいくつまで保持して報告するか？ これを1にして得られる結果がベストヒットとは限らないことに注意。 -subject データベース化してないFASTAファイルを検索対象として直接指定。 ただし -num_threads を指定しても並列化できない。 -num_threads (1)\n-outfmt (0) 0 = pairwise,\n1 = query-anchored showing identities,\n2 = query-anchored no identities,\n3 = flat query-anchored, show identities,\n4 = flat query-anchored, no identities,\n5 = XML Blast output,\n6 = tabular,\n7 = tabular with comment lines,\n8 = Text ASN.1,\n9 = Binary ASN.1\n10 = Comma-separated values\n11 = BLAST archive format (ASN.1) TSV/CSVテーブル(6, 7, 10)の場合は出力内容を細かく指定できる。デフォルトは -outfmt \u0026quot;7 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore\u0026quot;\nqseqid: Query Seq-id\nqgi: Query GI\nqacc: Query accesion\nsseqid: Subject Seq-id\nsallseqid: All subject Seq-id(s), separated by a \u0026lsquo;;\u0026rsquo;\nsgi: Subject GI\nsallgi: All subject GIs\nsacc: Subject accession\nsallacc: All subject accessions\nqstart: Start of alignment in query\nqend: End of alignment in query\nsstart: Start of alignment in subject\nsend: End of alignment in subject\nqseq: Aligned part of query sequence\nsseq: Aligned part of subject sequence\nevalue: Expect value\nbitscore: Bit score\nscore: Raw score\nlength: Alignment length\npident: Percentage of identical matches\nnident: Number of identical matches\nmismatch: Number of mismatches\npositive: Number of positive-scoring matches\ngapopen: Number of gap openings\ngaps: Total number of gap\nppos: Percentage of positive-scoring matches\nframes: Query and subject frames separated by a \u0026lsquo;/\u0026rsquo;\nqframe: Query frame\nsframe: Subject frame\nbtop: Blast traceback operations (BTOP)\nstaxids: unique Subject Taxonomy ID(s), separated by a \u0026lsquo;;\u0026rsquo;(in numerical order)\nsscinames: unique Subject Scientific Name(s), separated by a \u0026lsquo;;\u0026rsquo;\nscomnames: unique Subject Common Name(s), separated by a \u0026lsquo;;\u0026rsquo;\nsblastnames: unique Subject Blast Name(s), separated by a \u0026lsquo;;\u0026rsquo; (in alphabetical order)\nsskingdoms: unique Subject Super Kingdom(s), separated by a \u0026lsquo;;\u0026rsquo; (in alphabetical order)\nstitle: Subject Title\nsalltitles: All Subject Title(s), separated by a \u0026lsquo;\u0026lt;\u0026gt;\u0026rsquo;\nsstrand: Subject Strand\nqcovs: Query Coverage Per Subject\nqcovhsp: Query Coverage Per HSP\nblastn Nucleotide query vs Nucleotide subject\nhttps://www.ncbi.nlm.nih.gov/books/NBK279684/#appendices.T.blastn_application_options\n-word_size (11, short: 7, mega: 28) 最初にexact matchさせる配列の長さ。 短いほどいろんな開始地点から探せるが、遅くなる。 -gapopen (5, mega: 0)\n-gapextend (2, mega: none)\n-reward (2, short: 1, mega: 1)\n-penalty (-3, mega: -2)\n-perc_identity (0)\n-ungapped\nblastp Protein query vs Protein subject https://www.ncbi.nlm.nih.gov/books/NBK279684/#appendices.T.blastp_application_options blastx Nucleotide query (translated) vs Protein subject https://www.ncbi.nlm.nih.gov/books/NBK279684/#appendices.T.blastx_application_options tblastn Protein query vs Nucleotide subject (translated) https://www.ncbi.nlm.nih.gov/books/NBK279684/#appendices.T.tblastn_application_options tblastx Nucleotide query (translated) vs Nucleotide subject (translated) https://www.ncbi.nlm.nih.gov/books/NBK279684/#appendices.T.tblastx_application_options ",
  "href": "/bio/blast.html",
  "tags": [
   "genetics"
  ],
  "title": "BLAST",
  "type": "bio"
 },
 {
  "content": " https://www.boost.org/ https://www.boost.org/doc/libs/release/ https://boostjp.github.io/ Installation パッケージマネージャで Homebrew で最新版を簡単にインストールできる。 オプションは適当に:\nbrew install boost --layout=tagged でビルドされるため、 リンクするときは末尾に -mt が必要になる。\nソースから https://www.boost.org/doc/libs/release/more/getting_started/unix-variants.html https://www.boost.org/build/ https://boostjp.github.io/howtobuild.html https://www.boost.org/users/download/ から最新ソースを入手して展開。\nwget -O- https://dl.bintray.com/boostorg/release/1.67.0/source/boost_1_67_0.tar.bz2 | tar xj cd boost_1_67_0/ ビルドすべきライブラリを考える ./bootstrap.sh --show-libraries\n適当なオプションを与えて bootstrap.sh を実行:\n./bootstrap.sh --help ./bootstrap.sh --without-icu --with-libraries=context,filesystem,graph,iostreams,program_options,serialization,system,test 設定が project-config.jam に書き出され、 b2 がビルドされる。 ./b2 --help\n~/user-config.jam に [ツールセットを定義] (https://www.boost.org/build/doc/html/bbv2/reference/tools.html)。 darwinはMac-gcc用:\nusing gcc : 14 : g++-8 : \u0026lt;compileflags\u0026gt;-fPIC \u0026lt;cxxflags\u0026gt;-std=c++14 ; using darwin : 14 : g++-8 : \u0026lt;compileflags\u0026gt;-fPIC \u0026lt;cxxflags\u0026gt;-std=c++14 ; using clang : 14 : clang++ : \u0026lt;compileflags\u0026gt;-fPIC \u0026lt;cxxflags\u0026gt;-std=c++14 -stdlib=libc++ \u0026lt;linkflags\u0026gt;-stdlib=libc++ ; gccとclangの両方から使える統一ライブラリを作るのは難しいらしいので、 それぞれのコンパイラで別々にビルドしてインストールする。\nシステム標準zlibをリンクしようとしてエラーになるような場合は、 zlib公式からソースを落として展開し、 [一緒にビルドされるように] (https://www.boost.org/doc/libs/release/libs/iostreams/doc/installation.html) ZLIB_SOURCEをフルパス指定する。\nwget -O- https://zlib.net/zlib-1.2.8.tar.gz | tar xz -C ${HOME}/tmp/build export ZLIB_SOURCE=${HOME}/tmp/build/zlib-1.2.8 ツールセットを指定してビルド:\n./b2 -j2 toolset=gcc-14 link=static,shared runtime-link=shared threading=multi variant=release --layout=tagged --build-dir=../b2gcc --stagedir=stage/gcc stage ./b2 -j2 toolset=darwin-14 link=static,shared runtime-link=shared threading=multi variant=release --layout=tagged --build-dir=../b2gcc --stagedir=stage/gcc stage ./b2 -j2 toolset=clang-14 link=static,shared runtime-link=shared threading=multi variant=release --layout=tagged --build-dir=../b2clang --stagedir=stage/clang stage prefixを指定してインストール:\n./b2 -j2 toolset=gcc-14 link=static,shared runtime-link=shared threading=multi variant=release --layout=tagged --build-dir=../b2gcc --stagedir=stage/gcc --prefix=${HOME}/local install あるいは手動でインストール:\nrsync -auv stage/gcc/ ~/local/boost-gcc rsync -auv stage/clang/ ~/local/boost-clang rsync -auv boost ~/local/include 使うとき インストールした場所とリンクするライブラリをコンパイラに伝える必要がある。 コマンドを直打ちするなら:\nclang++ -I${HOME}/local/include -L${HOME}/local/lib mysource.cpp -lboost_iostreams-mt -o a.out Makefileの変数でいうと:\nCPPFLAGS = -I${HOME}/local/include LDFLAGS = -L${HOME}/local/lib LDLIBS = -lboost_iostreams-mt CMakeでは BOOST_ROOT にprefixを指定:\ncmake -DBOOST_ROOT=${HOME}/local .. math https://www.boost.org/doc/libs/release/libs/math/\ndistribution https://www.boost.org/doc/libs/release/libs/math/doc/html/dist.html\n確率分布に従った乱数生成はC++11から \u0026lt;random\u0026gt; でサポートされるようになったが、 確率密度関数(PDF)や累積密度関数(CDF)はまだ標準入りしてない。\n// #include \u0026lt;boost/math/distributions.hpp\u0026gt; #include \u0026lt;boost/math/distributions/normal.hpp\u0026gt; #include \u0026lt;boost/math/distributions/poisson.hpp\u0026gt; #include \u0026lt;boost/math/distributions/binomial.hpp\u0026gt; #include \u0026lt;boost/math/distributions/chi_squared.hpp\u0026gt; namespace bmath = boost::math; bmath::normal_distribution\u0026lt;\u0026gt; dist(mean, sd); bmath::mean(dist); bmath::median(dist); bmath::standard_deviation(dist); bmath::pdf(dist, x); bmath::cdf(dist, x); bmath::quantile(dist, p); 右側の裾が欲しいときは精度を保つために complement() を使う。 (Rでいう lower.tail=FALSE)\n// good bmath::quantile(bmath::complement(dist, p)); // bad bmath::quantile(dist, 1.0 - p) // good bmath::cdf(bmath::complement(dist, x)); // bad 1.0 - bmath::cdf(dist, x); iostreams https://www.boost.org/doc/libs/release/libs/iostreams/doc/\n要ビルド＆リンク -lboost_iostreams-mt\ngzip 圧縮と展開 #include \u0026lt;iostream\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;boost/iostreams/filtering_stream.hpp\u0026gt; #include \u0026lt;boost/iostreams/filter/gzip.hpp\u0026gt; #include \u0026lt;boost/iostreams/device/file_descriptor.hpp\u0026gt; int main() { namespace bios = boost::iostreams; { bios::filtering_ostream ost; ost.push(bios::gzip_compressor()); ost.push(bios::file_descriptor_sink(\u0026#34;hello.txt.gz\u0026#34;)); ost \u0026lt;\u0026lt; \u0026#34;Hello world!\u0026#34;; } { bios::filtering_istream ist; ist.push(bios::gzip_decompressor()); ist.push(bios::file_descriptor_source(\u0026#34;hello.txt.gz\u0026#34;)); std::string buffer; std::getline(ist, buffer, \u0026#39;\\0\u0026#39;); std::cout \u0026lt;\u0026lt; buffer \u0026lt;\u0026lt; std::endl; } } deviceを push() した時点でchain completeになるので、 先にfilterを push() する必要がある。 コンストラクタにfilterを渡してもよい。 file_descriptor はfailビットが立つとすぐ例外を投げて \u0026ldquo;No such file or directory\u0026rdquo; などを知らせてくれるので便利。 標準streamのような沈黙を求める場合は代わりに std::ifstream などを push() する。 ファイル名に応じてフィルタを切り替えるようなクラスを定義しておけば、 透過的に読み書きできる。e.g., wtl::zfstream program_options https://www.boost.org/doc/libs/release/libs/program_options/\n要ビルド＆リンク -lboost_program_options-mt\ncf. getopt\ncoroutine2 https://www.boost.org/doc/libs/release/libs/coroutine2/\n要ビルド＆リンク -lboost_context-mt\nPythonのyieldみたいなことをC++でもできるようになる。\nexample of Fibonacci generator\nオブジェクトの寿命に注意。\nyield返しはmoveなので、 次の処理で再利用するつもりならコピーコンストラクタ越しに新品を返す。 generator的なものを返す関数を作ると、 それを抜ける時に寿命を迎えるオブジェクトがあることに注意。 そのほか \u0026lt;boost/multiprecision/cpp_int.hpp\u0026gt;\n\u0026lt;boost/dynamic_bitset.hpp\u0026gt;\n",
  "href": "/cxx/boost.html",
  "tags": [
   "c++"
  ],
  "title": "Boost",
  "type": "cxx"
 },
 {
  "content": "理想 ビルド不要でヘッダ1つ 標準ライブラリのみに依存していてポータブル ヘルプを自動生成してくれる オプション定義時に格納先の変数を紐付けできる (argc, argv) だけでなく std::string とかからも読み込める 読み込める形式で全ての値を書き出せる マクロではなくtemplateやlambdaなど真っ当なC++ (できればC++11以降の簡潔なスタイル) で書ける GNU getopt http://www.gnu.org/s/libc/manual/html_node/Getopt.html\nUNIX的な環境ならインストール不要だがC/C++標準ではない ヘルプなど自動生成してくれない C++というよりCなので手作業が多い boost::program_options http://www.boost.org/doc/html/program_options.html\n格納先の変数を紐付け可能 ファイルからも読み込める 読み込み可能なファイルの出力方法は用意されてないので自分で書く必要がある ビルドとリンクが必要で大掛かり cf. boost gflags https://gflags.github.io/gflags/\nテンプレートではなくマクロをふんだんに使って実装されているのでちょっと怖い main() 関数まわりをほとんど変更することなく、 各ソースファイルで自由にオプションを定義できる(しかもたった1行で) 接頭辞 FLAGS_ のついた変数が自動的に定義されて、そこに値が格納される 入力可能なファイルを出力することも可能 要ビルド＆リンク Usage main() 関数に書く必要があるのはこれだけ\n#include \u0026lt;gflags/gflags.h\u0026gt; int main(int argc, char* argv[]) { gflags::SetUsageMessage(\u0026#34;This is a program to test gflags\u0026#34;); gflags::ParseCommandLineFlags(\u0026amp;argc, \u0026amp;argv, true); // do something return 0; } あとは個々のソースファイルでオプションを追加。namespace にも入れられる。\n#include \u0026lt;gflags/gflags.h\u0026gt; namespace tapiola { DEFINE_uint64(sibelius, 0, \u0026#34;string that is displayed with --help flag\u0026#34;); } void func(){ std::cout \u0026lt;\u0026lt; tapiola::FLAGS_sibelius \u0026lt;\u0026lt; std::endl; } cmdline https://github.com/tanakh/cmdline\nhttp://d.hatena.ne.jp/tanakh/20091028\nヘッダファイル1つ 直感的でC++らしいデザインなので分かりやすい demangle機能のためにポータビリティが犠牲に std::string から読める 変数に直接格納することはできず、パーサのメソッドで値を取得: template \u0026lt;class T\u0026gt; const T \u0026amp;parser::get(const std::string \u0026amp;name) TCLAP http://tclap.sourceforge.net/\n\u0026ldquo;Templatized C++ Command Line Parser Library\u0026rdquo; の名のとおり template で書かれておりヘッダだけで構成される が、configure と make install というインストール手順を踏む std::string からパース可能 格納する変数は指定できず、*Arg オブジェクトの getValue() メソッドで値を取得 getoptpp http://code.google.com/p/getoptpp/\nstd::istream っぽく \u0026gt;\u0026gt;operator を使う 格納する変数を指定できる 基本的にはライブラリをビルドして使うが、ちょっといじればヘッダの #include だけでも使える ファイルの読み込みやヘルプの生成は一切手伝ってくれない clipp https://github.com/muellan/clipp 理念がしっかりしていて、かなり柔軟に使える。\nそのまま使うには少し難しかったり、 値を一括して取得する機能が欠けたりという問題はある。 nlohmann/json を使ってそのへんをうまくやる補助ライブラリ clippson を作って利用中。 そのほか Githubで上位に出てくるこれらもそのうち試したい:\nhttps://github.com/docopt/docopt.cpp ヘルプを自動生成するのではなく、ヘルプからパーサを構築する 元々はPython用に作られ、それから多言語に移植されてる実績 boolかstd::stringでゲットするしかないので、手動でキャストして代入 std::vector\u0026lt;std::string\u0026gt;から読める 要ビルド＆リンク (header-only化しようとしてる雰囲気はある) https://github.com/jarro2783/cxxopts Lightweight C++ command line option parser parse(argc, argv) だけ https://github.com/Taywee/args 名前空間が args という大胆さ 同じ名前を何回も書かなきゃいけないような、少々やぼったいインターフェイス ヘッダ1つ、ヘルプ自動生成なのは良い https://github.com/adishavit/argh A minimalist argument handler. ハイフンの数を区別できないし、ヘルプ自動生成も無い。 ",
  "href": "/cxx/getopt.html",
  "tags": [
   "c++"
  ],
  "title": "C++コマンドライン引数",
  "type": "cxx"
 },
 {
  "content": "はじめに 速いプログラムで得られるメリットを超えるようなコストを払わないように。 まずは動くプログラムを書いて目的を達成することが大事。 自分律速じゃなくてプログラム律速だなと感じた段階でリファクタリングを考える。 プログラム本来の意図が読み取れなくなりそうなマニアックな高速化は避ける。 清く正しくメンテナンスしやすいプログラムを書くほうが結局は生産的。 学習目的でない限り、車輪の再発明を避ける。 やろうとしていることはきっと既に誰かが実現し、 再利用可能な形で公開してくれているはず。 まずは標準ライブラリとかBoostを探してみる。 あとGitHubでスターが多いやつとか。 頑張れコンパイラ Intelの icc でビルドされたプログラムは速いらしい。 gcc や clang の最適化技術も着々と進歩しており、 新しいコンパイラを使うほうがその恩恵を受けられる。\n最適化オプション https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html\ng++ -O3 main.cpp コンパイル時定数 constexpr コンパイル時に計算できるものを予め計算して定数にしておくことで、 実行時の計算を減らすことができる。 C++11では明示的に constexpr 修飾することができるので積極的に使うべし。 テンプレートメタプログラミングという手もある。 C++11を使えない場合でも、 定数同士の四則演算などはまとめておけばコンパイラが計算してくれるはず。\n// 掛け算も割り算も毎回計算 return 4.0 * M_PI * std::pow(radius, 3) / 3.0; // コンパイル時定数を掛けるだけ constexpr double c = 4.0 * M_PI / 3.0; return c * std::pow(radius, 3); また、四則演算の中でも割り算は特に遅いらしいので、 逆数のかけ算になるような書き方のほうが高速になるかもしれない。\nインライン展開 関数の呼び出しにはコストがかかり、 コードが短くて何度も呼ばれる関数では特にそのコストの割合が高くて馬鹿にできない。 呼び出される予定のところにコンパイルの段階で関数を展開してやること (=インライン展開) でそのコストを無くせる。\nファイルをまたいで使う関数の場合は定義宣言を ソース.cpp ではなく ヘッダ.h に置き、頭に inline を付ける。 メンバ関数の場合はクラス定義の中で定義宣言にするだけでよい。 ただしそのように書いてもコンパイラへのヒントになるだけで、 インライン展開に必要でも十分でもない。\n-O2 では -finline-small-functions がオンになり、 -O3 では -finline-functions がオンになる。 実行ファイルのサイズがデカくなるし、コンパイルは遅くなるので、バランスを考える。\ninline int pow2(int x) { return x *= x; } 関数オブジェクト 関数オブジェクトとは、operator() を定義した class または struct のこと。 std::for_each() や std::transform() の最後の引数に関数を渡す場合など、 何度も呼び出される小さい関数については、 普通の関数や関数ポインタで渡すよりも関数オブジェクトを使ったほうがよい。 コンパイラによる最適化がしやすいらしい。 メンバ変数として値を保持できるので、 引数のやり取りやメモリの確保・開放が少なくてすむという場面もありそう。\nclass isAboveThreshold { public: isAboveThreshold(const double t): threshold_(t); bool operator()(const double x) const { return x \u0026gt; threshold_; } private: const double threshold_; }; std::transform(v.begin(), v.end(), result.begin(), isAboveThreshold(3.14)); C++11からはラムダ式が便利\nconst double threshold = 3.14; std::transform(v.begin(), v.end(), result.begin(), [threshold](double x) -\u0026gt; bool {return x \u0026gt; threshold;}); 余計な一時オブジェクトを作らない 普通にプログラムを書くと、思わぬところで余計な一時オブジェクトが作られることになる。 メモリもcpu時間ももったいないので、なるべく避けよう。\nconst参照渡し or ポインタ渡し オブジェクトの値渡しにはコピーのコストが生じるので、 STLコンテナや自作クラスなどは参照渡しのほうが高速。 ただし参照を解決するコストも無いわけではないので、 intやdoubleのような単純な型はむしろ普通の値渡しがよい。\n参照渡しされた仮引数の中身を関数内で変更すると、 呼び出し元の実引数の値も変更されることになる。 変更するつもりがない場合は明示的に const をつけて受け取るようにすると安全\nvoid do_something(const std::vector\u0026lt;int\u0026gt;\u0026amp; huge_array) { // コピーせず本体への参照のみ受け取る。 // constがついているので、変更しようとするとコンパイルエラー } 引数の中身を変更するつもりがある場合はポインタを受け取るようにすると、 呼び出すときに \u0026amp; が必要となるので意図が伝わりやすい。\nvoid do_something(std::vector\u0026lt;int\u0026gt;* huge_array) { // コピーせずポインタを受け取る。 // huge_arrayの変更は呼び出し元にも影響する。 } do_something(\u0026amp;some_array); // ああ、この配列は変更されるんだな ムーブセマンティクス (C++11) 受け取ったものを関数の中で変更して返すような関数を作るとき、 引数の渡し方も受け取り方も3パターンずつ考えられる。\n渡し方 lvalue (vec0) xvalue (std::move(vec0)) prvalue (Vector{}) 受け取り方 const lvalue参照 (const Vector\u0026amp;) lvalue値 (Vector) const lvalue参照 (const Vector\u0026amp;) とrvalue参照 (Vector\u0026amp;\u0026amp;) のオーバーロード それぞれの組み合わせでコピーとムーブが何回起こるかテストするコード\nconst lvalue参照で受け取ったものを変更するためにはコピーが必要になるので、 普通のlvalue渡しでは問題ないけどrvalue渡しには適さない。 rvalue参照受け取りでオーバーロードすると、 コピーせず1回のムーブで済ませられる。 ただし引数が増えると手に負えなくなる。 lvalue値受け取りの関数はrvalueを受け取るときにはコピーを生じない。 オーバーロードの場合と比べてムーブが余計に1回生じるが、 そのコストと定義の手軽さを天秤に掛けて考慮する価値はある。\nreturn最適化にはムーブすらしない\u0026quot;copy elision\u0026quot;とムーブの2種類があって、 ローカル変数やprvalueを返す場合は両方を試み、 引数を返す場合は後者のみを試みるらしい。\n関数から値を返すときにムーブ返ししたくなるところだが、 多くの場合コンパイラがうまいことやってくれるのでわざわざ return std::move(output); などと書く必要はない。 やってしまうと、むしろコンパイラによる最適化を妨げるかもよと警告される。 ただし、返る変数と関数定義で型が一致せず暗黙の型変換が挟まる場合は、 明示的にムーブ返しする必要がある。\n関連記事はたくさん見つかるが、特に読みやすく参考になったのはこちら:\n本当は怖くないムーブセマンティクス - yohhoyの日記（別館） 参照渡し or 値渡し？ - yohhoyの日記\nnoexcept クラスに自前のデストラクタやコピーコンストラクタを書くと、 暗黙のムーブコンストラクタが作られなくなり、 ただ移動したいときにもコピーが行われてしまう。 自前のムーブコンストラクタを書いても、 noexcept が添えられていないと std::move_if_noexcept() による移動 (例えば std::vector のリアロケーション) ではコピーされてしまう。\nムーブコンストラクタの定義の仕方でコピー・ムーブのされ方が変わることを確かめるコード\nコンストラクタ類を必要に応じて書き足していくと忘れそうなので、 Cls (Cls\u0026amp;\u0026amp;) noexcept = default; のような明示的default/deleteを最初に全部書いてしまうほうがいいかも。 さらに static_assert() でコンパイル時に std::is_nothrow_move_constructible などをチェックしておくと安心。\nそれ以外の部分でも「この関数は例外を投げない」 と宣言したほうがコンパイラにとって最適化しやすくなる。\nコンテナ 用途に合わせる http://en.cppreference.com/w/cpp/container\nstd::array Cの配列に便利なメンバ関数を持たせたようなクラス。 長さはコンパイル時に固定で、 メモリ上に連続した領域を確保する。 要素へのアクセスは高速で、 インデックス[] によるランダムアクセスも可能。 std::vector 可変長にした array 、つまり実行時に伸長・縮小可能。 C++で配列作るならとりあえずこれ。 途中に insert() するのは遅い （メモリ領域を別のところに再確保して全部コピーしなければならないので）。 std::valarray 要素ごとの四則演算など関数・演算子が定義済みの vector 亜種。 便利なだけでなくきっと最適化もされやすい。 ただし長さの変更など苦手な点もあるので使い所は限られる。 本格的なベクタ演算・行列演算がしたければ Eigen や Armadillo などを使ったほうよさそう。 std::deque vector とほぼ同じだが、reserve() ができない。 代わりに、push_front() と pop_front() が使える。 つまり、先頭に対する追加・削除が必要な場合だけ使うコンテナ。 std::list 飛び飛びのメモリ領域に散らばって存在できる配列クラス。 これは、各要素の値とともに前後の要素を示すイテレータを格納することで実現されている。 そのため、途中への insert() はイテレータを書き換えるだけなので高速。 ただし、その分メモリは余分に食うし、ランダムアクセスできないし、イテレータで総なめするのも遅い。 std::unordered_set, std::unordered_map 順序を気にしないぶん std::set や std::map より高速。 ただしハッシュ関数の準備など多少めんどい。 メモリは一気に確保 std::vector の push_back() は勝手にメモリ領域を確保してくれるので、 大きさの心配をする必要がなくて便利。 これは多くの場合、領域が足りなくなる都度「別のところに倍の領域を確保してコピー」 という処理をすることによって行われる。 始め1、次2、4、8、16、、、という具合に。 なので、10000個の要素を格納すると分かっている場合には、始めからそれだけ確保しておくと速い。\nstd::vector\u0026lt;int\u0026gt; v; v.reserve(10000); // then push_back() many times ループ 継続条件 for や while を回すときの継続条件式は回る度に評価されるので意外とコストになるかも。 イテレータで回すときの v.end() も毎回呼び出されるので、 ループの中身が軽い処理の場合には無視できない差になるかもしれない。\nfor (size_t i=0; i \u0026lt; v.size(); ++i) { // v.size() many times! } for (size_t i=0, n=v.size(); i\u0026lt;n; ++i) { // v.size() only once } for (vector\u0026lt;int\u0026gt;::iterator it=v.begin(), v_end=v.end(); it!=v_end; ++it) { // v.end() only once } for (const auto\u0026amp; x: v) { // C++11 range-based for } 前置インクリメント 後置インクリメント i++ でも 前置インクリメント ++i でも for ループの結果は変わらない。 が、i++ だと前の値を記憶してからプラスする（一時オブジェクトが作られる）ので、 ++i のほうがいいらしい。特にイテレータのとき。\nfor (std::vector\u0026lt;int\u0026gt;::iterator it=v.begin(); it!=v.end(); ++it) { // do something } 出来る限り外で処理 if-else や try-catch のブロックをループの外で大きく取れないか、 一時変数の定義や演算などをループの外で予めやっておけないか、確認すべし。\n入出力 標準入出力 Cストリーム(std::printf とか)とC++ストリーム(std::cout とか) が混在するプログラムでもちゃんと関数が呼ばれた順に入出力を行うため、 デフォルトではこれら２つのストリームが同期するようになっている。 必要がなければ切っておく。\nstd::cin はデフォルトで std::cout に結びつけられてて、 std::cin される度に std::flush されてしまうらしいので、 そうならないように切り離す。\n#include \u0026lt;iostream\u0026gt; int main() { std::ios::sync_with_stdio(false); std::cin.tie(nullptr); } 関連書籍 ",
  "href": "/cxx/speed.html",
  "tags": [
   "c++"
  ],
  "title": "C++高速化",
  "type": "cxx"
 },
 {
  "content": "環境に合わせた Makefile を自動生成する。 似たようなことをする configure スクリプトと比べて動作が高速で、 ライブラリの依存関係なども簡潔・柔軟に記述できる。\nconfigure ではそれを生成する開発者だけが autotools を使うのに対して、 CMakeでは開発者と利用者の双方がCMakeをインストールして使う。\nhttps://cmake.org/cmake/help/latest/\n基本 CMakeLists.txt を各ディレクトリに配置して、階層的に管理する。 プロジェクトのトップに置くものは、以下のようなコマンドで始める必要がある。\ncmake_minimum_required(VERSION 3.1) project(helloworld VERSION 0.1.0 LANGUAGES CXX) add_executable() や add_library() でビルドターゲットを作成し、 target_*() でそれらの設定を整えて、 install() でインストールする対象や行き先を指定する、というのが基本の流れ。\nadd_executable(a.out hello.cpp) target_compile_options(a.out PRIVATE -Wall -Wextra -pedantic) install(TARGETS a.out RUNTIME DESTINATION bin ) cmake コマンドの使い方は後述\nCommands https://cmake.org/cmake/help/latest/manual/cmake-commands.7.html\nScripting commands configure_file(\u0026lt;input\u0026gt; \u0026lt;output\u0026gt; [COPYONLY] [@ONLY]): ファイルの一部を置換しつつ複製する。 例えば @PROJECT_VERSION@ などを含む config.hpp.in に値を埋め込んで config.hpp を生成するとか。 function(\u0026lt;name\u0026gt; [args...]) foreach(var IN LISTS list) message(STATUS \u0026quot;Hello world!\u0026quot;) option(VARIABLE \u0026quot;Message\u0026quot; ON) set(VARIABLE value) Project commands https://cmake.org/cmake/help/latest/manual/cmake-commands.7.html#project-commands\nサブディレクトリを利用する:\nadd_subdirectory(source_dir [binary_dir] [EXCLUDE_FROM_ALL]) ターゲットを定義する:\nadd_executable(\u0026lt;name\u0026gt; [EXCLUDE_FROM_ALL] ...) add_library(\u0026lt;name\u0026gt; [STATIC|SHARED|OBJECT] [EXCLUDE_FROM_ALL|IMPORTED] ...) ターゲットのプロパティを追加する:\nset_target_properties(\u0026lt;target\u0026gt; PROPERTIES p1 v1 p2 v2 ...) target_compile_definitions(\u0026lt;target\u0026gt; \u0026lt;INTERFACE|PRIVATE|PUBLIC\u0026gt; ...) target_compile_features(\u0026lt;target\u0026gt; \u0026lt;P|P|I\u0026gt; ...) target_compile_options(\u0026lt;target\u0026gt; [BEFORE] \u0026lt;I|P|P\u0026gt; ...) target_sources(\u0026lt;target\u0026gt; \u0026lt;I|P|P\u0026gt; ...) target_include_directories(\u0026lt;target\u0026gt; [SYSTEM] [BEFORE] \u0026lt;I|P|P\u0026gt; ...): 次の関数があるおかげでこれを直接使うことは意外と少ない。 target_link_libraries(\u0026lt;target\u0026gt; \u0026lt;I|P|P\u0026gt; ...): この関数でターゲット間の依存関係を繋げていくのがCMakeの肝。 ライブラリ側 -L -l だけではなく、インクルード側 -I のオプションもお世話してくれる。 ターゲットなしの include_directories() link_directories() link_libraries() などはディレクトリ単位で影響が及ぶ亜種で、非推奨。 インストールするものや宛先を指定する。\ninstall(TARGETS) install(\u0026lt;FILES|PROGRAMS\u0026gt;) install(DIRECTORY) install(EXPORT): 外部のCMakeから使いやすくするためのconfigをインストールする。 似て非なる export() はビルドツリーにあるものを使わせるための謎コマンド。 オプション PRIVATE このターゲットをビルドするときだけ使い、これを利用するときには参照させない。 例えば「このプロジェクトのライブラリをビルドするにはBoostヘッダーが必要だけど、 これを利用するときにそれらのパスを知る必要はない」とか。 INTERFACE このターゲットでは使わないけど、これを利用するときには参照させる。 例えば、ヘッダーライブラリを作る場合とか。 PUBLIC このターゲットにもこれを利用するターゲットにも使う。使う場面あるかな？ EXCLUDE_FROM_ALL make [all] から外れて、明示的なターゲット指定でのみビルドされるようになる。 Variables https://cmake.org/cmake/help/latest/manual/cmake-variables.7.html\nVariables that Provide Information PROJECT_SOURCE_DIR, PROJECT_BINARY_DIR: 直近の project() におけるソースツリー、ビルドツリーの最上階。 CMAKE_SOURCE_DIR, CMAKE_BINARY_DIR: ソースツリー、ビルドツリーの最上階。 他のプロジェクトから add_subdirectory() で使われる場合、 PROJECT_* よりも上になる。 CMAKE_CURRENT_SOURCE_DIR, CMAKE_CURRENT_BUILD_DIR: ソースツリー、ビルドツリーにおける現在地。 PROJECT_NAME, PROJECT_VERSION: project() で設定したやつ。 CMAKE_SKIP_RPATH CMAKE_VERBOSE_MAKEFILE: とりあえず ON Variables that Change Behavior BUILD_SHARED_LIBS: STATIC/SHAREDが明示されていない add_library() でどっちをビルドするか。 CMAKE_BUILD_TYPE: Debug, Release, RelWithDebInfo, MinSizeRel. CMAKE_INSTALL_PREFIX: configureでの--prefixに相当 CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT CMAKE_PREFIX_PATH: パッケージやファイル探索 find_*() の候補パスを追加する Variables that Describe the System APPLE, UNIX, WIN32\nVariables that Control the Build CMAKE_BUILD_RPATH CMAKE_BUILD_WITH_INSTALL_NAME_DIR CMAKE_BUILD_WITH_INSTALL_RPATH CMAKE_INSTALL_NAME_DIR CMAKE_INSTALL_RPATH CMAKE_INSTALL_RPATH_USE_LINK_PATH CMAKE_MACOSX_RPATH C++ target_compile_features(${PROJECT_NAME} PUBLIC cxx_std_14) set_target_properties(${PROJECT_NAME} PROPERTIES CXX_STANDARD_REQUIRED ON CXX_EXTENSIONS OFF POSITION_INDEPENDENT_CODE ON WINDOWS_EXPORT_ALL_SYMBOLS ON ) target_compile_options(${PROJECT_NAME} PRIVATE -march=native -Wall -Wextra -pedantic ) if (NOT CMAKE_BUILD_TYPE) set(CMAKE_BUILD_TYPE Release) endif() message(STATUS \u0026#34;CMAKE_BUILD_TYPE: ${CMAKE_BUILD_TYPE}\u0026#34;) set(CMAKE_CXX_FLAGS_DEV \u0026#34;-O2 -g\u0026#34;) CMAKE_CXX_* のようなグローバル設定を使わず target_*() でターゲットごとに設定するのが今後の主流。 CMAKE_CXX_KNOWN_FEATURES に cxx_std_14 などの便利なメタタグが導入されたのは CMake 3.8 から。\nPredefined variable default CMAKE_CXX_FLAGS CMAKE_CXX_FLAGS_DEBUG -g CMAKE_CXX_FLAGS_MINSIZEREL -Os -DNDEBUG CMAKE_CXX_FLAGS_RELEASE -O3 -DNDEBUG CMAKE_CXX_FLAGS_RELWITHDEBINFO -O2 -g -DNDEBUG #ifndef NDEBUG なコードを残しつつ、 そこそこ速くコンパイル＆実行したい、 という組み合わせ -O2 -g は用意されていないので自分で定義する。 CMAKE_CXX_FLAGS_??? を適当に作れば -DCMAKE_BUILD_TYPE=??? をcase-insensitiveに解釈してもらえる。\nGenerator expressions https://cmake.org/cmake/help/latest/manual/cmake-generator-expressions.7.html\n文脈に応じて変数を評価する仕組み。\nプロジェクト内のビルド時と、外部パッケージとして利用される時とで、 インクルードパスを使い分ける。\ntarget_include_directories(${PROJECT_NAME} INTERFACE $\u0026lt;BUILD_INTERFACE:${PROJECT_SOURCE_DIR}/include\u0026gt; $\u0026lt;INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}\u0026gt; ) Modules https://cmake.org/cmake/help/latest/manual/cmake-modules.7.html\ninclude() や find_package() から使う。\nGNUInstallDirs https://cmake.org/cmake/help/latest/module/GNUInstallDirs.html\nインストール先のディレクトリを指定するときの標準的な値を決めてくれる。\nVariable Value CMAKE_INSTALL_BINDIR bin CMAKE_INSTALL_INCLUDEDIR include CMAKE_INSTALL_LIBDIR lib, lib64 CMAKE_INSTALL_DATADIR share CMAKE_INSTALL_FULL_\u0026lt;dir\u0026gt; ${CMAKE_INSTALL_PREFIX}/${CMAKE_INSTALL_\u0026lt;dir\u0026gt;} Linuxでは lib64 が標準的だが、Linuxbrewで使うには lib にする必要がある。\nif(${CMAKE_INSTALL_PREFIX} MATCHES linuxbrew) set(CMAKE_INSTALL_LIBDIR lib) endif() CMakePackageConfigHelpers https://cmake.org/cmake/help/latest/module/CMakePackageConfigHelpers.html\n他のプロジェクトから以下のように利用されるライブラリを作りたい。 外部プロジェクトであることを明確にするため 名前空間::ターゲット という形でリンクするのが筋:\nproject(otherproject CXX) find_package(mylib) # add_subdirectory(mylib) target_link_libraries(othertarget PRIVATE mylib::mylib) ${CMAKE_INSTALL_PREFIX}/share/ らへんにconfigファイルを送り込む必要がある。 install(TARGETS) の中で EXPORT のためのターゲット定義し、 install(EXPORT) でその設定を行う:\nproject(mylib VERSION 0.1.0 LANGUAGES CXX) # ... install(TARGETS ${PROJECT_NAME} EXPORT ${PROJECT_NAME}-config LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR} ) install(EXPORT ${PROJECT_NAME}-config DESTINATION ${CMAKE_INSTALL_DATADIR}/${PROJECT_NAME} NAMESPACE ${PROJECT_NAME}:: ) バージョン情報も同じところに送り込む:\nset(VERSION_CONFIG ${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}-config-version.cmake) include(CMakePackageConfigHelpers) write_basic_package_version_file( ${VERSION_CONFIG} COMPATIBILITY AnyNewerVersion ) install(FILES ${VERSION_CONFIG} DESTINATION ${CMAKE_INSTALL_DATADIR}/${PROJECT_NAME} ) find_package() から使うにはここまでの設定で十分だが、 add_subdirectory() からでも同じ形で使えるようにするため、 ALIAS を設定しておいたほうがいい:\nadd_library(${PROJECT_NAME} SHARED) add_library(${PROJECT_NAME}::${PROJECT_NAME} ALIAS ${PROJECT_NAME}) FetchContent https://cmake.org/cmake/help/latest/module/FetchContent.html\n外部ライブラリを取ってきて配置する。 前からあった ExternalProject はビルド時に実行されるため add_subdirectory() の対象にできないなどの問題があったが、 こちらはコンフィグ時に実行される。\ninclude(FetchContent) set(FETCHCONTENT_QUIET OFF) message(STATUS \u0026#34;FETCHCONTENT_SOURCE_DIR_IGRAPH: ${FETCHCONTENT_SOURCE_DIR_IGRAPH}\u0026#34;) FetchContent_Declare( igraph GIT_REPOSITORY https://github.com/igraph/igraph.git GIT_TAG ${PROJECT_VERSION} GIT_SHALLOW ON ) FetchContent_MakeAvailable(igraph) message(STATUS \u0026#34;igraph_SOURCE_DIR: ${igraph_SOURCE_DIR}\u0026#34;) CMake 3.11 からの新機能なので、もう少し普及するまでお預け。 当面は execute_process() で凌ぐ:\nfind_package(Git) execute_process(COMMAND ${GIT_EXECUTABLE} clone --recursive --depth=1 --branch=master https://github.com/USER/REPO.git ${SUBDIR} WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} ) add_subdirectory(${SUBDIR} EXCLUDE_FROM_ALL) FindThreads https://cmake.org/cmake/help/latest/module/FindThreads.html\n-lpthread とか自分で書かない。\nfind_package(Threads) target_link_libraries(mytarget PRIVATE Threads::Threads) FindBoost https://cmake.org/cmake/help/latest/module/FindBoost.html\nset(Boost_NO_BOOST_CMAKE ON) find_package(Boost REQUIRED COMPONENTS filesystem) target_link_libraries(mytarget PRIVATE Boost::filesystem) ヘッダーだけでいい場合は Boost::boost ターゲットをリンクする。\n探索パスを追加するには BOOST_ROOT を設定する。\nCTest https://cmake.org/cmake/help/latest/module/CTest.html\ninclude(CTest) if(BUILD_TESTING) add_subdirectory(test) endif() # test/CMakeLists.txt add_executable(test-gene gene.cpp) add_test(NAME gene COMMAND $\u0026lt;TARGET_FILE:test-gene\u0026gt;) ctest -V で実行。 一部のテストのみ実行したいときは -R \u0026lt;pattern\u0026gt; で絞る。\ninclude(CTest) は勝手にCDashの設定をして DartConfiguration.tcl を生成する。 次のように書き換えればそのへんをスキップできる:\noption(BUILD_TESTING \u0026#34;Build the testing tree.\u0026#34; ON) enable_testing() CLI cmake https://cmake.org/cmake/help/latest/manual/cmake.1.html\nビルド用の空ディレクトリを外に作って out-of-source で実行するのが基本。 やり直したいときは、そのディレクトリごと消す。 3.0以降 cmake --target clean はあるが、 CMakeのバージョンを上げたときなどcleanしたい場面で使えない。\ncmake -S . -B build -DCMAKE_INSTALL_PREFIX=${HOME}/local -DCMAKE_BUILD_TYPE=Debug デフォルトでは Makefile が書き出されるので make \u0026amp;\u0026amp; make install のように実行してもいいけど、 cmake からそれを実行することもできる:\ncmake --build build -j 2 cmake --build build -j 2 --target install 3.15以降は cmake --install \u0026lt;dir\u0026gt; が使える。\n-S \u0026lt;dir\u0026gt; ソースツリーを指定する。 3.13から。それまではundocumentedで -H\u0026lt;dir\u0026gt; という形だった。 -B \u0026lt;dir\u0026gt; ビルドツリーを指定する。 3.13から。それまではundocumentedだった。 -DCMAKE_XXX=YYY コマンドラインから変数を設定する。 -G \u0026lt;generator-name\u0026gt; Makefile, Ninja, Xcode, etc. -E \u0026lt;command\u0026gt; シェルの違いを気にせず基本的なコマンドが使えるように。e.g., chdir \u0026lt;dir\u0026gt; \u0026lt;cmd\u0026gt; make_directory \u0026lt;dir\u0026gt; -L キャッシュされている変数をリストアップ。 H をつけると説明文も。 A をつけるとadvancedな変数も。 見るだけなら -N オプションと共に。 ",
  "href": "/dev/cmake.html",
  "tags": [
   "package",
   "c++"
  ],
  "title": "CMake",
  "type": "dev"
 },
 {
  "content": "\nAuthor Jacintha Ellers Book Community Ecology: Processes, Models, and Applications Editor Herman A. Verhoef and Peter J.Morin Publisher Oxford University Press 11章担当 岩嵜航 実施日 2013-03-01 11.1 イントロ 群集生態学の対象 distribution: 分布\nabundance: どの種がどれくらい存在するか\ndemography: 個体数の動態\ninteraction: 特定の場所に共存する個体群間の相互作用 種多様性、食物網構造、侵入などについての記述には成功\n群集生態学の2つの欠点\n単純化しすぎて定量的予測のための一般的・力学的な原則に欠けている (Lawton 1999: Oikos PDF)。 e.g. 栄養段階による大雑把なグループ分けで、種の特性を無視 個体差が無く均質な集団を仮定（すなわち自然選択はかからない）。 進化のタイムスケールは長すぎるものとして無視。 でも強い選択圧、迅速な共進化の証拠が続々(Thompson 2005)。 進化的な視点も合わせれば生態学的な問いに取り組みやすくなるはず\n11.1.1 進化生物学と群集生態学の橋渡し Agrawal et al. 2007 Front Ecol Environ レビュー Johnson and Stinchcombe 2007 TREE オピニオン (群集生態学から見た)進化生物学\n生態学的要因が遺伝的変異や進化にどう影響を与えるか、を問う 遺伝的変異・表現型変異が群集レベルのプロセスに与える影響について、力学的・量的な示唆を与える しかし、だいたい1つか2つの種を抜き出して扱うのみ 群集の構成比など生態学的な文脈の中に位置づけられれば、野外での変異の維持についてもっと理解できるはず 群集生態学のモノサシ\n生態系の機能 productivity: 生産力 carbon storage: 炭素含量 nutrient acquisition: 栄養摂取量 decomposition rate: 分解率 人間が自然を管理する上で望まれる特性 species diversity: 種多様性 community stabitlity: 群集安定性 resilience: レジリエンス (= resistance + recovery) それらを進化学的に見ると\nたしかに群集の種組成や個体のパフォーマンスには依存している しかしそれ自体が自然選択の対象というわけではなく、むしろ副産物 そういう食い違いがあったので\n群集生態学者は個体差や進化を無視して現象論的アプローチに\u0026hellip; 進化学者はそういう記載的な仕事に益々興味を失くし\u0026hellip; ということでまずは 共通の基準 を持つことが重要\nこの章では新しい融合領域「進化群集生態学」のアウトラインを提示したい。\n個体間変異、種間変異 ⇌ 群集の構成や機能性\n(ただし前半で示すのは右方向の矢印のみ)\n種分化が起こるような大進化のタイムスケールの話までは突っ込まない。\n11.2 進化生物学: 遺伝的変異と表現型変異の機構 進化生態学では\n量的遺伝学と集団遺伝学という頑健なフレームワークを用いる 適応度に関わる形質の個体間変異を取り扱う longevity: 寿命\\ fecundity: 繁殖力\\ feeding rate: 摂餌率 11.2.1 遺伝的多様性を維持することの、集団レベルでの利益 変異の源\n有性生殖 減数分裂時の組み換え 点突然変異 挿入・欠失 集団中の変異の増減\n適応的な配列・組み合わせを壊すものだから大部分が有害で、集団からすぐに排除される そうでなくても遺伝的浮動によって毎世代失われていく 遺伝的変異を持つことが明確に適応的（絶滅率を下げたり）なときのみ集団中に維持される 突然変異の大部分が有害だと著者は述べているが、 むしろ中立な場合が多いというのが一般的な認識では？\n「絶滅リスクを下げる」という将来の集団にかかるメリットのために遺伝的変異が維持される、 と考えるのは進化学的におかしい。 結構デリケートな話題なので少なくともこんなにさらっと流すべきではない。\n適応的な理由で多型が維持されやすくなる機構としては以下のようなものが考えられる\n超優勢: ヘテロな遺伝子型が有利 頻度依存選択: 稀なほど有利 \u0026ldquo;集団サイズ x 突然変異率\u0026rdquo; がそこそこ高ければ、 特に適応的意義がなくても mutation-selection-drift balance によってある程度の変異は維持される。\n集団内の遺伝的変異が絶滅率を下げる2つの主なメカニズム\nTangled Bank (絡まりあった土手) 仮説 特性の違う個体がいたほうが、ヘテロな資源（ニッチ）を相補的にうまいこと使いきれる Antonovics 1978 (PDF) 異なるマイクロニッチを探索するので遺伝子型間の対立・競争が弱まるだろう Bell 1991 クラミドモナスの複数系統を複数環境で培養して、 内的自然増加率 r と環境収容力 K を定量化し、 G × E が種内レベルでも重要であることを示した。 Barrett et al. 2005 シュードモナス属の蛍光細菌を異なる複雑さの環境(養分が1–8種類)で培養すると、 複雑な(多種類の養分を含む)環境ほど適応度の平均も分散も大きな遺伝子型が進化した。 集団内の個体差を見ると「得意分野の異なる半端なジェネラリスト」たちが共存していた。 語源は Darwin 1859 \u0026ldquo;On the Origins of Species\u0026rdquo;\nIt is interesting to contemplate an entangled bank, clothed with many plants of many kinds, with birds singing on the bushes, with various insects flitting about, and with worms crawling through the damp earth, and to reflect that these elaborately constructed forms, so different from each other, and dependent on each other in so complex a manner, have all been produced by laws acting around us.\nRed Queen (赤の女王) 仮説 寄生者や感染症など、種特異性が高く進化速度の早い敵に対しては、 遺伝的変異をどんどん作って進化しないとヤバいだろう (Jaenike 1978) 種内の遺伝的多様性がプラスに働く実証例\nHughes and Stachowicz 2004 ガチョウに対するアマモの食害耐性 Mattila and Seeley 2007 ミツバチの摂餌率、食料備蓄、増殖速度 Gamfeldt et al. 2005 フジツボのキプリス幼生の定着成功率 最初に提唱したのは Van Valen 1973\n化石記録を見てみると、種分化が成立してからの年代と絶滅確率には相関がなかった。 適応しきれば絶滅しにくくなる、ということはなく、 どんな種も絶滅せずその場に留まるために常に進化を強いられてるんじゃないか、という。 元ネタはもちろん「鏡の国のアリス」\n遺伝的多様性により適応度が上昇するメカニズムを実際に特定した研究例は稀。\nReusch et al. 2005 アマモ Zostera marina 集団内の遺伝子型の数を変化(1, 3, 6種類)させたアマモを飼育すると、 多様な集団ほど芽の数もバイオマスも増加。 強い遺伝子型がいたから、ではなく、遺伝子型間の相互作用によるものであると示した。 (Figure 11.1) {height=\u0026ldquo;200px\u0026rdquo;}\nTagg et al. 2005 ミジンコ Daphnia obtusa 無性生殖 vs 有性生殖で双方向に侵入実験をしたところ、 遺伝的多様性が高い有性生殖集団ほうが侵入力、防御力ともに高い。 これは competitive release によるものである。 Schmid 1994 セイタカアワダチソウ Solidago altissima 遺伝的多様度を操作する実験区で、うどんこ病の感染と植物の成功度を測った。 (あまりキレイな結果ではないっぽい) Semlitsch et al. 1997 ヨーロッパトノサマガエル Rana esculenta R. ridibunda と R. lessonae のその雑種である R. esculenta を高密度・低密度の2環境で、単独・混合飼育した場合に変態までの時間がどうなるか調べた。 ハイブリッドジェネシスによるヘミクローン繁殖\n他種のオスの精子を取り込んで雑種の受精卵を作るが、 その受精卵から生まれるのはメスばかりで、 しかもその娘が卵を作るときは父由来のゲノムを捨てて母由来のゲノムだけを使う。 これだけでも材料としておもしろい。 このカエルのほかには、ナナフシ、カダヤシ、アイナメ(?) でしか確認されていない。 (図は Neaves and Bumann 2011 Trends Genet)\n{height=\u0026ldquo;400px\u0026rdquo;}\n2013年2月の Nature にも宿主両生類の種多様性が寄生性吸虫類の伝播を抑制して疾患を低減するという報告\n11.2.2 遺伝的変異の定量化 集団内の遺伝子型の数、というのが一般的 非類似度 dissimilarity も考慮したい ゲノムワイドな遺伝的多様性を測りたい with AFLP, microsatellite, SNP これまでのコード/非コードという分け方は、表現型への影響を単純化しすぎ。 非コード領域も転写や翻訳などの調節に関わっているので表現型・適応度への影響大アリ 中立マーカーによって測った遺伝的変異と、量的形質のばらつき、強い相関があるとは限らない。\nMerila and Crnokrak 2001 18研究のメタ解析で FST と QST を比較。 基本的に両者はそこそこ相関しているが、 だいたいいつも QST \u0026gt; FST であり、 その差は利用する中立マーカーによっても異なる。 Reed and Frankham 2001 71研究例のメタ解析してみたら、 中立マーカーのばらつきと量的形質のばらつきの相関はたったの r = 0.217 しかない。 McKay and Latta 2002 29種のメタ解析で QST と FST を比較。 同じ種でも QST がどれだけ FST から逸脱するかは形質により異なる。 単純に量的形質のばらつきのプロキシとしてではなく、 ちゃんと分離して情報を得るべし。 集団間の分化の度合いを測る FST と QST\nFST 中立マーカーにおける集団間の遺伝的分化 QST 量的形質における集団間の遺伝的分化。 QST \u0026gt; FST ならば集団間で異なる表現型が好まれるような多様化選択\nQST \u0026lt; FST ならば集団間で同じ表現型好まれるような純化選択\n11.2.3 遺伝的多様性と表現型多様性の関係 {height=\u0026ldquo;360px\u0026rdquo;}\n遺伝的多様性が表現型多様性のproxyとして危うい、というもうひとつの理由\n表現型可塑性 (phenotypic plasticity) ひとつの遺伝子型で環境に応じて異なる表現型（形態、生理的機能）を可塑的に発現 応答規範、反応基準 (reaction norm) ある遺伝子型が環境条件に応じてどのような表現型を発現するか、という関係 Figure 11.2a ひとつの曲線はひとつの遺伝子型。 どの環境に適応しているか(ピークのx座標)は共通しているが、 ピークの高さと裾野の広がりが異なる。 この場合、至適な環境でのパフォーマンスが高いやつほど小さな環境変化でダメになる。 Via et al. 1995 TREE Perspective 応答規範は遺伝的に決まっており、それ自体が進化しうるものだ。 表現型可塑性は環境の時空間的な 異質性 (heterogeneity) への適応\nたまにしか使わないタンパク質を常に発現させておくのはコストが大きいので、 刺激に応じてオンオフ切り替える。\ne.g. ショウジョウバエの熱応答 (Krebs and Holbrook 2001) ヒートショックを与えてHsp70を発現させると、 その遺伝子コピーを余計に持っている株では 熱感受性のADH遺伝子の活性が落ちてしまった。 分子シャペロンが過剰に発現していると未熟なペプチドに結合したりして有害だ、と示唆。 「環境」には気温などの無機的なものだけでなく、生物間相互作用も含まれる\n草食動物に対する植物の防御物質 捕食者に対する防御形態 canalization (運河化、しいて言うなら) 環境が多少ぶれても普通の表現型を発現するように進化的に発生過程が安定化すること epigenetic landscape\n本文で引用されてる Schlichting and Pigliucci 1998 はいい本だけどやはりそこは Waddington 1957 に触れてほしいところ。\n{height=\u0026ldquo;200px\u0026rdquo;}\n{height=\u0026ldquo;200px\u0026rdquo;}\nFigure 11.2b 環境中央付近ではどの遺伝子型も同じような表現型応答を示している (canalizeされている) が、閾値を超えた両端の環境では遺伝子型によって応答規範が異なり、 遺伝子型の違いが表現型の違いとして顕在化する。 表現型可塑性の度合いが遺伝子型によって異なることを示した研究例\nScheiner and Lyman 1989 表現型分散 = 遺伝分散 + 環境分散 + G × E と切り分け、 G × E の部分を表現型可塑性の遺伝率としてキイロショウジョウバエで定量化。 Loeschcke et al. 1999 同所的に生息するサボテン食いのショウジョウバエ2種について、 いろいろな温度環境で飼育して胸部や翅脈の長さを計測。 両種の応答規範は異なっており、温度適応の過程も異なっていたことを示唆。 Liefting and Ellers 2008 森林と荒地からトビムシを採ってきて、 室内で最低2世代飼育して母性効果を取り除いた後、 異なる温度で生まれてきた子の成長速度と卵サイズを計測。 変わりやすい荒地から採ってきたやつのほうが急峻な応答規範を示した。 11.3 群集の特性が個体レベルの遺伝子型や選択によって決まる実例 延長された表現型 (Dawkins 1982) 進化の基本単位は遺伝子であるが、 遺伝子の表現型は個体だけでなくその外部にも拡張して解釈できる。\nWhitham et al. 2003 種内変異が群集・生態系のパフォーマンスにも影響する、 ということを示したメタ研究 植物の二次代謝産物の遺伝的差異が対植食者防御に影響する例 Havill and Raffa 2000 ポプラ、その葉を食うマイマイガ、その幼虫に寄生するコマユバチ。 ダメージを受けたポプラからでる二次代謝産物はマイマイガに毒性があるだけでなく、 マイマイガの天敵コマユバチを誘引することで間接的にさらに防御。 Harvey et al. 2003 アブラナ科のカラシナとヤセイカンラン、 それを食べる植食者モンシロチョウ、 その幼虫に寄生するアオムシコマユバチ、 さらにそいつに寄生する超寄生蜂 Lysibia nana、 という4栄養段階に渡って植物の二次代謝産物がボトムアップ的に影響。 植物の種間変異が上位の群集構造に影響する例 Dungey et al. 2000 ユーカリ属2種とその雑種 F1, F2 でそれぞれ実験区を作って そこに生息する節足動物の種多様性を計測したところ、 F1で種多様性が最大となった。 Sznajder and Harvey 2003 アブラナ科3種の上でヤガ2種とそれぞれの寄生蜂を飼育。 二次代謝産物と寄生蜂を介した間接的防御は植食者のスペシャリスト度合いに依存。 Wimp et al. 2005 ハコヤナギ2種とその雑種を2年間に渡って野外およびcommon-gardenで観察。 遺伝子型によって生息する節足動物の種多様性は変わらないが、 その中に含まれる種組成は異なっていた。 その他、延長された表現型の例 両生類全般における対捕食者防御 (なぜか本文中に文献引用ないので北大岸田さんのウェブサイトにリンク) Fabricius et al. 2004 サンゴの白化耐性は、そこに棲む藻類の熱耐性株とうまくやれるかどうかに依存。 一次生産者たる植物の種内変異が群集に影響を及ぼすことを示した研究例 Roscher et al. 2007 同じプロットに一緒に植える植物の種多様性(1, 2, 4, 8, 16, and 60) と機能群多様性 (1, 2, 3, and 4) を操作して、 ホソムギの複数品種における冠サビ病と黒サビ病の感染率・強度を調べた。 Stiling and Rossi 1996 異なる場所に生息するキク科植物の同種2系統を相互移植して、 タマバエによる虫コブのサイズ、放棄率、コバチによる寄生率を計測。 虫コブのサイズを介して、ボトムアップ効果が間接的にトップダウン効果に影響。 Johnson and Agrawal 2005 メマツヨイグサ14系統を5つの野外サイトに植えて、その上に生息する節足動物をカウント。 種多様性、種組成、バイオマスなどのばらつきの40%も植物の遺伝子型で説明できた。 ただし G × E 相互作用も有意に効いていたし、 環境と比べた相対的な影響力は環境のスケールに依存していた。 上位捕食者よりも植食者のほうが強い影響を受けていた。 Crawford et al. 2007 セイタカアワダチソウにロゼット型虫コブを作るタマバエの生態系エンジニアとしての役割。 虫コブありのほうがほかの節足動物の種多様性も高くなる。 その度合は宿主植物の遺伝子型との相性が影響し、 宿主植物の遺伝的多様性が高いほど全体として高くなる。 Madritch et al. 2006 common-garderで育った5つの遺伝子型のアメリカヤマナラシ(ポプラ)の葉のリターを1年間追跡調査し、 遺伝子型によって分解率とそれに伴う栄養塩供給が異なるを明らかにした。 植物以外に着目した研究は多くないが、あるにはある\nアーバスキュラー菌根菌の種内変異が植物の成長や菌糸体の伸長に影響\nMunkvold et al. 2004: アーバスキュラー菌根菌の種内多型と種間多型を操作し、 キュウリの生長とリン取り込みへの影響を計測。 種内変異もかなり重要だということを示した。\nKoch et al. 2006: 似たようなことをオオバコ科、シソ科、イネ科の植物で\n植食者の表現型(可塑性)が群集プロセスに影響する例\nHarvey et al. 2003: 植食者が毒の少ない餌を食べられたかどうかで、 そいつの寄生蜂・超寄生蜂の体サイズや生残率が決まる。 References ",
  "href": "/lectures/verhoef_comm_ecol-11.html",
  "tags": [
   "book"
  ],
  "title": "Community Ecology 輪読会 11章",
  "type": "lectures"
 },
 {
  "content": " https://docs.python.org/library/copy.html https://docs.python.org/reference/datamodel.html C++からプログラミングを始めて、Pythonにおけるオブジェクトの扱い、 特に代入・参照・コピー・mutable/immutableらへんの理解に苦しんでる人のメモ。\nPythonの代入は基本的に参照渡し \u0026gt;\u0026gt;\u0026gt; l = [1, 2, 3] \u0026gt;\u0026gt;\u0026gt; m = l \u0026gt;\u0026gt;\u0026gt; l[1] = 0 \u0026gt;\u0026gt;\u0026gt; m [1, 0, 3] m に渡るのは l と同じ実体に対する参照。 l[1] = 0 は l を変更しているのではなく、l を通してその実体を変更してる。 その変更は m を通して見ても同じ。\n\u0026gt;\u0026gt;\u0026gt; l = [1, 2, 3] \u0026gt;\u0026gt;\u0026gt; l = m \u0026gt;\u0026gt;\u0026gt; l = [6, 6, 6] \u0026gt;\u0026gt;\u0026gt; m [1, 2, 3] l = [6, 6, 6] は新しい実体への参照を l に与える。 l が参照していた実体に対する変更ではないので、 m から見える実体にも変化は無い。\n\u0026gt;\u0026gt;\u0026gt; x = 0 \u0026gt;\u0026gt;\u0026gt; y = x \u0026gt;\u0026gt;\u0026gt; x = 1 \u0026gt;\u0026gt;\u0026gt; y 0 これも同様。y には x と同じ 0 への参照が渡る。 x = 1 は x を通した 0 への変更ではなく x の参照先を 1 に変えるだけなので、y の指す値に変更は無い。\nmutable / immutable mutable list set dict immutable 0, 1, 2, ... \u0026quot;strings\u0026quot; tuple immutableなオブジェクトを変えることはできない。 immutableなオブジェクトを参照していた変数に別のオブジェクトの参照を渡すことはできる。\n\u0026gt;\u0026gt;\u0026gt; t = (1, 2, 3) \u0026gt;\u0026gt;\u0026gt; t[1] = 0 Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; TypeError: \u0026#39;tuple\u0026#39; object does not support item assignment \u0026gt;\u0026gt;\u0026gt; t = (0, 0, 0) \u0026gt;\u0026gt;\u0026gt; t (0, 0, 0) 文字列や数字も考え方は同じ。\n\u0026gt;\u0026gt;\u0026gt; s = \u0026#34;abc\u0026#34; \u0026gt;\u0026gt;\u0026gt; s[1] = \u0026#34;z\u0026#34; Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; TypeError: \u0026#39;str\u0026#39; object does not support item assignment \u0026gt;\u0026gt;\u0026gt; s = \u0026#34;xyz\u0026#34; \u0026gt;\u0026gt;\u0026gt; s \u0026#39;xyz\u0026#39; \u0026gt;\u0026gt;\u0026gt; 0 = 1 File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1 SyntaxError: can\u0026#39;t assign to literal \u0026gt;\u0026gt;\u0026gt; x = 0 \u0026gt;\u0026gt;\u0026gt; x = 1 \u0026gt;\u0026gt;\u0026gt; x 1 immutableなオブジェクトで変更できないのは「何を参照しているか」という情報。 参照先の実体がmutableなオブジェクトならそいつは変更できる。\n\u0026gt;\u0026gt;\u0026gt; l = [1, 2, 3] \u0026gt;\u0026gt;\u0026gt; t = (l, l) \u0026gt;\u0026gt;\u0026gt; t ([1, 2, 3], [1, 2, 3]) \u0026gt;\u0026gt;\u0026gt; l[1] = 0 \u0026gt;\u0026gt;\u0026gt; t ([1, 0, 3], [1, 0, 3]) \u0026gt;\u0026gt;\u0026gt; t[0][2] = 0 \u0026gt;\u0026gt;\u0026gt; l [1, 0, 0] t が参照しているのは、l が指す実体へのimmutableな参照を持つ tuple 。 「その tuple が [0] 番目で何を参照しているか」を変更することはできないが、 参照している実体はmutableな list なので l や t[0] を通して変更できる。 つまり、Pythonの tuple はC++でいうところの int* const なポインタを格納した固定長配列みたいなもの？\ncopy \u0026gt;\u0026gt;\u0026gt; import copy \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; l = [1, 2, 3] \u0026gt;\u0026gt;\u0026gt; m = copy.copy(l) \u0026gt;\u0026gt;\u0026gt; l[1] = 0 \u0026gt;\u0026gt;\u0026gt; l [1, 0, 3] \u0026gt;\u0026gt;\u0026gt; m [1, 2, 3] copy.copy() によって l と同じ中身の新しい list 実体が作られ、 それへの参照が m に渡される。l を通した古い list への変更は m から見える list には影響しない。 中身がすべてimmutableな一重のコンテナならこれでいいが、 そうじゃない場合「l と同じ中身」というのが問題になる。 C++で言えば、メンバ変数としてポインタを持つクラスを デフォルトのコピーコンストラクタでコピーしたような状態になる。\n\u0026gt;\u0026gt;\u0026gt; import copy \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; l = [1, 2, 3] \u0026gt;\u0026gt;\u0026gt; m = [l, l] \u0026gt;\u0026gt;\u0026gt; x = m \u0026gt;\u0026gt;\u0026gt; y = copy.copy(m) \u0026gt;\u0026gt;\u0026gt; z = copy.deepcopy(m) \u0026gt;\u0026gt;\u0026gt; m [[1, 2, 3], [1, 2, 3]] \u0026gt;\u0026gt;\u0026gt; m[1][1] = 0 \u0026gt;\u0026gt;\u0026gt; m[1] = 0 \u0026gt;\u0026gt;\u0026gt; x [[1, 0, 3], 0] \u0026gt;\u0026gt;\u0026gt; y [[1, 0, 3], [1, 0, 3]] \u0026gt;\u0026gt;\u0026gt; z [[1, 2, 3], [1, 2, 3]] x はただの代入によって m と同じ実体への参照を受け取ったので、 m を通して行った変更はすべて共有している。 y は m の実体と同じ中身（l = [1, 2, 3] で作った list への参照2つ） を持つ新しい list への参照を与えられている。 m[1] = 0 は m が参照している外側の list を変更するものなので、 y や y[1] には影響しない。 m[1][1] = 0 は l や x[0] や y[1] が参照している list 実体への変更なので y から見えるものも変わる。 z は y と同じように新しい list への参照が与えられている。 y とは違ってその list 実体が持つのは m と同じ中身ではなく、 同じ値になるようにすべての要素が再起的にコピーされたもの。 z の中にある [1, 2, 3] は l とは異なる実体であり、 l や m などを通して行われた変更は全く影響しない。 \u0026gt;\u0026gt;\u0026gt; import copy \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; class CopyTest(object): ... def __init__(self): ... self.l = [1,2,3] ... self.s = \u0026#34;xyz\u0026#34; ... def __repr__(self): ... return str([self.l, self.s]) ... def self_copy(self): ... self = copy.copy(self) ... def self_deepcopy(self): ... self = copy.deepcopy(self) ... def copy(self): ... return copy.copy(self) ... def deepcopy(self): ... return copy.deepcopy(self) ... \u0026gt;\u0026gt;\u0026gt; a = CopyTest() \u0026gt;\u0026gt;\u0026gt; b = a \u0026gt;\u0026gt;\u0026gt; b.self_copy() \u0026gt;\u0026gt;\u0026gt; c = a \u0026gt;\u0026gt;\u0026gt; c.self_deepcopy() \u0026gt;\u0026gt;\u0026gt; d = a.copy() \u0026gt;\u0026gt;\u0026gt; e = a.deepcopy() \u0026gt;\u0026gt;\u0026gt; a.l[1] = 0 \u0026gt;\u0026gt;\u0026gt; a.s = \u0026#34;---\u0026#34; \u0026gt;\u0026gt;\u0026gt; a [[1, 0, 3], \u0026#39;---\u0026#39;] \u0026gt;\u0026gt;\u0026gt; b [[1, 0, 3], \u0026#39;---\u0026#39;] \u0026gt;\u0026gt;\u0026gt; c [[1, 0, 3], \u0026#39;---\u0026#39;] \u0026gt;\u0026gt;\u0026gt; d [[1, 0, 3], \u0026#39;xyz\u0026#39;] \u0026gt;\u0026gt;\u0026gt; e [[1, 2, 3], \u0026#39;xyz\u0026#39;] self = copy.copy(self) なメソッドは成功しないらしい。\n",
  "href": "/python/copy.html",
  "tags": [
   "python"
  ],
  "title": "copy",
  "type": "python"
 },
 {
  "content": "https://bitbucket.org/gutenkunstlab/dadi\nInstallation https://bitbucket.org/gutenkunstlab/dadi/wiki/Installation\nmacOS 標準 Python 2.7 に pip と virtualenv を入れる:\n/usr/bin/python -m ensurepip -v --user ~/Library/Python/2.7/bin/pip install --user -U setuptools pip virtualenv dadi専用のvirtualenvを作って、その中に依存パッケージをインストール:\n~/Library/Python/2.7/bin/virtualenv ~/.virtualenv/dadi source ~/.virtualenv/dadi/bin/activate # (dadi) virtualenv is active pip install -U setuptools pip flake8 pip install -U numpy scipy matplotlib リポジトリのクローンを取得し、リリース版をチェックアウトしてインストール:\n# (dadi) virtualenv is active git clone https://bitbucket.org/gutenkunstlab/dadi.git cd dadi/ git tag git checkout 1.7.0 python setup.py install テスト:\n# (dadi) virtualenv is active cd tests/ python run_tests.py cd .. virtualenvから抜けたいときは deactivate コマンドを打つ。\nまた入りたいときは source ~/.virtualenv/dadi/bin/activate を打つ。\ndoc/ 以下に結構詳しいドキュメントが入ってるので読む (これもウェブで公開してくれたらいいのに)\n入力データ https://bitbucket.org/gutenkunstlab/dadi/wiki/DataFormats\nTips $\\theta = 4N_0\\mu$ は ms と同じだが、 時間とmigration rateは $2N_0$ 単位。 時間とmigration rateは大きすぎると遅くなるので upper bound はそれぞれ 10, 20 を推奨 population size は逆に小さいとき遅くなるので lower bound 1e-3 程度を推奨 ",
  "href": "/bio/dadi.html",
  "tags": [
   "genetics",
   "python"
  ],
  "title": "dadi",
  "type": "bio"
 },
 {
  "content": "自分で書いた関数が多くなってきたら、まとめてパッケージを作るとよい。 少しだけ面倒だが、以下のようなメリットがある。\n普通に変数や関数を定義するとワークスペースが名前でいっぱいになってしまうが、 パッケージ内で定義されている変数や関数は ls() で出てこないのでスッキリ 既存のオブジェクトと名前が衝突するような場合でも、 mypackage::func1 のように名前空間を明示的に指定して呼び出せる CRANに上げる程ではないとしても、 GitHubに公開しておけば誰でも使えるようになるので、 共同研究者と解析環境を共有したり、 ひとつの論文のワークフローを置いておいたり、いろいろ使い道はある。\nRパッケージ https://r-pkgs.org/ How to develop good R packages (for open science) 最低限の作成手順 開発支援パッケージをインストールする: install.packages(c(\u0026quot;devtools\u0026quot;, \u0026quot;usethis\u0026quot;))\nusethis の関数をいくつか使って骨組みを作る:\nusethis::create_package(\u0026#34;hello\u0026#34;) usethis::use_mit_license() usethis::use_package_doc() devtools::check() で様子を見てみる。\nローカルgitリポジトリを作って最初のコミットをする: usethis::use_git()\nGitHubに同名のリポジトリを作ってプッシュ: usethis::use_github()\n（リポジトリの名前をパッケージ名と違うものにしたい場合などは手動で）\nとりあえず誰でもインストール可能なパッケージができたはず:\nremotes::install_github(\u0026#34;heavywatal/rhello\u0026#34;) ソース構造 https://r-pkgs.org/structure.html\nDESCRIPTION # 一番大事 NAMESPACE # 見せるオブジェクトを列挙 README.md # 全体の説明を簡単に R/ # Rソースコード data/ # サンプルデータなど inst/ # CITATION man/ # ヘルプファイル.Rd src/ # C++ソースコード tests/ vignettes/ CRANから落としてくる .tar.gz ソースコード (bundle package) とは違う。\nDESCRIPTION https://r-pkgs.org/description.html\nどうでも良さそうなファイル名とは裏腹に、ちゃんと書かないと動かない。 始めはusethisとかに生成してもらい、他のパッケージを参考にしつつ修正していく。 Imports に列挙したものは依存パッケージとして同時にインストールされる。 しかし library() 時に同時に読み込まれるわけではない。 Title はピリオドを含まない一文でタイトルケース。 Description はピリオドを含む一段落。 ライセンスを別ファイルにする場合は License: file LICENSE と書く Authors@R のとこは後でRで評価されるので変な形。 NAMESPACE https://r-pkgs.org/namespace.html\n後述のroxygen2がソースコードから自動生成するので直接触らない。 ここで export() された関数だけがユーザーから見える。 外部パッケージから importFrom(package, function) された関数はattachされてパッケージ内で利用可能になる。 import(package) で全ての関数をまとめて処理できるけど名前の衝突が怖い。 R/ ソースコード https://r-pkgs.org/code.html\nNAMESPACE や man/*.Rd を自動生成してもらえるように 後述のroxygen形式でコメントを書く ファイルの数や名前は何でもいいので、開発者が分かりやすいようにしとく。 例えば、似たような関数をひとつのファイルにまとめ、 同名の@rdnameを指定してman/と同じ構造にするとか。 library() や require() を書かない。 必要なパッケージは DESCRIPTION の Imports に書き、 名前空間::関数() のようにして使う。 どうしても名前空間を省略したいものだけ @importFrom を指定する。 パッケージ読み込み時などに実行される特殊関数 .onLoad(...), .onAttach(...), .onUnload(...) は慣習的にzzz.Rというファイルに記述する。 src/ C++ソースコード https://r-pkgs.org/src.html\nRcppページの\u0026quot;Rパッケージで使う\u0026quot;セクションを参照\nvignettes/ https://r-pkgs.org/vignettes.html\n個々の関数の使用例はRソースファイルの @examples に書くとして、 複数の関数を組み合わせてどう使うかとか、 パッケージ全体の使い方とかを説明するのがvignettes/の役割。 Rmarkdown形式で書いてHTMLやPDFで表示できるので表現力豊か。\nusethis::use_vignette(\u0026quot;hello\u0026quot;) で雛形を作ってもらうのが楽。\npandoc と pandoc-citeproc が必要なので Homebrew とかでインストールしておく。\ncheck()がデフォルトでvignette=TRUEかつ処理がやや重いので、 毎回その重さを受け入れるか、わざわざFALSE指定するかというのは悩みどころ。\npkgdownでウェブサイトを構築すると、 ここに置いてあるvignettesはArticlesという位置づけになる。\ntests/ https://r-pkgs.org/tests.html\ntestthatパッケージを使うのがデファクトスタンダード。 use_testthat() で初期設定して use_test(\u0026quot;somefunction\u0026quot;) のようにテストファイルを追加する。 tests/testthat/ 以下のファイル構成は R/ と同じにしておくとわかりやすい。\nさらにcovrパッケージを使って、 ソースコードのうちどれくらいがテストでカバーされてるかを可視化すると良い。\ndata/ https://r-pkgs.org/data.html https://usethis.r-lib.org/reference/use_data.html usethis::use_data_raw() で data-raw/\u0026lt;dataset\u0026gt;.R をセットアップし、 その中で usethis::use_data() を呼んで data/\u0026lt;dataset\u0026gt;.rda や R/sysdata.rda に配置する。\ndata/ R から save(), load() で読むようなバイナリ形式のファイルを置く。 1オブジェクト1ファイルで同名にして .rda, .RData 拡張子を付ける。 勝手にexportされるので R/ 内のソースにroxygenドキュメントを書く。 R/sysdata.rda ユーザーに公開せずパッケージ内部で使うためのデータ。 data-raw/ data/ のファイルを作るためのソースコードやテキストデータを置いておく。 bundled package には入れない (ように usethis::use_data_raw() が .Rbuildignore を設定する)。 inst/extdata/ データ読み書きを例示するためのデータファイルを置いておく。 system.file(\u0026quot;extdata\u0026quot;, \u0026quot;mtcars.csv\u0026quot;, package = \u0026quot;readr\u0026quot;) のようにアクセスできる。 その他 https://r-pkgs.org/misc.html\ndemo/ vignettesに取って代わられた古い機能。 ソースコード*.Rを置いておくとdemo()関数で呼び出せるというだけ。 check()でソースコードの中身は実行されないが、 demo/00Index というファイルとの整合性が取れてないと警告される。 「デモの名前 + スペース3つ以上かタブ + 適当な説明」という形式。 ファイル名から拡張子を取り除いたものがデモの名前になる。 mydemo1 Description of demo 1 mydemo2 Description of demo 2 exec/ 実行可能スクリプトの置き場所。 インストール先はそのままパッケージ内の exec/ (つまり最初からパスが通ってるような場所ではない)。 パーミッションはインストール時に設定してもらえるのでソースツリーでは 644 でOK。 Rscript をshebangで指定することも可能。 例えばRStudioを使いこなせないターミナル勢としては knitr::knit() するだけのコマンド とか作っておくと便利。 inst/ ここに入ってるものはインストール先でトップディレクトリに移される謎仕様。 論文で引用されることを想定している場合は inst/CITATION を作る。 citation(\u0026quot;ggplot2\u0026quot;) のように参照できるようになる。 tools/ たぶん何を入れても一切チェックされずインストールもされない自由なディレクトリ。 tests/ や vignettes/ に入れる前のガラクタコードを一時的に置いとくとか。 .Rbuildignore Rパッケージらしからぬ変なファイルやディレクトリがあると怒られるので、 そういうやつをこのファイルに列挙して無視してもらう。 devtools 骨組みを作るとこからCRANにデプロイするとこまでお世話してくれる。 いろんな専門パッケージの集合体。\n主な関数 document(pkg = \u0026quot;.\u0026quot;, roclets = NULL, quiet = FALSE) roxygen2 を呼び出してソースコードから NAMESPACE や man/*.Rd を自動生成する。 src/ 内のドキュメント変更はこれ1回の実行では反映されない。 check(pkg = \u0026quot;.\u0026quot;, document = NA, ...) パッケージとしての整合性を確認。 ついでに document() は実行できるけど spell_check() はできないので手動で。 test(pkg = \u0026quot;.\u0026quot;, filter = NULL, ...) testthat を呼び出して tests/ 以下のテストコードを実行する build(pkg = \u0026quot;.\u0026quot;, path = NULL, ...) R CMD install でインストール可能な tar ball (bundle package) を作る。 Rcppコードをコンパイルするという意味ではない。 install(pkg = \u0026quot;.\u0026quot;, reload = TRUE, quick = FALSE, build = !quick, ...) ローカルにあるソースからインストール。 build = TRUE のとき(デフォルト)、わざわざ bundle package を tempdir() に作ってからそいつでインストールする。 install_github(repo, ref = \u0026quot;master\u0026quot;, subdir = NULL, ...) GitHubリポジトリからインストール。 unload(pkg = \u0026quot;.\u0026quot;, quiet = FALSE) datach(\u0026quot;package:XXX\u0026quot;) とか unloadNamespace(XXX) よりもちゃんとまっさらにパッケージを外す。 load_all(pkg = \u0026quot;.\u0026quot;, reset = TRUE, recompile = FALSE, export_all = TRUE, ...) install() せずファイルから直接 library() する。 ロード済みでもまず unload() が呼ばれるので安心。 clean_dll(pkg = \u0026quot;.\u0026quot;) src/ 以下に生成される .o, .so を消す。 普段は触る必要ないが、たまにこれが必要な不具合に出くわす。 roxygen2 RソースコードのコメントからNAMESPACEとヘルプ(man/*.Rd)を自動生成する。\nhttps://cran.r-project.org/web/packages/roxygen2/ https://github.com/klutometis/roxygen https://r-pkgs.org/man.html https://kbroman.org/pkg_primer/pages/docs.html https://cran.r-project.org/web/packages/roxygen2/vignettes/rd.html roxygen2::roxygenise(package.dir=\u0026quot;.\u0026quot;, ..., clean=FALSE) を直接呼んでもよいが、 基本的には devtools::document() を使って間接的に利用する。\n使い方 #\u0026#39; Title of the simple function to add 1 #\u0026#39; #\u0026#39; The second paragraph is recognized as the description. #\u0026#39; It is not recommended to use @@title and @@description explicitly. #\u0026#39; @param x A numeric vector #\u0026#39; @export #\u0026#39; @examples #\u0026#39; increment(42) increment = function(x) {x + 1} #' から始まる行がroxygenコメントとして扱われる。\nタグは @ で始まる。 @ そのものを入力したいときは重ねて @@ とする。\n1行目にタイトル。1行あけて2段落目に説明文を書く。 明示的に @title, @description タグを使うことも可能だが非推奨らしい。 タイトルをコピペして全部同じにすると怒られる。 2段落目を省略するとタイトルが流用される。\n空行だけでは切れ目として扱われないので NULL などを置いたりする。\nRd形式の代わりにMarkdown形式で記述できる。 usethis::create_package() がデフォルトで Roxygen: list(markdown = TRUE) を DESCRIPTION に書いてくれる。 その全体設定をせず使いたいブロックにいちいち @md を書く個別設定も可能。\n\u0026quot;_PACKAGE\u0026quot; という文字列の上に書かれたブロックは、 パッケージそのものに対応するヘルプ *-package.Rd になる。 @useDynLib など全体に関わる設定はここでやるのが良いのでは。\n#\u0026#39; Example package to say hello #\u0026#39; @aliases NULL hello-package #\u0026#39; @useDynLib hello, .registration = TRUE #\u0026#39; @importFrom Rcpp sourceCpp #\u0026#39; @importFrom rlang .data := %||% #\u0026#39; @keywords internal \u0026#34;_PACKAGE\u0026#34; dplyrなどで列名を直に指定すると undefined global variables という警告が出るので dplyr::filter(diamonds, .data$cut == \u0026quot;Ideal\u0026quot;) のように pronoun を使って抑える。 そのためにどこかに #' @importFrom rlang .data を書いておく。 ただし $ によるアクセスがやや遅いので、 group_by() などで多数のグループを処理するときなど気になる場合は !!as.name(\u0026quot;carat\u0026quot;) のようにする。\nタグ 使用可能なタグ一覧は準備中？\n@import pkg1, pkg2, ... NAMESPACE で import() するパッケージを指定。 名前の衝突が怖いので基本的に使わない。 @importFrom pkg func NAMESPACE で importFrom() するパッケージと関数を指定。 パッケージ内でよほど何回も登場する関数や演算子を登録する。 e.g., @importFrom rlang .data 。 重複して何度も書いちゃっても大丈夫。 @export NAMESPACE で export() する関数を指定。 一般ユーザーからはこれが付いてる関数だけ見える。 @param arg1 description... 関数の引数。型や役割の説明を書く。 @inherit package::function [fields...] 別の関数から継承する。 部分的に継承したければ関数名の後に指定。 params return title description details seealso sections references examples author source note @inheritParams は @param の不足を補うショートカット。 @noRd と組み合わせて使えればいろいろ楽できそうだけどダメっぽい。 @template template-name man-roxygen/template-name.R の内容を利用する。 @inheritParams と違って、その関数で使用しないparamまで展開されちゃうのが欠点。 @eval fun() 関数を評価して出てきた文字列ベクタをroxygenコメントとして処理する。 各要素の先頭は #' ではなく@タグで。 関数には引数も渡せるのでいろいろできる。 @return description 関数の返り値。 @examples code... 例となるコードを記述する。 exportしないやつには書いてはいけない。 \\dontrun{} に入れるとチェックから除外される。 単数形の @example は外部ファイルのパスを受け取る。 @rdname basename man/に書き出すRdファイルの名前。 複数の関数で同じものを指定すればヘルプをひとつにまとめられる。 このときタイトルや @param などは共有される。 似たような引数を持つ関数や、関連する関数をまとめるのによく使う。 @include other-file.R 指定したファイルを先に読み込む。 メソッドの定義などで順序が重要になる場合に使う。 @seealso ... [mean()], [ggplot2::diamonds], \u0026lt;https://...\u0026gt; のようにしてリンクを張れる。 @family これを共通して持つ関数同士に @seealso が自動生成される。 ただし @rdname とかで関数をまとめたりしてるとうまくいかないっぽい。 @section Some Title: 新しいセクションを作る。前には空行、行末にコロン、後には普通の文章。 @docType, @name パッケージやデータを記述するのに必要だったが今では不要っぽい。 関連書籍 ",
  "href": "/rstats/devtools.html",
  "tags": [
   "r",
   "tidyverse",
   "package"
  ],
  "title": "devtools",
  "type": "rstats"
 },
 {
  "content": "https://docs.docker.com/\nホストOSから隔離された環境でアプリケーションを動かすためのプラットフォーム。 ゲストOSを丸ごと動かす仮想マシンとは異なり、ホストLinuxのカーネルを使うので軽い。 LinuxじゃないホストOSでは仮想化が挟まるぶんのコストが結局かかる。\n管理者権限を持っている状況でサービスを開発・運用するのに向いている。 HPCにおけるCLIツール導入や一般ユーザーからの利用が目的なら Apptainer のほうが便利。\nGet Started Glossary container ホストOSから隔離されたプロセスやファイルシステムを持つ実行環境。 目的のアプリケーションを動かすのに必要な構成要素のうちkernelより上をすべて含む。 imageを読み込んで起動したインスタンス。 image containerの構成要素を集めて固めたファイル。 Docker独自の形式をベースにOCIとして標準化されている。 dockerfile imageのレシピ。 Linux https://docs.docker.com/engine/install/ubuntu/\napt repository を追加してインストール。\nsudo なしで実行できるようにするには:\nsudo gpasswd --add $(whoami) docker sudo chgrp docker /var/run/docker.sock sudo systemctl restart docker exit # login again docker --version Mac FormulaではなくCaskのほうの Docker Desktop を入れるのが簡単:\nbrew install --cask docker open -a Docker ユーザーのホームにCLIのシムリンクができるのでパスを通す:\nPATH=${HOME}/.docker/bin:$PATH docker --version docker-compose --version Use Virtualization framework VirtioFS. 古い gRPC FUSE よりも速いらしい。 サードパーティのmutagenはもっと速いらしい。 CPUやRAMの上限など適宜変更。 Hello world docker image pull hello-world docker image ls docker container run hello-world docker container ls --all run で手元にimageが見つからなければ勝手に pull される。 containerは実行終了後も残る。\nCLI 操作対象がcontrainerかimageかなどによってサブコマンドを使う。 前は docker pull, docker run のようにフラットなコマンド体系だった。\ndocker container subcommands https://docs.docker.com/engine/reference/commandline/container/\ndocker container ls 実行中のcontainerを表示。 -a: 終了後のものも表示。 -q: IDのみ表示。 docker container rm CONTAINER containerを削除。 停止中のものを一括削除するなら docker container prune docker container run [OPTIONS] IMAGE [COMMAND] 新しいcontainerを走らせてコマンドを実行。 -i, --interactive: 標準入力を開けておく。 -t, --tty: 標準出力を開けておく。 -d, --detach: バックグラウンドで実行させておく。 うっかりフォアグラウンドで起動してしまっても ^p^q でデタッチできる。 --rm: 終了後に削除。これを指定しないと ls -a のリストに残る。 --name string: コンテナに名前を付ける。 exec で指定するときに便利。 --mount: containerから外のファイルシステムにアクセスできるように割り当てる。 type=volume,src=\u0026lt;VOLUME-NAME\u0026gt;,dst=\u0026lt;CONTAINER-PATH\u0026gt;: Docker管理下に場所を確保する。 container再起動やcontainer間共有のための永続化にはこっちを使う。 Docker外でも使うデータを読み書きするのには向かない。 type=bind,src=\u0026lt;HOST-PATH\u0026gt;,dst=\u0026lt;CONTAINER-PATH\u0026gt;: ホスト側の場所を直接指定する。 古い -v オプションによる指定は非推奨。 --mount type=bind,src=\u0026quot;$PWD\u0026quot;,dst=\u0026quot;$PWD\u0026quot;\n--workdir \u0026quot;$PWD\u0026quot;\n--user \u0026quot;$(id -u):$(id -g)\u0026quot;\nのようにするとcontainer内のプログラムにカレント以下を渡せる。 Apptainerならこのへんの設定をデフォルトでやってくれる。 docker container exec [OPTIONS] IMAGE COMMAND 既に走ってるcontainerでコマンドを実行。 docker container stats\ndocker image subcommands https://docs.docker.com/engine/reference/commandline/image/\ndocker image ls 手元にあるimageを一覧表示。 docker image pull NAME:TAG Docker Hub などからimageをダウンロード。 misc. docker system info docker system df Registry BioContainers BioConda receipes を使っているらしい。 実際のregistry機能をホストしているのは他所のサーバー: https://hub.docker.com/u/biocontainers OCI https://quay.io/organization/biocontainers OCI https://depot.galaxyproject.org/singularity/ SIF. singularity-build-bot がQuayからOCIを取得して変換。 通信が遅く、~500MB以上のイメージをpullしようとするとタイムアウトになって使えない。 一旦 wget か何かで落とすか、自分でOCIから変換するほうが早くて確実。 Docker Hub 100 pulls / 6 hours の制限がある。 無料のPersonalアカウントで認証していれば200。 Quay.io 無料アカウントは無い。pull制限は? Google Container Registry シェルさえ含まず軽量セキュアな distroless を提供。 GitHub Container Registry (GitHub Packages)\nGitLab Container Registry\n",
  "href": "/dev/docker.html",
  "tags": [
   "package"
  ],
  "title": "Docker",
  "type": "dev"
 },
 {
  "content": " data.frameに対して抽出(select, filter)、部分的変更(mutate)、要約(summarize)、ソート(arrange)などの処理を施すためのパッケージ。 前作plyrのうちdata.frameに関する部分が強化されている。 purrr や tidyr と一緒に使うとよい。\ntidyverse に含まれているので、 install.packages(\u0026quot;tidyverse\u0026quot;) で一括インストール、 library(tidyverse) で一括ロード。\nhttps://r4ds.hadley.nz/data-transform.html https://github.com/tidyverse/dplyr パイプ演算子 |\u0026gt; による関数の連結 x |\u0026gt; f(a, b) は f(x, a, b) と等価。 左の値 x を第一引数として右の関数 f() に渡す。 一時変数を作ったり、関数を何重にも重ねたりすることなく、 適用する順に次々と処理を記述することができるようになる。 慣れれば書きやすく読みやすい。\nlibrary(conflicted) library(tidyverse) result = diamonds |\u0026gt; # 生データから出発して dplyr::select(carat, cut, price) |\u0026gt; # 列を抽出して dplyr::filter(carat \u0026gt; 1) |\u0026gt; # 行を抽出して dplyr::group_by(cut) |\u0026gt; # グループ化して dplyr::summarize(mean(price)) |\u0026gt; # 平均を計算 print() # 表示してみる cut mean(price) 1 Fair 7177.856 2 Good 7753.601 3 Very Good 8340.549 4 Premium 8487.249 5 Ideal 8674.227 同じことをパイプなしでやると読みにくい:\n## with a temporary variable result = dplyr::select(diamonds, carat, cut, price) # 列を抽出して result = dplyr::filter(result, carat \u0026gt; 1) # 行を抽出して result = dplyr::group_by(result, cut) # グループ化して result = dplyr::summarize(result, mean(price)) # 平均を計算 ## with nested functions result = dplyr::summarize( # 平均を計算 dplyr::group_by( # グループ化して dplyr::filter( # 行を抽出して dplyr::select(diamonds, carat, cut, price), # 列を抽出して carat \u0026gt; 1), # 行を抽出して cut), # グループ化して mean(price)) # 平均を計算 R 4.1 から標準で |\u0026gt; が使えるようになり、徐々に普及してきた。 それまでdplyr始めtidyverseでは magrittr の %\u0026gt;% が長らく使われていた。 R 4.2 の段階では |\u0026gt; のプレースホルダー _ の使い勝手がイマイチなので、 %\u0026gt;% から完全移行できる状態ではない。\nさらに高性能なものを求める人々は pipeR パッケージの %\u0026gt;\u0026gt;% を使っていたが最近はあまり見かけない。\n抽出・絞り込み 列 dplyr::select(.data, ...) 列を絞る。:範囲指定、!負の指定が可能。 selection helpers によるパターン指定も便利。 複数の条件を組み合わせるには \u0026amp; (AND), | (OR) で。 残るのが1列だけでも勝手にvectorにはならずdata.frameのまま。 diamonds |\u0026gt; dplyr::select(1, 2, 7) diamonds |\u0026gt; dplyr::select(carat, cut, price) diamonds |\u0026gt; dplyr::select(c(\u0026#34;carat\u0026#34;, \u0026#34;cut\u0026#34;, \u0026#34;price\u0026#34;)) diamonds |\u0026gt; dplyr::select(!c(carat, cut, price)) diamonds |\u0026gt; dplyr::select(starts_with(\u0026#34;c\u0026#34;)) diamonds |\u0026gt; dplyr::select(where(is.numeric)) # diamonds |\u0026gt; dplyr::select(-carat, -cut, -price) カンマ区切りはORの意味で働くはずだがマイナス指定 dplyr::select(-carat, -cut, -price) の場合だけANDのように働く(つまり3列とも抜ける)ので特殊。 (だからマニュアルから消えたのかな？)\n文字列ベクタで指定しようとすると意図が曖昧になるので、 ヘルパー関数 all_of(), any_of()を挟む。あるいは {{embrace}}, !!unquoting, pronoun$を使う:\nclarity = c(\u0026#34;carat\u0026#34;, \u0026#34;cut\u0026#34;, \u0026#34;price\u0026#34;) diamonds |\u0026gt; dplyr::select(clarity) # ambiguous! diamonds |\u0026gt; dplyr::select(.data$clarity) # clarity diamonds |\u0026gt; dplyr::select(all_of(clarity)) # carat, cut, price diamonds |\u0026gt; dplyr::select(any_of(clarity)) # carat, cut, price diamonds |\u0026gt; dplyr::select({{clarity}}) # carat, cut, price diamonds |\u0026gt; dplyr::select(!!clarity) # carat, cut, price diamonds |\u0026gt; dplyr::select(!!!rlang::syms(clarity)) # carat, cut, price これらの指定方法は rename() や pull() でも有効。\n一方、文字列を受け取れない distinct() や group_by() などの関数には普通のunquoteは通用しない。 最後の例のように rlang::data_syms() でシンボルのリストを作って !!!unquote-splicing して渡す必要がある。\ncolumns = c(\u0026#34;cut\u0026#34;, \u0026#34;color\u0026#34;) diamonds |\u0026gt; distinct(!!as.name(columns[1L])) diamonds |\u0026gt; distinct(!!!rlang::data_syms(columns)) 詳しくは rlangパッケージのドキュメント か 宇宙船本第3章のコラム 「selectのセマンティクスとmutateのセマンティクス」を参照。\ndplyr::pull(.data, var = -1, name = NULL) 指定した1列をvector(またはlist)としてdata.frameから抜き出す。 diamonds |\u0026gt; head() |\u0026gt; dplyr::pull(price) diamonds |\u0026gt; head() |\u0026gt; dplyr::pull(\u0026#34;price\u0026#34;) diamonds |\u0026gt; head() |\u0026gt; dplyr::pull(7) diamonds |\u0026gt; head() |\u0026gt; dplyr::pull(-4) diamonds |\u0026gt; head() %\u0026gt;% `[[`(\u0026#34;price\u0026#34;) diamonds |\u0026gt; head() %\u0026gt;% {.[[\u0026#34;price\u0026#34;]]} name オプションにID的な列を渡すと名前付きvectorを楽に作れる: starwars |\u0026gt; dplyr::pull(mass, name) 行 dplyr::filter(.data, ..., .by = NULL, .preserve = FALSE) 条件を満たす行だけを返す。base::subset() と似たようなもの。 diamonds |\u0026gt; dplyr::filter(carat \u0026gt; 3 \u0026amp; price \u0026lt; 10000) carat cut color clarity depth table price x y z 1 3.01 Premium I I1 62.7 58 8040 9.10 8.97 5.67 2 3.11 Fair J I1 65.9 57 9823 9.15 9.02 5.98 3 3.01 Premium F I1 62.2 56 9925 9.24 9.13 5.73 評価結果が NA となる行は除去される。 特に不等号を使うときやや直感に反するので要注意。 e.g., filter(gene != \u0026quot;TP53\u0026quot;)\n複数列で条件指定するには if_any(), if_all() が使える。\ndiamonds |\u0026gt; dplyr::filter(if_any(c(x, y, z), \\(v) v \u0026gt; 20)) carat cut color clarity depth table price x y z 1 2.00 Premium H SI2 58.9 57.0 12210 8.09 58.90 8.06 2 0.51 Very Good E VS1 61.8 54.7 1970 5.12 5.15 31.80 3 0.51 Ideal E VS1 61.8 55.0 2075 5.15 31.80 5.12 diamonds |\u0026gt; dplyr::filter(if_all(where(is.numeric), \\(x) x \u0026gt; 4.1)) carat cut color clarity depth table price x y z 1 4.13 Fair H I1 64.8 61 17329 10.00 9.85 6.43 2 5.01 Fair J I1 65.5 59 18018 10.74 10.54 6.98 3 4.50 Fair J I1 65.8 58 18531 10.23 10.16 6.72 dplyr::distinct(.data, ..., .keep_all = FALSE) 指定した列に関してユニークな行のみ返す。 base::unique.data.frame() よりも高速で、 filter(!duplicated(.[, ...])) よりスマートで柔軟。 指定しなかった列を残すには .keep_all = TRUE とする。 diamonds |\u0026gt; dplyr::distinct(cut) cut 1 Ideal 2 Premium 3 Good 4 Very Good 5 Fair dplyr::slice(.data, ..., .by = NULL, .preserve = FALSE) 行番号を指定して行を絞る。 `[`(i,) の代わりに。 diamonds |\u0026gt; dplyr::slice(1, 2, 3) carat cut color clarity depth table price x y z 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 dplyr::slice_head(.data, ..., n, prop), slice_tail() 先頭・末尾の行を抽出。 dplyr::slice_max(.data, order_by, ..., n, prop, by = NULL, with_ties = TRUE, na_rm = FALSE) order_by, ... で指定した列の降順で n 行だけ返す。 境界のタイを保持する場合は n 行以上の出力になる。 昇順で取りたいときはマイナス指定か slice_min()。 dplyr::slice_sample(.data, ..., n, prop, weight_by = NULL, replace = FALSE) 指定した行数・割合だけランダムサンプルする。 列の変更・追加 dplyr::mutate(.data, ..., .by = NULL, .keep = \u0026quot;all\u0026quot;, .before = NULL, .after = NULL) 新しい列を作ったり、既存の列を変更したり。 base::transform() の改良版。 diamonds |\u0026gt; dplyr::mutate(gram = 0.2 * carat) |\u0026gt; # create new dplyr::mutate(price = price * 140) # modify existing carat cut color clarity depth table price x y z gram 1 0.23 Ideal E SI2 61.5 55 45640 3.95 3.98 2.43 0.046 2 0.21 Premium E SI1 59.8 61 45640 3.89 3.84 2.31 0.042 -- 53939 0.86 Premium H SI2 61.0 58 385980 6.15 6.12 3.74 0.172 53940 0.75 Ideal D SI2 62.2 55 385980 5.83 5.87 3.64 0.150 変数に入った文字列を列名として使いたい場合は上記 select() のときと同様、 {{embrace}} や !!unquote を使う。 A = \u0026#34;carat\u0026#34; B = \u0026#34;gram\u0026#34; diamonds |\u0026gt; dplyr::mutate(B = 0.2 * !!as.name(A)) carat cut color clarity depth table price x y z B 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 0.046 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 0.042 -- 53939 0.86 Premium H SI2 61.0 58 2757 6.15 6.12 3.74 0.172 53940 0.75 Ideal D SI2 62.2 55 2757 5.83 5.87 3.64 0.150 左辺(代入先)の名前も文字列を使って表現したい場合は walrus(セイウチ)演算子 := を使う: diamonds |\u0026gt; dplyr::mutate({{B}} := 0.2 * !!as.name(A)) carat cut color clarity depth table price x y z gram 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 0.046 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 0.042 -- 53939 0.86 Premium H SI2 61.0 58 2757 6.15 6.12 3.74 0.172 53940 0.75 Ideal D SI2 62.2 55 2757 5.83 5.87 3.64 0.150 .keep = \u0026quot;all\u0026quot;: すべての列を残すデフォルト。 .keep = \u0026quot;used\u0026quot;: 入力に使った列と出力の列のみ残す。 .keep = \u0026quot;unused\u0026quot;: 入力に使った列は捨て、出力の列とほかの列を残す。 .keep = \u0026quot;none\u0026quot;: グループ化と出力の列のみ残す。旧 transmute() のような挙動。 新しい列をどこに作るかを .before, .after 指定できる。 複数の列をtidyselectして1列作りたいときは dplyr::pick(...) と Reduce(f, x) / purrr::reduce(.x, .f) を組み合わせる:\ndiamonds |\u0026gt; dplyr::mutate(prod = Reduce(`*`, pick(where(is.numeric)))) |\u0026gt; dplyr::mutate(mean_xyz = Reduce(`+`, pick(x:z)) / ncol(pick(x:z))) |\u0026gt; dplyr::mutate(max_xyz = Reduce(pmax, pick(x:z))) |\u0026gt; print() dplyr::select() と違って .data を渡さなくてもいいので、 プレースホルダの貧弱な base pipe |\u0026gt; でも使える。\ndplyr::rename(.data, ...) 列の改名。 mutate()と同じようなイメージで new = old と指定。 diamonds |\u0026gt; dplyr::rename(SIZE = carat) SIZE cut color clarity depth table price x y z 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 -- 53939 0.86 Premium H SI2 61.0 58 2757 6.15 6.12 3.74 53940 0.75 Ideal D SI2 62.2 55 2757 5.83 5.87 3.64 変数に入った文字列を使う場合もmutate()と同様に {{embrace}} や !!unquote で: old_name = \u0026#34;carat\u0026#34; new_name = toupper(old_name) diamonds |\u0026gt; dplyr::rename({{new_name}} := {{old_name}}) CARAT cut color clarity depth table price x y z 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 -- 53939 0.86 Premium H SI2 61.0 58 2757 6.15 6.12 3.74 53940 0.75 Ideal D SI2 62.2 55 2757 5.83 5.87 3.64 名前付きベクターと !!!unquote-splicing を使えば一括指定できる:\nnamed_vec = setNames(names(diamonds), LETTERS[seq(1, 10)]) diamonds |\u0026gt; dplyr::rename(!!!named_vec) A B C D E F G H I J 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 -- 53939 0.86 Premium H SI2 61.0 58 2757 6.15 6.12 3.74 53940 0.75 Ideal D SI2 62.2 55 2757 5.83 5.87 3.64 rename_with(.tbl, .fn, .cols = everything(), ...) はリネーム関数を渡せる亜種: diamonds |\u0026gt; dplyr::rename_with(toupper, everything()) data.frameの要約・集計・整列 dplyr::summarize(.data, ..., .by = NULL, .group = NULL) 指定した列に関数を適用して1行のdata.frameにまとめる。 グループ化されていたらグループごとに適用して bind_rows() する。 diamonds |\u0026gt; group_by(cut) |\u0026gt; summarize(avg_carat = mean(carat), max_price = max(price)) cut avg_carat max_price 1 Fair 1.0461366 18574 2 Good 0.8491847 18788 3 Very Good 0.8063814 18818 4 Premium 0.8919549 18823 5 Ideal 0.7028370 18806 複数カラムに関数を適用するには across() を使う:\n# summarize_at diamonds |\u0026gt; summarize(across(starts_with(\u0026#34;c\u0026#34;), max), .by = cut) # summarize_if diamonds |\u0026gt; summarize(across(where(is.numeric), max), .by = cut) # summarize_all diamonds |\u0026gt; summarize(across(everything(), max), .by = cut) 各列の計算結果は長さ1でなければいけない。 そうじゃなくても大丈夫な時期もあったが、その機能は reframe() に分離された。\ndplyr::reframe(.data, ..., .by = NULL, .group = NULL) 各グループの結果が複数行にまたがっても大丈夫な summarize() 亜種。 diamonds |\u0026gt; dplyr::reframe(range(carat), range(price)) range(carat) range(price) 1 0.20 326 2 5.01 18823 各グループの結果がtibbleの場合、結合・展開してくれる:\ndiamonds |\u0026gt; dplyr::nest_by(cut) |\u0026gt; dplyr::reframe(head(data, 2L)) これを利用して複数ファイルを一気読みできる:\npath = fs::dir_ls(\u0026#34;path/to/data\u0026#34;, glob = \u0026#34;*.tsv\u0026#34;) df = tibble(path) |\u0026gt; dplyr::rowwise() |\u0026gt; dplyr::reframe(readr::read_tsv(path)) けどまあ purrr::map(path, read_tsv) |\u0026gt; purrr::list_rbind() のほうが読みやすい気がする。\ndplyr::tally(x, wt, sort = FALSE) summarize(x, n = n()) のショートカット。 wt にカラムを指定して重み付けすることもできる。 dplyr::add_tally() は元の形を維持したままカウント列を追加。 dplyr::count(x, ..., wt = NULL, sort = FALSE) group_by(...) |\u0026gt; tally() のショートカット。 dplyr::add_count() は元の形を維持したままカウント列を追加。 dplyr::arrange(.data, ..., .by_group = FALSE) 指定した列の昇順でdata.frameの行を並べ替える。 arrange(desc(column)) で降順になる。 order() を使うよりもタイピングの繰り返しが少ないし直感的 mtcars[order(mtcars$cyl, mtcars$disp), ] # is equivalent to mtcars |\u0026gt; dplyr::arrange(cyl, disp) dplyr::relocate(.data, ..., .before = NULL, .after = NULL) 指定した列を左端(もしくは .before/.after 別の列)に移動する。 data.frameを結合 dplyr::***_join(x, y, by = NULL, copy = FALSE) by で指定した列がマッチするように行を合わせて cbind() full_join(): x と y の全ての行を保持。\ninner_join(): x と y の by がマッチする行のみ\nleft_join(): x の全ての行を保持。y に複数マッチする行があったらすべて保持。\nright_join(): y の全ての行を保持。x に複数マッチする行があったらすべて保持。\nsemi_join(): x の全ての行を保持。y に複数マッチする行があっても元の x の行だけ保持。\nanti_join(): y にマッチしない x の行のみ。\ncross_join(): マッチするものではなく、全ての組み合わせを作って結合。\n列名が異なる場合は by を名前付きvectorにする:\ndplyr::full_join(band_members, band_instruments2, by = c(name = \u0026#34;artist\u0026#34;)) join_by() を使えば不等号などを含めて柔軟に結合条件を指定できる。 dplyr::bind_rows(...), dplyr::bind_cols(...) 標準の rbind(), cbind() より効率よくdata.frameを結合。 引数は個別でもリストでもよい。 そのほかにも標準の集合関数を置き換えるものが提供されている: intersect(), union(), union_all(), setdiff(), setequal() その他の関数 主にmutate()やfilter()を補助するもの\ndplyr::if_else(condition, true, false, missing = NULL) 標準のifelse()よりも型に厳しく、高速らしい。 NAのときにどうするかを指定できるのも大変良い。 ネストせずにスッキリ書ける dplyr::case_when() も便利。 dplyr::coalesce(x, ...) 最初のvectorでNAだったとこは次のvectorのやつを採用、 というifelse(!is.na(x), x, y)的な処理をする。 基本的には同じ長さのvectorを渡すが、 2つめに長さ1のを渡してtidyr::replace_na()的に使うのも便利。 dplyr::na_if(x, y) x[x == y] = NA; x のショートカット dplyr::recode(.x, ..., .default = NULL, .missing = NULL) vectorの値を変更する。e.g., recode(letters, a = \u0026quot;A!\u0026quot;, c = \u0026quot;C!\u0026quot;) dplyr::row_number(x) rank(x, ties.method = \u0026quot;first\u0026quot;, na.last = \u0026quot;keep\u0026quot;) のショートカット。 グループ毎に連番を振るのに便利。同点でも登場順に違う順位を与える。 dplyr::min_rank(x): 同点には同じ値。2位タイがあれば3位不在。ties.method = \u0026quot;min\u0026quot; dplyr::dense_rank(x): 同点には同じ値。2位タイがあっても3位から再スタート。 dplyr::consecutive_id(...) 値が変化したら番号をインクリメント。こういうグループ化したいときがある: c(0, 0, 1, 1, 1, 0, 0, 1) |\u0026gt; dplyr::consecutive_id() [1] 1 1 2 2 2 3 3 4 dplyr::ntile(x, n) 数値ベクトル x を順位によって n 個のクラスに均等分け dplyr::n_distinct(x) 高速で簡潔な length(unique(x)) dplyr::n_groups(x) グループ数。 dplyr::last(x, order_by = NULL, default = default_missing(x)) 最後の要素にアクセス。 x[length(x)] や tail(x, 1) よりも楽チンだが、 安全性重視の dplyr::nth() を内部で呼び出すため遅い。 dplyr::lead(x n = 1, default = NA, order_by = NULL), dplyr::lag(...) x の中身を n だけずらして default で埋める。 lead() は前に、lag() は後ろにずらす lag(seq_len(5), 2) [1] NA NA 1 2 3 ## [1] NA NA 1 2 3 dplyr::between(x, left, right) left \u0026lt;= x \u0026amp; x \u0026lt;= right のショートカット。 dplyr::near(x, y, tol = .Machine$double.eps^0.5) abs(x - y) \u0026lt; tol のショートカット。 dplyr::case_when(...) if {} else if {} else if {} ... のショートカット。 グループ化 tidyr でネストして、 purrr でその list of data.frames に処理を施し、 dplyr でその変更を元の data.frame に適用する、 というのがtidyverse流の柔軟なやり方。\nv0.8.1で group_modify() などが導入され、 ネストせずグループ化までで済ませやすくなった。\nv1.1.0 からは多くの関数に一時グループ化 .by / by オプションが追加され、 事前のグループ化と事後のグループ解除を省略しやすくなった。\ndiamonds |\u0026gt; dplyr::group_nest(cut) |\u0026gt; dplyr::mutate(data = purrr::map(data, \\(x) head(x, n = 2L))) |\u0026gt; tidyr::unnest() # dplyr \u0026gt;= 0.8.1 diamonds |\u0026gt; dplyr::group_by(cut) |\u0026gt; dplyr::group_modify(\\(x, key) head(x, 2L)) |\u0026gt; dplyr::ungroup() # dplyr \u0026gt;= 1.1.0 diamonds |\u0026gt; dplyr::slice_head(n = 2L, by = cut) dplyr::group_by(.data, ..., add = FALSE, .drop = group_by_drop_default(.data)) グループごとに区切って次の処理に渡す。 e.g. summarize(), slice(), tally(), group_modify() など .drop = FALSE とすると行数ゼロになるグループも捨てずに保持できる。 dplyr::group_data(.data) グループ情報を参照: diamonds |\u0026gt; dplyr::group_by(cut) |\u0026gt; dplyr::group_data() cut .rows 1 Fair \u0026lt;int [1610]\u0026gt; 2 Good \u0026lt;int [4906]\u0026gt; 3 Very Good \u0026lt;int [12082]\u0026gt; 4 Premium \u0026lt;int [13791]\u0026gt; 5 Ideal \u0026lt;int [21551]\u0026gt; 左側のキー列だけ欲しければ dplyr::group_keys() 、\n左端の行番号だけ欲しければ dplyr::group_rows() 。\ndplyr::group_nest(.tbl, ..., .key = \u0026quot;data\u0026quot;, keep = FALSE) 入れ子 data.frame を作る。 tidyr::nest(.by = c(...)) と役割が被りすぎて微妙な立場らしい。 keep = TRUE とするとグループ化に使った列も入れ子のdata.frameに残す。 グループ化を解除してフラットな tbl_df を返す。 dplyr::nest_by(.tbl, ..., .key = \u0026quot;data\u0026quot;, keep = FALSE) 上記 group_nest とほぼ同じだが、こちらはグループ化されたまま rowwise_df を返す。 dplyr::group_split(.tbl, ..., .key = \u0026quot;data\u0026quot;, .keep = FALSE) list of data.frames に分割する。 .tbl |\u0026gt; split(.$group) と同等だが、 ドットダラーを使わくて済むし複数列をキーにするのも簡単。 dplyr::group_indices(.data, ...) grouped_df ではなくグループIDとして1からの整数列を返す版 group_by() dplyr::group_modify(.tbl, .f, ..., .keep = FALSE) グループごとに .f を適用して再結合したdata.frameを返す。 .f は2つの引数をとる関数。 1つめは区切られたdata.frame、 2つめはグループ化のキー(となった列を含む1行のdata.frame)。 引数が1つだったり、違うものを2つめに受け取る関数はそのままじゃ渡せないので、 purrrのときと同様に無名関数に包んで渡す。 diamonds |\u0026gt; dplyr::group_by(cut) |\u0026gt; dplyr::group_modify(\\(x, key) head(x, 2L)) cut carat color clarity depth table price x y z 1 Fair 0.22 E VS2 65.1 61 337 3.87 3.78 2.49 2 Fair 0.86 E SI2 55.1 69 2757 6.45 6.33 3.52 3 Good 0.23 E VS1 56.9 65 327 4.05 4.07 2.31 4 Good 0.31 J SI2 63.3 58 335 4.34 4.35 2.75 -- 7 Premium 0.21 E SI1 59.8 61 326 3.89 3.84 2.31 8 Premium 0.29 I VS2 62.4 58 334 4.20 4.23 2.63 9 Ideal 0.23 E SI2 61.5 55 326 3.95 3.98 2.43 10 Ideal 0.23 J VS1 62.8 56 340 3.93 3.90 2.46 group_map() は結果を bind_rows() せずlistとして返す亜種。\ngroup_walk() は .f 適用前の .tbl を返す亜種。\ndplyr::rowwise(data, ...) 受け取ったdataに rowwise_df クラスを付与して返す。 これは1行ごとにグループ化された grouped_df のようなもので、 mutate などを適用すると列全体ではなく1行ごとに関数に渡される。 一旦非推奨となったがv1.0.0で蘇った。 ループ処理が重いので、計算内容と行数のバランスに注意。 例えば dplyr::c_across() の併用で横方向の合計や平均を簡潔に書けるが、 ベクトル演算を使う場合に比べてかなり遅い。 diamonds |\u0026gt; dplyr::rowwise() |\u0026gt; dplyr::mutate(mean_xyz = mean(c_across(x:z))) # slow diamonds |\u0026gt; dplyr::mutate(mean_xyz = Reduce(`+`, pick(x:z)) / ncol(pick(x:z))) # fast deprecated/superceeded dplyr::do(.data, ...) reframe(), slice*() などに取って代わられた。 diamonds |\u0026gt; dplyr::group_by(cut) |\u0026gt; dplyr::do(head(., 2L)) diamonds |\u0026gt; dplyr::slice_head(n = 2L, by = cut) diamonds |\u0026gt; dplyr::reframe(across(everything(), range), .by = cut) dplyr::transmute(.data, ...) 指定した列以外を保持しない版 mutate() 。 言い換えると、列の中身の変更もできる版 select() 。 mutate(.keep = \u0026quot;none\u0026quot;) に取って代わられた。 *_at(), *_if(), *_all(), *_each() 条件を満たす列・複数の列に関数を適用する mutate(), summarize() 亜種。 across() の登場により非推奨。 top_n(), top_frac() slice_min(), slice_max() に取って代わられた。 sample_n(), sample_frac() slice_sample() に取って代わられた。 matrix, array data.frame を主眼とする dplyr では matrix や array を扱わない。 一時期存在していた tbl_cube 関連の機能は cubelyr に隔離された。\nas.tbl_cube(x, dim_names, met_names, ...) matrix/arrayからdata.frameの一歩手前に変換する。 reshape2::melt の改良版。 これの結果に tibble::as_tibble() を適用するとわかりやすい。 ただし dimnames(x) が空ではダメで、長さの正しい名前付きlistになっている必要がある。 # deprecated iris3 |\u0026gt; reshape2::melt() |\u0026gt; tibble::as_tibble() |\u0026gt; dplyr::rename(obs = Var1, metrics = Var2, species = Var3) # new x = iris3 dimnames(x)[[1L]] = seq_len(dim(iris3)[[1L]]) names(dimnames(x)) = c(\u0026#34;obs\u0026#34;, \u0026#34;metrics\u0026#34;, \u0026#34;species\u0026#34;) cubelyr::as.tbl_cube(x, met_name = \u0026#34;value\u0026#34;) |\u0026gt; as_tibble() 関連書籍 ",
  "href": "/rstats/dplyr.html",
  "tags": [
   "r",
   "tidyverse"
  ],
  "title": "dplyr",
  "type": "rstats"
 },
 {
  "content": "https://bioconductor.org/packages/release/bioc/html/edgeR.html\nRobinson MD, McCarthy DJ, Smyth GK (2010) \u0026ldquo;edgeR: a Bioconductor package for differential expression analysis of digital gene expression data.\u0026rdquo; Bioinformatics 26 (1):139\u0026ndash;140 https://www.ncbi.nlm.nih.gov/pubmed/19910308\nBioconductor パッケージとしてインストール\nBiocManager::install(\u0026#34;edgeR\u0026#34;) ユーザーガイドPDFを開く\nedgeRUsersGuide() とても良くできたドキュメントなので必読\n使い方 HTSeq などで遺伝子ごとのリードカウントを用意する\nRで読み込む\nlibrary(edgeR) targets = data.frame(group=c(\u0026#34;control\u0026#34;, \u0026#34;case\u0026#34;), files=c(\u0026#34;control.txt\u0026#34;, \u0026#34;case.txt\u0026#34;)) dge = readDGE(targets, header = FALSE) 低カウント過ぎる遺伝子を除去\nok_c = (dge$counts \u0026gt; 5) |\u0026gt; rowSums() |\u0026gt; as.logical() ok_cpm = (cpm(dge) \u0026gt; 1) |\u0026gt; rowSums() |\u0026gt; as.logical() dge = dge[ok_c \u0026amp; ok_cpm, , keep.lib.sizes = FALSE] 正規化係数を計算\ndge = calcNormFactors(dge) dge$samples モデル\ndesign = model.matrix(~ 0 + group, data = targets) common dispersion, trended dispersion, tagwise dispersionを推定\ndge = estimateDisp(dge, design) ただしこれにはbiological replicatesが必要。 1グループ1サンプルずつしか無い場合は4つの選択肢がある。\n検定を諦めてdescriptive discussion (推奨)\n経験的な値を適当に入れとく\nbcv = 0.4 # for human bcv = 0.1 # for genetically identical model organisms bcv = 0.01 # for technical replicates dge$common.dispersion = bcv ^ 2 説明変数を減らしたモデルでdispersionを推定し、フルモデルで使う\nreduced = estimateGLMCommonDisp(dge, reduced.design, method=\u0026#34;deviance\u0026#34;, robust=TRUE, subset=NULL) dge$common.dispersion = reduced$common.dispersion 合計カウントが多くてDEGが比較的少ないときに有効。 当然、1変数2群比較では使えない。\n群間で差がないと考えられるhousekeeping genesから推定。 100個以上の遺伝子を使うのが望ましい。 例えばヒトなら\nsystem(\u0026#34;wget http://www.tau.ac.il/~elieis/HKG/HK_genes.txt\u0026#34;) hk_genes = read_tsv(\u0026#34;HK_genes.txt\u0026#34;, col_names=c(\u0026#34;gene_symbol\u0026#34;, \u0026#34;refseq\u0026#34;)) tmp = dge tmp$samples$group = 1 housekeeping = rownames(tmp$counts) %in% hk_genes$gene_symbol hk = estimateCommonDisp(tmp[housekeeping,]) dge$common.dispersion = hk$common.dispersion 1変数2群ならシンプルに検定\net = exactTest(dge) topTags(et, 20, adjust.method=\u0026#34;BH\u0026#34;, sort.by=\u0026#34;PValue\u0026#34;) 多群ならGLMで尤度比検定\nfit = glmFit(dge, design) lrt = glmLRT(fit, coef=2:3) lrt_1vs2 = glmLRT(fit, coef=2) lrt_1vs3 = glmLRT(fit, coef=3) lrt_2vs3 = glmLRT(fit, contrast=c(0, -1, 1)) 検定結果からDEGを抜き出す\nmin_lfc = 1 de = decideTestsDGE(et, adjust.method=\u0026#34;BH\u0026#34;, p.value=0.01, lfc=min_lfc) de_tags = rownames(dge)[as.logical(de)] P値とlogFCの両方で切ったほうがいいらしい。\ntopTags() や decideTestsDGE() で多重検定の補正をするということは、 et や lrt の PValue は補正前。\nプロット logFC ~ mean(logCPM) してみる\nplotSmear(et, de.tags=de_tags) abline(h=min_lfc * c(-1, 1), col=\u0026#34;blue\u0026#34;) Gene Ontology解析\ngo = goana(lrt, species=\u0026#34;Hs\u0026#34;) topGO(go) NCBI RefSeqアノテーションを使うのでIDはEntrez系で\nAppendix レプリケート i における遺伝子 g の観察リード数を $y _{gi}$、 知りたい真の発現fractionを $\\pi _{gi}$ とする。\n\\[\\begin{aligned} \\sum _g \\pi _{gi} \u0026= 1 \\\\ \\sqrt {\\phi _g} \u0026\\equiv \\text{CV}[\\pi _{gi}]_i = \\frac {\\text{sd}[\\pi _{gi}]_i} {\\text{mean}[\\pi _{gi}]_i} \\\\ N_i \\pi _{gi} \u0026\\sim \\text{Gamma}(\\phi _g^{-1}, \\mu _{gi} \\phi _g) \\\\ \\mathrm E[N_i \\pi _{gi}] \u0026= k\\theta = \\mu _{gi} \\\\ \\text{var}[N_i \\pi _{gi}] \u0026= k\\theta^2 = \\mu _{gi}^2 \\phi _g \\\\ y_{gi} \u0026\\sim \\text{Poisson}(N_i \\pi _{gi}) \\\\ \u0026= \\int _0^\\infty \\text{Poisson}(N_i \\pi _{gi})~ \\text{Gamma}(\\phi _g^{-1}, \\mu _{gi} \\phi _g)~ \\mathrm d N_i \\pi _{gi} \\\\ \u0026= \\text{NB}(\\phi _g^{-1}, \\frac {\\mu _{gi} \\phi _g} {1 + \\mu _{gi} \\phi _g}) \\\\ \\mathrm E[y_{gi}] \u0026= \\mu _{gi} \\\\ \\text{var}[y_{gi}] \u0026= \\mathrm E \\left[\\text{var}[y_{gi} \\mid \\pi _{gi}]_i \\right] _\\pi + \\text{var} \\left[\\mathrm E[y_{gi}\\mid \\pi _{gi}]_i \\right] _\\pi \\\\ \u0026= \\mathrm E[N _i \\pi _{gi}] _\\pi + \\text{var}[N _i \\pi _{gi}] _\\pi \\\\ \u0026= \\mu _{gi} + \\mu _{gi}^2 \\phi _g \\\\ \\text{CV}^2[y_{gi}] \u0026= 1 / \\mu _{gi} + \\phi _g \\\\ \u0026= \\text{CV}^2[y_{gi} \\mid \\pi _{gi}] + \\text{CV}^2[\\pi _{gi}] \\\\ \u0026= \\text{Technical~CV}^2 + \\text{Biological~CV}^2 \\\\ \\end{aligned}\\] dispersion $\\phi _g$ 普通は $D = \\sigma^2 / \\mu$ と定義されるけど、 ここでは $\\text{CV}^2$ BCV $\\sqrt{\\phi _g}$ biological coefficient of variation. こっちは普通と同じように $\\text{CV} = \\sigma / \\mu$ 。 sequencing depthがどんなに大きくても残るばらつき。 DGE digital gene expression CPM counts per million logFC log2-fold-change ",
  "href": "/rstats/edger.html",
  "tags": [
   "r",
   "bioconductor"
  ],
  "title": "edgeR",
  "type": "rstats"
 },
 {
  "content": "単語 buffer ファイル編集領域 minibuffer 画面の下端でコマンドを受け付けるところ kill-ring paste board window tmuxでいうpane frame tmuxでいうwindow Keybind C- は左手小指の control\nM- は esc または C-[\nkey command action C-g keyboard-quit とにかくキャンセル C-z suspend-emacs とりあえずemacsを抜ける C-x u advertised-undo 元に戻す C-_ advertised-undo 元に戻す C-/ advertised-undo 元に戻す M-h help ヘルプ M-x ミニバッファをコマンド受付状態にする M-! ミニバッファをシェルコマンド受付状態にする C-x C-f find-file 開く C-x C-s save-file 上書き保存 C-x C-w write-file 別名で保存 C-x C-c save-buffers-kill-emacs 終了 C-x d dired directory edit に突入 (下記) C-s isearch-forward 下に検索 C-r isearch-backward 上に検索 M-% 置換 C-@ set-mark-command 開始位置をマーク C-SPC set-mark-command (IME切り替え) C-w kill-region マークから現在地までカット M-w copy-region-as-kill マークから現在地までコピー C-y yank 貼り付け C-x r k kill-rectangle 矩形にカット C-x r y yank-rectangle 矩形にペースト C-x r o open-rectangle 矩形にスペース C-q TAB タブコード \\t 入力 C-x TAB indent-rigidly 選択された領域を左右キーで手動インデント C-x k kill-buffer バッファを消す = ファイルを閉じる C-x b switch-to-buffer バッファを切り替える C-x C-b list-buffers バッファリストを開く(下記) C-x 0 このwindowを閉じて分割解除 C-x 1 分割解除してこのwindowを最大化 C-x 2 上下分割 C-x 3 左右分割 C-x o other (next) windowにフォーカスを移す C-x 5 0 delete-frame C-x 5 1 delete-other-frame C-x 5 2 make-frame-command C-x 5 o other-frame C-x 5 b switch-tobuffer-other-frame C-x 5 f find-file-other-frame C-x 5 d dired-other-frame list-buffers C-x C-b バッファリストを隣のウィンドウで開く。\nC-u を頭につけるとファイルを開いてるbufferに限定して。\nバッファメニューをそのペインで開くコマンドはずばり buffer-menu\n? ヘルプ\no 隣のwindowで開く\nf そのwindowで開く (RET)\nk 閉じる候補にマーク\ns 保存候補にマーク\nx マーク内容の実行\nu マーク取り消し\nq list終了\ndired C-x d ファイラのようなバッファ。 コピーや移動もできるけど、まあそれはシェルからやればいい。\no 別バッファで開く\nf そのバッファで開く return\nv 見てみる\nq 終了\n繰り返し入力 C-u N X あるいは M-N X で、XをN回入力する。\ne.g., C-u 79 - と打てば水平線を入力できる。\ne.g., C-u 3 C-_ とすれば3操作分だけ元に戻せる。\nモード C-c モード特有コマンドのprefix\nM-; モードに従ってコメント記号の自動挿入\nM-/ コード補完\nlist-faces-display\ncustomize-face\nMarkdown C-c C-c p preview on browser\nC-c C-c m preview on buffer\nC-c C-c v write preview and open in browser\nC-c C-c e write preview\nR (ESS) M-x R R起動\nC-c C-r ess-eval-region\nC-c C-q R終了\n設定 ~/.emacs.d/init.el\nhttps://github.com/heavywatal/dotfiles/blob/master/.emacs.d/init.el\npackage.el でパッケージ管理 Emacs 24で標準入りしたので、基本的にこれを使うのが良さそう。 まずinit.elの先頭でこのように宣言\n(require \u0026#39;package) (add-to-list \u0026#39;package-archives \u0026#39;(\u0026#34;melpa\u0026#34; . \u0026#34;https://melpa.org/packages/\u0026#34;)) ;; Added by Package.el. This must come before configurations of ;; installed packages. Don\u0026#39;t delete this line. If you don\u0026#39;t want it, ;; just comment it out by adding a semicolon to the start of the line. ;; You may delete these explanatory comments. (package-initialize) あとは使いたいパッケージを以下のように列挙。\n(package-install \u0026#39;flycheck) (package-install \u0026#39;auto-complete) (package-install \u0026#39;markdown-mode) initialize後に (package-refresh-contents) を入れておくとパッケージ情報を最新に保てるが、 結構遅いので気が向いたときに手動でやったほうがいい: M-x package-list-packages U x\n",
  "href": "/dev/emacs.html",
  "tags": [
   "editor"
  ],
  "title": "Emacs",
  "type": "dev"
 },
 {
  "content": " 使えるフリーフォント Noto Fonts \u0026ldquo;No tofu\u0026rdquo; を目指してGoogleが開発している多言語対応フォント。 Noto Serif はディスプレイ上での視認性や和文フォントとのサイズバランスも良くて便利。 Noto Sans/Serif CJK はAdobeと共同開発の和文フォントで、ほぼ同じものを違う名前で公開している。 英数字がNoto SansではなくSource Sans Proなのはなぜ？ Source Sans Pro 小文字の l が曲がってて良いけど全体的に小さくて狭苦しい。 ファミリー名がバージョン入りの \u0026ldquo;Source Sans 3\u0026rdquo; になって使いにくくなった。 Googleと共同開発の Source Han Sans (源ノ角ゴシック) と Source Han Serif (源ノ明朝) はコンセプトも内容も素晴らしい日本語フォント。ウェイトもたくさんある。 ダウンロードの選択肢がありすぎて迷うが、 \u0026ldquo;Region-specific Subset\u0026rdquo; のSourceHanSansJPが軽くて良さそう。 Source Han Mono は普通の固定幅フォントよりも幅広で、 和文と欧文の幅が3:2になるよう調整されている。 俺は2:1のUbuntu Monoのほうが好き。 Ubuntu スクリーン上での視認性を重視してデザインされたフォントで、 個性的でありながら見やすい。 エルの小文字 l が曲がってて良い。 Ubuntu Mono は特にプログラミング用のフォントとして最適。 普通の欧文 monospace よりもひとまわり小さく、 全角和文フォントのちょうど半分になるのもポイント。 Open Sans すっきりゆったりニュートラルで見やすい。 ヒラギノとの相性も抜群。 ファイルサイズも小さいのでウェブフォントとして使いやすい。 TeX Gyre Pagella 個人的に一番好きなセリフフォント Palatino はMacでもWinでも利用可能だがフリーではない。 デザイナー本人によるフリーな合法クローンの URW Palladio L やそれをTeXコミュニティで拡張した TeX Gyre Pagella ならLinuxでもどこでも使える。 最新改良版 Palatino nova は文句なしに美しいけど有償。 Roboto Helvetica似、かっちりコンパクト、いかにもシステム向けなフォント。 でもちょっと c とかの切れ目が小さすぎたりして、視認性はイマイチ。 あまり使わなくなったやつ DejaVu Bitstream Vera の後を継いで多言語対応させた、Linuxの標準的なフォント。 DejaVu Sans は Verdana のように幅が広く、スクリーン上で読みやすい。 DejaVu Sans Mono はプログラミングに使いやすい。 Linux Libertine 印刷に耐えうるクオリティを目指して開発されたカッコいいセリフ体。 兄弟の Linux Biolinum は Optima 似の非均等サンセリフ。 いろいろ残念な側面もあってLaTeXの外では使いにくい: (1) ひとまわり小さいので和文フォントとバランスをとるのが難しい。 (2) OpenTypeだと名前の末尾にOがつく。 (3) TrueTypeのItalicがバグってる？開発も停滞。 Libertinus という名前のフォークがメンテナンスされている。 Liberation Fedora が Arial, Times New Roman, Courier New からの解放を目指して作ってる。 英数字はいいけど、グリフ数は少ない。 でも Liberation Mono は Courier New というより Courier な太さ? GNU FreeFont Helvetica, Times, Courier の置き換えを狙ったフォント。 でも FreeMono は Courier というより Courier New な細さ? IPA Source Han の登場により役目を終えた感。 Takao はUbuntuコミュニティが保守している派生フォント。 MigMix M+フォントに足りない漢字をIPAゴシックから補完した合成フォント。 メイリオと同じで視認性は高いけどちょっと丸すぎる印象。 Sample serif serif !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ ui-serif !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ Times !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ Times New Roman !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ Georgia !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ Palatino !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ TeX Gyre Pagella !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ Noto Serif !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ Source Serif 4 !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ Source Serif Pro !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ Merriweather !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ Libertinus Serif !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ sans-serif sans-serif !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ ui-sans-serif !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ system-ui !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ -apple-system !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ BlinkMacSystemFont !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ Helvetica Neue !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ Helvetica !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ Arial !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ Roboto !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ Verdana !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ DejaVu Sans !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ Ubuntu !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ Open Sans !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ Noto Sans !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ Source Sans 3 !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ Source Sans Pro !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ Libertinus Sans !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ Optima !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~ monospace monospace !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n噌祇逢餅鯖鰯わたるメタル ui-monospace !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n噌祇逢餅鯖鰯わたるメタル Courier !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n噌祇逢餅鯖鰯わたるメタル Courier New !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n噌祇逢餅鯖鰯わたるメタル Monaco !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n噌祇逢餅鯖鰯わたるメタル Menlo !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n噌祇逢餅鯖鰯わたるメタル Ubuntu Mono !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n噌祇逢餅鯖鰯わたるメタル Source Code Pro !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n噌祇逢餅鯖鰯わたるメタル Source Han Mono !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n噌祇逢餅鯖鰯わたるメタル DejaVu Sans Mono !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n噌祇逢餅鯖鰯わたるメタル Osaka-Mono !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n噌祇逢餅鯖鰯わたるメタル ゴシック体 Hiragino Sans !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n！＂＃＄％＆＇（）＊＋，－．／０１２３４５６７８９：；＜＝＞？＠\n噌祇逢餅鯖鰯わたるメタル Hiragino Kaku Gothic ProN !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n！＂＃＄％＆＇（）＊＋，－．／０１２３４５６７８９：；＜＝＞？＠\n噌祇逢餅鯖鰯わたるメタル Hiragino Kaku Gothic Pro !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n！＂＃＄％＆＇（）＊＋，－．／０１２３４５６７８９：；＜＝＞？＠\n噌祇逢餅鯖鰯わたるメタル YuGothic !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n！＂＃＄％＆＇（）＊＋，－．／０１２３４５６７８９：；＜＝＞？＠\n噌祇逢餅鯖鰯わたるメタル Toppan Bunkyu Gothic !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n！＂＃＄％＆＇（）＊＋，－．／０１２３４５６７８９：；＜＝＞？＠\n噌祇逢餅鯖鰯わたるメタル Toppan Bunkyu Midashi Gothic !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n！＂＃＄％＆＇（）＊＋，－．／０１２３４５６７８９：；＜＝＞？＠\n噌祇逢餅鯖鰯わたるメタル Tsukushi A Round Gothic !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n！＂＃＄％＆＇（）＊＋，－．／０１２３４５６７８９：；＜＝＞？＠\n噌祇逢餅鯖鰯わたるメタル Tsukushi B Round Gothic !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n！＂＃＄％＆＇（）＊＋，－．／０１２３４５６７８９：；＜＝＞？＠\n噌祇逢餅鯖鰯わたるメタル Osaka-Mono !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n！＂＃＄％＆＇（）＊＋，－．／０１２３４５６７８９：；＜＝＞？＠\n噌祇逢餅鯖鰯わたるメタル Source Han Sans !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n！＂＃＄％＆＇（）＊＋，－．／０１２３４５６７８９：；＜＝＞？＠\n噌祇逢餅鯖鰯わたるメタル MS PGothic !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n！＂＃＄％＆＇（）＊＋，－．／０１２３４５６７８９：；＜＝＞？＠\n噌祇逢餅鯖鰯わたるメタル 明朝体 Hiragino Mincho ProN !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n！＂＃＄％＆＇（）＊＋，－．／０１２３４５６７８９：；＜＝＞？＠\n噌祇逢餅鯖鰯わたるメタル Hiragino Mincho Pro !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n！＂＃＄％＆＇（）＊＋，－．／０１２３４５６７８９：；＜＝＞？＠\n噌祇逢餅鯖鰯わたるメタル YuMincho !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n！＂＃＄％＆＇（）＊＋，－．／０１２３４５６７８９：；＜＝＞？＠\n噌祇逢餅鯖鰯わたるメタル YuKyokasho !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n！＂＃＄％＆＇（）＊＋，－．／０１２３４５６７８９：；＜＝＞？＠\n噌祇逢餅鯖鰯わたるメタル Toppan Bunkyu Mincho !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n！＂＃＄％＆＇（）＊＋，－．／０１２３４５６７８９：；＜＝＞？＠\n噌祇逢餅鯖鰯わたるメタル Toppan Bunkyu Midashi Mincho !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n！＂＃＄％＆＇（）＊＋，－．／０１２３４５６７８９：；＜＝＞？＠\n噌祇逢餅鯖鰯わたるメタル Source Han Serif !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n！＂＃＄％＆＇（）＊＋，－．／０１２３４５６７８９：；＜＝＞？＠\n噌祇逢餅鯖鰯わたるメタル MS PMincho !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n！＂＃＄％＆＇（）＊＋，－．／０１２３４５６７８９：；＜＝＞？＠\n噌祇逢餅鯖鰯わたるメタル 絵文字 Apple Color Emoji !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n🍺🍻🤘🪙 Noto Color Emoji !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n🍺🍻🤘🪙 Segoe UI Emoji !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n🍺🍻🤘🪙 Segoe UI Symbol !\"#$%\u0026'()*+,-./0123456789:;\u003c=\u003e?@\nABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`\nabcdefghijklmnopqrstuvwxyz{|}~\n🍺🍻🤘🪙 ",
  "href": "/misc/fonts.html",
  "tags": [
   "writing"
  ],
  "title": "Fonts",
  "type": "misc"
 },
 {
  "content": "foreach カウンター無しでループを書けるようにするパッケージ。 普段は purrr::map() とかのほうが使いやすいけど、 後述の並列化の場面ではお世話になる。\nlibrary(foreach) slow_square = function(x) { Sys.sleep(0.5) x * x } foreach(x = seq(1, 4), .combine = c) %do% { slow_square(x) } # 2.0 sec with 1 core https://CRAN.R-project.org/package=foreach\nforeach() .combine (list) 型が既知でvectorが欲しい場合に c にするなど .multicombine (FALSE) 結果が出る度に二値関数で結合していくか、まとめてか。 .combine=c や cbind を指定すると暗黙TRUE。 .maxcombine (100) まとめる場合の最大個数 .export (NULL) 並列化する場合、処理ブロック内に持ち込みたいオブジェクト .packages (NULL) 並列化する場合、名前空間省略で使いたいパッケージ .inorder (TRUE) 並列化する場合、順序を保持したいか .init, .final, .noexport, .verbose\nparallel https://cran.r-project.org/web/views/HighPerformanceComputing.html https://stat.ethz.ch/R-manual/R-patched/library/parallel/html/00Index.html Rの並列化では snow や multicore が使われてきたが、 バージョン2.14からそれらを統合した parallel が標準ライブラリに入った。\nlibrary(parallel) options(mc.cores = detectCores(logical = FALSE)) # 4 mclapply(seq(1, 4), slow_square) # 0.5 sec with 4 cores lapply(seq(1, 4), slow_square) # 2.0 sec with 1 cores purrr::map(seq(1, 4), slow_square) # 2.0 sec with 1 cores mclapply() は lapply() のお手軽並列化バージョン。 UNIX系OSのforkに依存するためWindows不可。\nmclapply(X, FUN, ..., mc.preschedule = TRUE, mc.set.seed = TRUE, mc.silent = FALSE, mc.cores = getOption(\u0026#34;mc.cores\u0026#34;, 2L), mc.cleanup = TRUE, mc.allow.recursive = TRUE, affinity.list = NULL) purrr::map()のように無名関数を渡せる ラッパー関数 mcmap() を書いてみた。\nそのへんをもっとしっかりやった future, future.apply, furrr を使っていくのが良さそう。\nもっと細かくいろいろ制御したい場合は foreach (とその橋渡しライブラリdoParallel) を介して使う。 この場合、クラスタの生成や破棄なども自前でやることになる。\nlibrary(doParallel) cluster = makeCluster(getOption(\u0026#34;mc.cores\u0026#34;, 2L), type = \u0026#34;FORK\u0026#34;) registerDoParallel(cluster) foreach(x = seq(1, 4)) %dopar% { slow_square(x) } stopCluster(cluster) makeCluster() spec いくつのworkerを立ち上げるか。 物理コア数を取得するには parallel::detectCores(logical = FALSE) type = \u0026quot;PSOCK\u0026quot; デフォルト。高コストだけどだいたいどの環境でも使える。 マルチCPUのサーバーで並列化したい場合はこれ。 foreach() で使う場合 .export= や .packages= の指定が重要。 type = \u0026quot;FORK\u0026quot; 4コア1CPUとかの普通のデスクトップマシンで気楽に並列化したいならこれ。 低コストだし .export= や .packages= を指定せず foreach() できる。 Windowsでは使えないらしいけど。 outfile = \u0026quot;\u0026quot; print()やmessage()などの出力先を標準に戻す。 デフォルトでは/dev/nullに捨てられてしまう。 iterators https://CRAN.R-project.org/package=iterators\n大抵はメモリを一気に確保してしまう方が速いが、 データがRAMを超えるほど大きいときはそうも言ってられない。 最大要求メモリを減らしたり、 並列foreachのノード間通信を減らすためにはiteratorsを利用する。\nnextElem(it, ...) イテレータを進めて値を得る。 デバッグ時にとりあえず全部見たいときは as.list(it) が便利。 icount(n) イテレータ版 seq_len() icountn(vn) 自然数限定イテレータ版 expand.grid() iter(obj, by = c(\u0026quot;column\u0026quot;, \u0026quot;row\u0026quot;)) イテレータ版 purrrlyr::by_row() のようなもので、 並列foreachで各ノードに巨大data.frameを送りたくない場合に有用。 data.frame以外も適用可。 e.g., iter(diamonds, by = \u0026quot;row\u0026quot;) isplit(x, f, drop = FALSE, ...) イテレータ版 purrrlyr::slice_rows() のようなもので、 fは列名じゃなくてfactor。 data.frame以外も適用可。 e.g., isplit(diamonds, diamonds$cut) iread.table(file, ..., verbose = FALSE), ireadLines(con, n = 1, ...) ファイルを1行ずつ読み込む irbinom(..., count), irnbinom(), irnorm(), irpois(), irunif() 乱数 idiv(n, ..., chunks, chunkSize) 整数nをいい感じに振り分ける。 関連書籍 ",
  "href": "/rstats/foreach.html",
  "tags": [
   "r",
   "concurrent"
  ],
  "title": "foreach/parallel",
  "type": "rstats"
 },
 {
  "content": "https://gcc.gnu.org/\nUsage 直接コマンドを実行するなら:\ng++ -g -Wall -Wextra -O3 main.cpp -o a.out make や CMake などのツールを使ってビルドすることが多く、 ターミナルからコンパイラを直接実行することはほぼ無い。\nOptions https://gcc.gnu.org/onlinedocs/gcc/Invoking-GCC.html https://gcc.gnu.org/onlinedocs/gcc/Option-Summary.html 出力オプション https://gcc.gnu.org/onlinedocs/gcc/Overall-Options.html\n-c コンパイルするだけでリンクしない -o {file} 出力先のファイル名を指定 C/C++オプション https://gcc.gnu.org/onlinedocs/gcc/C-Dialect-Options.html\n-std=c++11 2011年のISO標準でコンパイルする。 -std=c++14 2014年のISO標準でコンパイルする。 g++-6 ではこれのGNU方言である gnu++14 がデフォルトになった。 警告オプション https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html\n-Wall 基本的な警告 -Wextra Wallより厳しい警告 -Werror 警告をエラー扱いにする デバッグオプション https://gcc.gnu.org/onlinedocs/gcc/Debugging-Options.html\n-g gdb で使えるデバッグ情報を埋め込む 最適化オプション https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html\n-O1 軽い最適化 -O2 プログラムサイズを大きくしない最適化をすべて実行 -O3 インライン展開なども行う -Os コードサイズが最小になるように プリプロセッサオプション https://gcc.gnu.org/onlinedocs/gcc/Preprocessor-Options.html\n-D {name}, -D {name=definition} ソースコードで #define するのと同じようにマクロを定義する -I {dir}, -isystem {dir} #include \u0026lt;***\u0026gt; のインクルードパスの先頭に {dir} を追加。 -iquote {dir} #include \u0026quot;***\u0026quot; のインクルードパスの先頭に {dir} を追加。 -M, -MM ファイルの依存関係を書き出す。 前者はシステムヘッダーを含み、後者は含まない。 リンクオプション https://gcc.gnu.org/onlinedocs/gcc/Link-Options.html\n-l {library} リンクするライブラリを指定する。 ファイル名が libsfmt.a のときは -lsfmt -L {dir} ライブラリのサーチパスを追加する Installation https://gcc.gnu.org/install/\nMac Macにある /usr/bin/gcc は gcc の顔をした clang なので、 本物の gcc が欲しいときは別途インストールが必要。 Homebrew で入れるのが楽チン:\nbrew install gcc Ubuntu 元から入ってるけど最新版をソースからインストールしたい。\ngmp, mpfr, mpc をインストール:\nsudo apt-get install libgmp-dev libmpfr-dev libmpc-dev --with-system-zlib でビルドするには zlib が必要:\nsudo apt-get install zlib1g-dev システムに既に入ってるやつの configure オプションを gcc -v 見るでみて、それを参考に configure\n--enable-languages は使う言語だけに --prefix は /usr/local 既存のバージョンとごっちゃになるのを避けるため --program-suffix を設定 他言語サポートは不要なので --disable-nls --disable-multilib を付けないと fatal error: gnu/stubs-32.h: No such file or directory. stub-32.h などと怒られて止まる get -O- http://ftp.gnu.org/gnu/gcc/gcc-4.9.1/gcc-4.9.1.tar.bz2 | tar xj kdir build d build/ ./gcc-4.9.1/configure --enable-languages=c,c++,go,fortran --prefix=/usr/local --program-suffix=-4.9 --enable-shared --enable-linker-build-id --without-included-gettext --enable-threads=posix --disable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --enable-gnu-unique-object --disable-libmudflap --enable-plugin --with-system-zlib --disable-browser-plugin --enable-gtk-cairo --with-arch-directory=amd64 --enable-multiarch --disable-werror --with-abi=m64 --disable-multilib --with-tune=core2 --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu コンパイル。ただしデフォルトではコンパイラオプションが -g O2 となっていて膨大なデバッグシンボルがついた状態で出来上がってしまうらしいので、 オプションを上書きしてそれを回避する:\nmake BOOT_CFLAGS=\u0026quot;-O2\u0026quot; CFLAGS=\u0026quot;-O2\u0026quot; CXXFLAGS=\u0026quot;-O2\u0026quot; bootstrap fatal error: bits/predefs.h: No such file or directory って怒られたら:\nsudo apt-get install libc6-dev-i386 /usr/bin/ld: cannot find crti.o: No such file or directory って怒られたら:\nsudo ln -s /usr/lib/x86_64-linux-gnu /usr/lib64 sudo make install でインストール。の前に make DESTDIR=${HOME}/tmp/gcc-test install などとしてテストするといいらしい。 ディスクレスクラスタの場合、子ノードにもインストール: sudo make DESTDIR=/nfsroot install\n",
  "href": "/cxx/gcc.html",
  "tags": [
   "c++"
  ],
  "title": "gcc",
  "type": "cxx"
 },
 {
  "content": "生物種や分野によらない共通の語彙で遺伝子産物の機能を記述するための用語体系。\n機能に応じて遺伝子にたくさんのタグ(GO term)を付けましょうってこと。\ne.g. Human RB1 http://amigo.geneontology.org/amigo/gene_product/UniProtKB:P06400\nGO term http://geneontology.org/page/ontology-structure\ne.g. http://amigo.geneontology.org/amigo/term/GO:0043065#display-graphics-tab\n小さくて専門的な下位termから、大きくて一般的な上位termに向かう directed acyclic graph (DAG) を構成している。 ある下位termを持つ場合はそれに連なる上位termも全て持つ (True Path Rule)。 かなり細かい多層構造。 分岐したあと上位でまた合流するので、木構造での表示には限界がある。\n最上位のtermは3つ:\nBiological Process 生物学的な機能。 下位には例えば、分化、細胞分裂、細胞死など。 GO:0008150 Cellular Component 細胞内での局在。 下位には例えば、核内、小胞体など。 GO:0005575 Molecular Function 化学的な機能。 下位には例えば、加水分解酵素、DNA結合など。 GO:0003674 ほかのアノテーションと同じようにEvidenceのレベルもいろいろある。\nGO relation http://geneontology.org/page/ontology-relations\nDAGのエッジのことをrelationと呼ぶ。\nis a part of regulates: positively or negatively フィードフォワード的なショートカットも推定される。。\nGO解析 何らかの解析で遺伝子のサブセットが抽出されたとして、 それらが持つGO termの組成を全ゲノムでの組成と比較してみたときに、 特定の機能群におけるenrichmentが見られるかどうか？\ne.g. 対照区と処理区で発現が変化した遺伝子には、XXXというtermが多く含まれる\n超幾何分布\nツール Amigo http://amigo.geneontology.org\nBioConductor GO.db GO term/relationの構造データ。遺伝子のデータではない。 GOstats enrichment解析。あまり人気がないっぽい。 topGO enrichment解析。複数のアルゴリズムと検定をサポート。 GOexpress 発現パターンで遺伝子をクラスタリングしてGO解析 GOSemSim 2つの遺伝子群の機能的類似度をGOベースで goseq RNA-seqからGOまで直行？ length bias (長いtranscriptほどDEG検出されやすい) を考慮。 ",
  "href": "/bio/gene_ontology.html",
  "tags": [
   "genetics",
   "database"
  ],
  "title": "Gene Ontology",
  "type": "bio"
 },
 {
  "content": "https://bioconductor.org/packages/GenomicRanges\nゲノム上の座標・範囲を表現するための型として S4 class GenomicRanges を提供する。 これは整数の範囲を扱う IRanges を拡張したもの。\nInstallation install.packages(\u0026#34;BiocManager\u0026#34;) BiocManager::install(\u0026#34;GenomicRanges\u0026#34;) library(GenomicRanges) IRanges 両端の値を含む閉区間 [start, end] なのが厄介。 end = start + width - 1 の関係。 width = 0 のときだけ例外的に end \u0026lt; start になり、両端を含まない。\nir1 = IRanges(start = 1:4, width = 3:0) ir2 = IRanges(start = 1:4, end = 3) ir3 = IRanges(end = 3, width = 3:0) stopifnot(identical(ir1, ir2), identical(ir1, ir3)) ir1 IRanges object with 4 ranges and 0 metadata columns: start end width [1] 1 3 3 [2] 2 3 2 [3] 3 3 1 [4] 4 3 0 start(ir1) [1] 1 2 3 4 end(ir1) [1] 3 3 3 3 width(ir1) [1] 3 2 1 0 GenomicRanges gr = GRanges( seqnames = c(\u0026#34;chr2\u0026#34;, \u0026#34;chr1\u0026#34;, \u0026#34;chr1\u0026#34;), # 染色体の名前 ranges = IRanges(101:103, width = 100), # 座標 strand = c(\u0026#34;-\u0026#34;, \u0026#34;+\u0026#34;, \u0026#34;*\u0026#34;), score = 51:53, # 任意のelementMetadata列 GC = 0.1 * 5:7, # 任意のelementMetadata列 seqinfo = NULL, seqlengths = c(chr1 = 249250621, chr2 = 243199373) ) gr GRanges object with 3 ranges and 2 metadata columns: seqnames ranges strand | score GC \u0026lt;Rle\u0026gt; \u0026lt;IRanges\u0026gt; \u0026lt;Rle\u0026gt; | \u0026lt;integer\u0026gt; \u0026lt;numeric\u0026gt; [1] chr2 101-200 - | 51 0.5 [2] chr1 102-201 + | 52 0.6 [3] chr1 103-202 * | 53 0.7 ------- seqinfo: 2 sequences from an unspecified genome 上記のように自分で作る機会はほぼ無くて、 GenomicRanges::makeGRangesFromDataFrame(df), rtraclayer::import(\u0026quot;annotation.gff3\u0026quot;), GenomicFeatures::genes(txdb), のような形で読み込むことが多い。\n個々の区間の情報へのアクセスはIRangeと同じ start(gr), end(gr), width(gr) に加えて:\nseqnames(gr) factor-Rle of length 3 with 2 runs Lengths: 1 2 Values : chr2 chr1 Levels(2): chr1 chr2 ranges(gr) IRanges object with 3 ranges and 0 metadata columns: start end width [1] 101 200 100 [2] 102 201 100 [3] 103 202 100 strand(gr) factor-Rle of length 3 with 3 runs Lengths: 1 1 1 Values : - + * Levels(3): + - * mcols(gr) DataFrame with 3 rows and 2 columns score GC \u0026lt;integer\u0026gt; \u0026lt;numeric\u0026gt; 1 51 0.5 2 52 0.6 3 53 0.7 S4Vectors::mcols() で参照・代入するのは区間ごとのメタデータ。\nデータセット全体のメタデータとして染色体の長さなども扱う:\nseqinfo(gr) Seqinfo object with 2 sequences from an unspecified genome: seqnames seqlengths isCircular genome chr1 249250621 NA \u0026lt;NA\u0026gt; chr2 243199373 NA \u0026lt;NA\u0026gt; seqlevels(gr) [1] \u0026#34;chr1\u0026#34; \u0026#34;chr2\u0026#34; seqlengths(gr) chr1 chr2 249250621 243199373 isCircular(gr) chr1 chr2 NA NA genome(gr) chr1 chr2 NA NA Functions 多くの関数は IRanges と GenomicRanges で同様に動作する。 より単純な前者で例を示し、後者固有の話は適宜挟む。\nir = IRanges(c(1, 8, 14, 15, 19, 34, 40), width = c(12, 6, 6, 15, 6, 2, 7)) ir IRanges object with 7 ranges and 0 metadata columns: start end width [1] 1 12 12 [2] 8 13 6 [3] 14 19 6 [4] 15 29 15 [5] 19 24 6 [6] 34 35 2 [7] 40 46 7 Intra-range methods 個々の区間を操作\nshift(x, shift = 0L, use.names = TRUE) ずらす。 narrow(x, start = NA, end = NA, width = NA, use.names = TRUE) 狭める。startには正、endには負の値を与える。 start = 1, end = -1 のとき何もしないというのがわかりにくすぎて怖い。 identical(ir, narrow(ir, 1, -1)) [1] TRUE threebands(x, start = NA, end = NA, width = NA) は削られる両端部分も含めて $left, $middle, $right のリストで返してくれる亜種。 逆に広げるには flank() を punion() するか、 + 演算子で両側に広げてから片側を narrow() するか？ resize(x, width, fix = \u0026quot;start\u0026quot;, use.names = TRUE, ...) 幅を変える。 fix: start, end, center flank(x, width, start = TRUE, both = FALSE, use.names = TRUE, ...) start上流もしくはend下流の領域。両方いっぺんには取れない。 both = TRUE は start (or end) を起点に両側という意味であり、範囲の両側ではない。 flank(ir, 1, both = TRUE) IRanges object with 7 ranges and 0 metadata columns: start end width [1] 0 1 2 [2] 7 8 2 [3] 13 14 2 [4] 14 15 2 [5] 18 19 2 [6] 33 34 2 [7] 39 40 2 promoters(x, upstream=2000, downstream=200, use.names=TRUE, ...) はstart起点に上下異なる幅で取ってこられる亜種。 reflect(x, bounds, use.names = TRUE) bounds の裏から見た相対位置にする。 reflect(ir, IRanges(1, 1000)) IRanges object with 7 ranges and 0 metadata columns: start end width [1] 989 1000 12 [2] 988 993 6 [3] 982 987 6 [4] 972 986 15 [5] 977 982 6 [6] 966 967 2 [7] 955 961 7 いかにも負のstrandの座標処理に使えそうだがなぜかGenomicRangesには未対応。 自分で書くならこんな感じか: y = gr bounds = IRanges(start = 1, width = seqlengths(gr)[as.vector(seqnames(gr))]) ranges(y)[strand(gr) == \u0026#34;-\u0026#34;] = reflect(ranges(gr), bounds)[strand(gr) == \u0026#34;-\u0026#34;] y GRanges object with 3 ranges and 2 metadata columns: seqnames ranges strand | score GC \u0026lt;Rle\u0026gt; \u0026lt;IRanges\u0026gt; \u0026lt;Rle\u0026gt; | \u0026lt;integer\u0026gt; \u0026lt;numeric\u0026gt; [1] chr2 243199174-243199273 - | 51 0.5 [2] chr1 102-201 + | 52 0.6 [3] chr1 103-202 * | 53 0.7 ------- seqinfo: 2 sequences from an unspecified genome reverse(x) は reflect(x, range(x)) のショートカット。 区間を逆順に並べる rev() とは違う。 restrict(x, start = NA, end = NA, keep.all.ranges = FALSE, use.names = TRUE) start から end までの範囲のみ残して外を捨てる。境界含む。 end = 14 で15から始まる区間が取れてきちゃうのはバグじゃない？ restrict(ir, 10, 14) IRanges object with 4 ranges and 0 metadata columns: start end width [1] 10 12 3 [2] 10 13 4 [3] 14 14 1 [4] 15 14 0 足し算・引き算は両側に伸縮:\nIRanges(101:200) + 100 IRanges object with 1 range and 0 metadata columns: start end width [1] 1 300 300 掛け算はズームイン・ズームアウト:\nIRanges(101:200) * 2 IRanges object with 1 range and 0 metadata columns: start end width [1] 126 175 50 IRanges(101:200) * 0.5 IRanges object with 1 range and 0 metadata columns: start end width [1] 51 250 200 Inter-range methods 区間の集合を操作\nrange(x, ..., with.revmap = FALSE, na.rm = FALSE) 端から端まで1つの区間として返す。 range(ir) IRanges object with 1 range and 0 metadata columns: start end width [1] 1 46 46 reduce(x, drop.empty.ranges = FALSE, min.gapwidth = 1L, with.revmap = FALSE) 重なっている区間をつなげて平らにする。 with.revmap = TRUE とすると入力した区間がどこに含まれるかをmcolsに保持する。 reduce(ir, with.revmap = TRUE) IRanges object with 3 ranges and 1 metadata column: start end width | revmap [1] 1 29 29 | 1,2,3,... [2] 34 35 2 | 6 [3] 40 46 7 | 7 min.gapwidth を大きくすると離れた区間もつなげられる。 例えば reduce(ir, min.gapwidth = 10L) で全部つながって range(ir) と同じになる。 gaps(x, start=NA, end=NA, ...) IRanges::setdiff(IRanges(start, end), x) と同等。 start/end 省略時は range(x) からの差分。 gaps(ir) IRanges object with 2 ranges and 0 metadata columns: start end width [1] 30 33 4 [2] 36 39 4 GRangesに対しては染色体全体からの差分がstrandごとに計算される: gaps(gr) GRanges object with 9 ranges and 0 metadata columns: seqnames ranges strand \u0026lt;Rle\u0026gt; \u0026lt;IRanges\u0026gt; \u0026lt;Rle\u0026gt; [1] chr1 1-101 + [2] chr1 202-249250621 + [3] chr1 1-249250621 - [4] chr1 1-102 * [5] chr1 203-249250621 * [6] chr2 1-243199373 + [7] chr2 1-100 - [8] chr2 201-243199373 - [9] chr2 1-243199373 * ------- seqinfo: 2 sequences from an unspecified genome disjoin(x, ...) 全ての始点・終点を使い、重なり無しで最多の小区間にして返す。 disjoin(ir) IRanges object with 10 ranges and 0 metadata columns: start end width [1] 1 7 7 [2] 8 12 5 [3] 13 13 1 [4] 14 14 1 [5] 15 18 4 [6] 19 19 1 [7] 20 24 5 [8] 25 29 5 [9] 34 35 2 [10] 40 46 7 disjointBins(x, ...) 重ならないように描画するときのy座標に使える。 disjointBins(ir) [1] 1 2 1 2 3 1 1 union(x, y), intersect(x, y), setdiff(x, y) 結果は reduce() 済みの区間。 element-wise の punion(), pintersect(), psetdiff(), pgap() もある。 GenomicRanges::subtract(x, y, minoverlap = 1L, ...) setdiff() と似ているが元の区間ごとの GRangesList にして返す。 Overlaps type any: ちょっとでも重なっているか距離が maxgap 以下ならOK。 start/end/equal: 開始/終了/両方の距離が maxgap 以下ならOK。 within: queryがすっぽり含まれていればOK。 maxgap の意図・挙動は不明。 findOverlaps(query, subject, maxgap = -1L, minoverlap = 0L, type, select, ...) select: subject側に複数マッチした場合に何を返すか。 デフォルトの \u0026ldquo;all\u0026rdquo; ならHits型オブジェクト、 \u0026ldquo;first\u0026rdquo; や \u0026ldquo;last\u0026rdquo; なら整数vectorでインデックスを返す。 \u0026ldquo;arbitrary\u0026rdquo; の挙動は謎だが乱数を振ったりするわけではなさそう。 findOverlaps(ir, ir) Hits object with 15 hits and 0 metadata columns: queryHits subjectHits \u0026lt;integer\u0026gt; \u0026lt;integer\u0026gt; [1] 1 1 [2] 1 2 [3] 2 1 [4] 2 2 [5] 3 3 ... ... ... [11] 5 3 [12] 5 4 [13] 5 5 [14] 6 6 [15] 7 7 ------- queryLength: 7 / subjectLength: 7 findOverlaps(ir, ir, select = \u0026#34;first\u0026#34;) [1] 1 1 3 3 3 6 7 mergeByOverlaps(query, subject, ...) と findOverlapPairs(query, subject, ...) はHits型とは違う謎の形式で返す亜種。 countOverlaps(query, subject, maxgap = -1L, minoverlap = 0L, type, ...) いくつsubjectと重なるか、queryと同じ長さの整数vectorを返す。 countOverlaps(ir, ir) [1] 2 2 3 3 3 1 1 overlapsAny(query, subject, maxgap = -1L, minoverlap = 0L, type, ...) ひとつでもsubjectと重なるものがあるか、queryと同じ長さの論理vectorを返す。 overlapsAny(ir, ir[4]) [1] FALSE FALSE TRUE TRUE TRUE FALSE FALSE %over%, %within% %outside% という演算子で短く書けるけど可読性低下とconflictが怖いので使わない。 subsetByOverlaps(x, ranges, maxgap = -1L, minoverlap = 0L, type, invert = FALSE, ...) x[overlapsAny(x, ranges)] のショートカット。 overlapsRanges(query, subject, hits = NULL, ...) subsetByOverlapsした上、重なっている部分のみ切り詰めて返す。 poverlaps(query, subject, maxgap = 0L, minoverlap = 1L, type, ...) min() に対する pmin() のように、element-wiseに比較する。 poverlaps(ir, rev(ir)) [1] FALSE FALSE TRUE TRUE TRUE FALSE FALSE coverage(x, shift=0L, width=NULL, weight=1L, method=c(\u0026quot;auto\u0026quot;, \u0026quot;sort\u0026quot;, \u0026quot;hash\u0026quot;, \u0026quot;naive\u0026quot;)) 重なりの数をRle型で。 coverage(ir, shift = 200, width = 800) integer-Rle of length 800 with 13 runs Lengths: 200 7 5 2 4 1 5 5 4 2 4 7 554 Values : 0 1 2 1 2 3 2 1 0 1 0 1 0 cvg(x, from = NA, to = NA, weight = 1L, varname = \u0026quot;cvg\u0026quot;, collapse = FALSE, ...) も似たような関数だがドキュメント無し。 slice(x, ...) を使うと「カバレッジがこれ以上の区間」みたいなのを簡単に取れる: coverage(ir) |\u0026gt; IRanges::slice(lower = 2, rangesOnly = TRUE) IRanges object with 2 ranges and 0 metadata columns: start end width [1] 8 12 5 [2] 15 24 10 Neighboring nearest(x, subject, select = c(\u0026quot;arbitrary\u0026quot;, \u0026quot;all\u0026quot;)) 最も近いもの、あるいはoverlapしているもの。 nearest(ir, ir) [1] 1 1 2 3 3 6 7 precede(x, subject, select = c(\u0026quot;first\u0026quot;, \u0026quot;all\u0026quot;)) subjectのうちどれの上流にあるか。重なるものは除外。 precede(ir, ir) [1] 3 3 6 6 6 7 NA follow(x, subject, select = c(\u0026quot;last\u0026quot;, \u0026quot;all\u0026quot;)) subjectのうちどれの下流にあるか。重なるものは除外。 follow(ir, ir) [1] NA NA 2 2 2 4 6 distanceToNearest(x, subject, select = c(\u0026quot;arbitrary\u0026quot;, \u0026quot;all\u0026quot;)) 最も近いsubjectへの距離。overlapしている場合はゼロ。 distanceToNearest(ir, ir[1:3]) Hits object with 7 hits and 1 metadata column: queryHits subjectHits | distance \u0026lt;integer\u0026gt; \u0026lt;integer\u0026gt; | \u0026lt;integer\u0026gt; [1] 1 1 | 0 [2] 2 1 | 0 [3] 3 2 | 0 [4] 4 3 | 0 [5] 5 3 | 0 [6] 6 3 | 14 [7] 7 3 | 20 ------- queryLength: 7 / subjectLength: 3 distance(x, y) はelement-wizeに計算。 distance(ir, rev(ir)) [1] 27 20 0 0 0 20 27 ignore.strand GenomicRangesの操作はデフォルトでseqnamesとstrandごとに分けて行われる。 strandを無視するには ignore.strand = TRUE を渡す。\nreduce(gr) GRanges object with 3 ranges and 0 metadata columns: seqnames ranges strand \u0026lt;Rle\u0026gt; \u0026lt;IRanges\u0026gt; \u0026lt;Rle\u0026gt; [1] chr1 102-201 + [2] chr1 103-202 * [3] chr2 101-200 - ------- seqinfo: 2 sequences from an unspecified genome reduce(gr, ignore.strand = TRUE) GRanges object with 2 ranges and 0 metadata columns: seqnames ranges strand \u0026lt;Rle\u0026gt; \u0026lt;IRanges\u0026gt; \u0026lt;Rle\u0026gt; [1] chr1 102-202 * [2] chr2 101-200 * ------- seqinfo: 2 sequences from an unspecified genome ",
  "href": "/rstats/genomicranges.html",
  "tags": [
   "r",
   "bioconductor"
  ],
  "title": "GenomicRanges",
  "type": "rstats"
 },
 {
  "content": " \u0026ldquo;The Grammer of Graphics\u0026rdquo; という体系に基づいて設計されたパッケージ。 単にいろんなグラフを「描ける」だけじゃなく「一貫性のある文法で合理的に描ける」。\nRのグラフ描画システムにはgraphicsとgridの2つが存在しており、 R標準のboxplot()やhist()などは前者の上に、 本項で扱うggplot2は後者の上に成り立っている。 使い方が全く異なるので、前者を知らずにいきなりggplot2から始めても大丈夫。\ntidyverse に含まれているので、 install.packages(\u0026quot;tidyverse\u0026quot;) で一括インストール、 library(tidyverse) で一括ロード。\n初学者向け講義資料2023 https://ggplot2.tidyverse.org https://r-graphics.org/ https://r4ds.hadley.nz/data-visualize.html https://r4ds.hadley.nz/communication.html 基本的な使い方 まずは手元のデータを整形して、1行が1観測、1列が1変数という形の 整然データ を用意することが重要。 言い換えると「この列がx軸、この列がy軸、この列によって色を変える」というように指示できる形。 例えばggplot2に付属の mpg データ:\nlibrary(conflicted) library(tidyverse) mpg # manufacturer model displ year cyl trans drv cty hwy fl class # \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; # 1 audi a4 1.8 1999 4 auto(l5) f 18 29 p compact # 2 audi a4 1.8 1999 4 manual(m5) f 21 29 p compact # -- # 233 volkswagen passat 2.8 1999 6 manual(m5) f 18 26 p midsize # 234 volkswagen passat 3.6 2008 6 auto(s6) f 17 26 p midsize このようなデータを渡して、指示をどんどん + していく:\nggplot(data = mpg) + # mpgデータでキャンバス準備 aes(x = displ, y = cty) + # displ,cty列をx,y軸にmapping geom_point() + # 散布図を描く facet_wrap(vars(drv)) + # drv列に応じてパネル分割 theme_classic(base_size = 20) # クラシックなテーマで 途中経過をオブジェクトとして取っておける:\np1 = ggplot(data = mpg) p2 = p1 + aes(x = displ, y = cty) p3 = p2 + geom_point() p4 = p3 + facet_wrap(vars(drv)) p5 = p4 + theme_classic(base_size = 20) print(p5) 画像ファイルに保存するところまできっちり書く:\nggsave(\u0026#34;mpg.png\u0026#34;, p5, width = 6, height = 4, dpi = 300) Aesthetic mapping データと見せ方を紐付ける。 aes(color = Species) のように aes() 内で列名を指定すると、 その変数に応じて色やサイズなどを変えることができる。 言い換えると、データの値を色やサイズに変換する(スケールする)ことに相当する。\n# データによって点のサイズ・色・形を変える p2 + geom_point(mapping = aes(color = drv, size = cyl)) # aes() の外で指定するとデータによらず全体に反映。 # 常に色はオレンジ、サイズは6、不透明度は0.4 p2 + geom_point(color = \u0026#34;darkorange\u0026#34;, size = 6, alpha = 0.4) Aestheticsまとめ https://ggplot2.tidyverse.org/articles/ggplot2-specs.html\n色・透明度を変える color: 点や線の色 fill: 面の色 alpha: 不透明度 (0が透明、1が不透明) 大きさ・形を変える size: 点や文字の大きさ、線の太さ shape: 点の形 linetype: 線の種類 単にグループ分けする group: 反復試行の折れ線グラフなど、色や形はそのままで切り分けたいときに。 座標、始点、終点 x, y: 多くのグラフで必須項目 xmin, xmax, ymin, ymax, xend, yend: ものによって scale_*() で調整 変数をどうやって色やサイズに反映させるか、 各項目に対応する scale_*() 関数で調整する。\np1 + aes(x = displ, y = cty) + geom_point(aes(color = hwy)) + scale_color_fermenter() geom_point(mapping = aes(color = drv, size = cyl)) + scale_color_brewer(palette = \u0026#34;Set1\u0026#34;) + scale_size_identity() scale_color_gradient 連続値のデフォルト色スケール。 グラデーションの基準となる色を自由に指定できるけど、 後述のviridisのようによく計算されたスケールを使ったほうが無難。 scale_color_gradient(..., low, high, ...): 普通の連続値に scale_color_gradient2(..., low, mid, high, midpoint = 0, ...): ある中間値を挟んで上下に分けたいとき scale_color_gradientn(..., colors, values = NULL, ...): 多色のヒートマップなどに e.g., colors = c(\u0026quot;#000000\u0026quot;, \u0026quot;#0000FF\u0026quot;, \u0026quot;#00FFFF\u0026quot;, \u0026quot;#00FF00\u0026quot;, \u0026quot;#FFFF00\u0026quot;, \u0026quot;#FF0000\u0026quot;) 離散的に塗るには scale_color_steps()。 scale_color_viridis_c 色覚多様性が考慮されたパレットで、グレースケールでの明度変化も一定。 使用例と詳細説明はviridisパッケージのサイトを参照。 option = \u0026quot;viridis\u0026quot;, \u0026quot;magma\u0026quot;, \u0026quot;inferno\u0026quot;, \u0026quot;plasma\u0026quot;, \u0026quot;cividis\u0026quot;, \u0026quot;mako\u0026quot;, \u0026quot;rocket\u0026quot;, \u0026quot;turbo\u0026quot; 連続値は _c、離散値は _d、連続値を離散的に塗るには _b。 黄色が明るすぎて白背景で見にくい場合などは begin, end オプションで調整可能。 scale_color_hue 離散値のデフォルト色スケール。 scale_color_brewer いい感じに考えられたパレット Colorbrewer から選んべるので楽ちん。 利用可能なパレットは RColorBrewer::display.brewer.all() でも一覧できる。 離散値は _brewer、連続値は _distiller、連続値を離散的に塗るには _fermenter。 scale_*_identity for color, fill, size, shape, linetype, alpha 色の名前やサイズなどを示す列が予めデータに含まれている場合にそのまま使う。 scale_*_manual for color, fill, size, shape, linetype, alpha 対応関係を手動で指定する。 e.g., scale_color_manual(values = c(\u0026quot;4\u0026quot; = \u0026quot;red\u0026quot;, f = \u0026quot;green\u0026quot;, r = \u0026quot;blue\u0026quot;)) scale_size デフォルトではpointの面積を値にほぼ比例させるが、面積0にはならない。 値0に面積0を対応させるには scale_size_area() を使う。 半径を比例させるには scale_radius() があるけど要注意。 scale_alpha(..., range = c(0.1, 1))\nscale_linetype\nscale_shape\nスケール共通オプション 値と見え方の対応関係が凡例(legend/colorbar)として表示される。 scale_*() 関数に以下のオプションを指定することで設定を変えられる。 連続値の場合 と 離散値の場合 で微妙に意味が変わるけどだいたいこんな感じ:\nname: 凡例のタイトル。複数スケールで同じ名前にすると凡例が統合される。 breaks: 目盛りや凡例に登場させる値。 n.breaks: 目盛りをいくつに表示したいか。 labels: breaksの値に対応して実際に表示されるラベル。 breaksの値を受け取って文字列を返す関数 (e.g., scales::percent, scales::comma) を渡すこともできる。 limits: 数値なら最大値と最小値のvector。 文字列なら表示したいすべての値(順序も反映される)。 rescaler: データの値を [0, 1] の範囲にrescaleする関数。 デフォルトは scales::rescale()。 oob: out-of-boundsをどうするか。デフォルトは NA 扱い。 scales::squish を渡すとlimitsの値につぶす。 na.value: 欠損値のときどうするか。 guide: 文字列で \u0026quot;legend\u0026quot; か \u0026quot;colorbar\u0026quot;。 さらに細かく制御したい場合は guide_legend() や guide_colorbar() で。消したい場合は \u0026quot;none\u0026quot; か FALSE を渡せる。\n軸ラベルが密すぎて重なる場合に間引いたり、角度を付けたり、ずらしたりするには guide_axis() が使える。\nこれらを複数使う場合はscale関数のオプションではなく独立の guides() 関数を使う手もある。 変数によってパネルを分割する 多変量データを俯瞰するには、データに応じたパネル分割も便利。 色・サイズなどと合わせれば、x軸y軸プラス3次元程度はパッと可視化できることになる。\nfacet_wrap() 1変数で分割して並べる facet_wrap(facets, nrow = NULL, ncol = NULL, scales = \u0026#34;fixed\u0026#34;, shrink = TRUE, labeller = \u0026#34;label_value\u0026#34;, as.table = TRUE, switch = NULL, drop = TRUE, dir = \u0026#34;h\u0026#34;, strip.position = \u0026#34;top\u0026#34;) p3 + facet_wrap(vars(class), ncol = 4L) # new style p3 + facet_wrap(~ class, ncol = 4L) # old style facet_grid() 2変数以上で分割して縦横に並べる facet_grid(rows = NULL, cols = NULL, scales = \u0026#34;fixed\u0026#34;, space = \u0026#34;fixed\u0026#34;, shrink = TRUE, labeller = \u0026#34;label_value\u0026#34;, as.table = TRUE, switch = NULL, drop = TRUE, margins = FALSE, facets = NULL) p3 + facet_grid(vars(cyl), vars(class, drv)) # new style p3 + facet_grid(cyl ~ class + drv) # old style 1変数でいい場合は . ~ class のように片方をドットにする。\nファセットラベルの調整 labeller = デフォルトでは値だけがfacetラベルに表示されるが、 変数名を同時に表示したり、数式を表示したりもできる。 それを変数ごとに指定したい場合は labeller() 関数を使う。e.g.,\nlabeller = labeller(cyl = label_both, class = label_value) 見た目の調整はテーマの strip.* で。 パネル同士が近すぎて axis.text が重なってしまう場合は theme(panel.spacing = grid::unit(1, \u0026quot;lines\u0026quot;)) などとすれば間隔を空けられる。\n内部変数を使う ヒストグラムや箱ヒゲなどの表示に必要な計算は stat_*() を通して内部的に行われる。 そうした値の一部は after_stat() 関数を通じて参照し、 aes() 内で使うことができる。 例えば geom_histogram() や geom_bar() の縦軸を生のカウントから密度に変えるには aes(y = after_stat(density)) のようにする。 (昔は ..density.. とか stat(density) のようにしていた。)\nこれらはggplotオブジェクトを作るときではなく、描画するときに計算されるらしい。 強制的にそこまで計算させて値を参照するには ggplot_build() を使う。 たとえば h = hist(mpg$cty) のようなものを取得したいときは:\np = ggplot(mpg) + aes(x = cty) + geom_histogram(bins = 6L) ggplot_build(p)$data[[1L]] # layer_data(p) colorとfillを同色の透明度違いにする、とかやりたい場合も似たようなイメージで、 after_scale() を使って計算後・描画前に割り込める。\nhttps://ggplot2-book.org/layers.html#generated-variables\n座標軸やタイトルを変更 軸の区切りを変更したり対数にしたり scale_x_continuous(breaks = seq(10, 100, by = 10)) scale_y_log10(\u0026quot;Beer consumption\u0026quot;) scale_y_reverse() 上記のスケール共通オプションに加えて: expand = expansion(mult, add): デフォルトでは値域よりも少し(連続値5%、離散値0.6個分)だけ広く描かれるので、 それをゼロにするとか、もっと広くとるとか。 trans: 数値の変換。exp, identity, log, log10, reciprocal, reverse など。 文字列変数の順序を変えたい場合は limits のほうを使う。 position: top, bottom, left, right sec.axis: 第二軸 描画する範囲を指定 ylim(0, 42) + xlim(\u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;) coord_cartesian(xlim = NULL, ylim = NULL) 前者はデータそのものを切るが、後者はデータを変えずに描画領域だけ切る ゼロを含むようにちょっと伸ばすだけとかなら expand_limits(x = 0) が便利。 X軸とY軸の比率を固定 coord_fixed(ratio = 1) XY軸の転置 coord_flip() geom_*(orientation = \u0026quot;y\u0026quot;) も可。 geom_bar(), geom_histogram() では aes(y = にマッピングするだけでもいい。 極座標 パイチャートも作れるらしい 座標変換 coord_trans(x = \u0026quot;log10\u0026quot;, y = \u0026quot;sqrt\u0026quot;) 表示する座標を変換する。 stat前に適用される scale_x_* とは微妙に違う。 軸ラベルとタイトル labs(x = \u0026quot;time\u0026quot;, y = \u0026quot;weight\u0026quot;, title = \u0026quot;growth\u0026quot;, tag = \u0026quot;A\u0026quot;) xlab(\u0026quot;time\u0026quot;) + ylab(\u0026quot;weight\u0026quot;) + ggtitle(\u0026quot;growth\u0026quot;) 凡例 散布図なら点、折れ線なら線が凡例のキーとして自動的に選ばれるが、 geom_line(key_glyph = draw_key_rect) のように変更することもできる。 theme: 背景やラベルの調整 https://ggplot2.tidyverse.org/reference/ggtheme.html\n既成テーマ theme_gray(base_size = 11, base_family = \u0026quot;\u0026quot;)\ntheme_grey() 灰色背景に白い格子。 ggplotらしいデフォルトだが、論文には使いにくい。 theme_bw() 黒枠白背景にうっすら灰色格子 theme_linedraw() 細いけど濃い色の panel.grid theme_light() それを薄くした感じ theme_minimal() 外枠なしの theme_bw theme_classic() xy軸がL字に描かれているだけで枠もグリッドも無し theme_dark() 濃灰色背景に黒い格子。黄色の点とかが見やすい。 theme_void() 完全に枠なし これらをカッコ無しでコンソールに打ち込むと、 下記の各エレメントの設定方法やデフォルト値を知ることができる。\n引数に base_family = \u0026quot;Helvetica Neue\u0026quot; などとしてフォントを指定できる。 テーマを構成する axis.text などはこの設定を継承するが、 geom_text() や annotate(\u0026quot;text\u0026quot;, ...) などプロット内部の要素には影響しないことに注意。\nどうしても日本語を表示しないといけない場合は https://ill-identified.hatenablog.com/entry/2020/10/03/200618 を参考にして設定する。\nオプションとして base_line_size, base_rect_size もいじれるようになった。\nほかにもいろんなテーマが ggthemes というパッケージなどで提供されている。\n設定項目 https://ggplot2.tidyverse.org/reference/theme.html\ntheme() 関数に項目と値を指定したものを、 ほかのレイヤーと同じようにどんどん足していける。 テーマは直線、長方形、文字の3種類のエレメントからなり、 後述の element_***()を介して変更していく。\n## ベースとなるテーマを先に適用してから微調整 p3 + theme_bw() + theme( panel.background = element_rect(fill = \u0026#34;white\u0026#34;), # 箱 panel.grid = element_line(color = \u0026#34;blue\u0026#34;), # 線 axis.title = element_text(size = 32), # 文字 axis.text = element_blank() # 消す ) 全体 line: (element_line) rect: (element_rect) text: (element_text) title: (element_text; inherits from text) aspect.ratio: 軸タイトル、軸ラベル、目盛 axis.title: (element_text; inherits from text)\n__.x, __.x.top, __.y, __.y.right axis.text: (element_text; inherits from text)\n__.x, __.x.top, __.y, __.y.right axis.ticks: (element_line; inherits from line)\n__.x, __.y axis.ticks.length: (unit) axis.line: (element_line; inherits from line)\n__.x, __.y 凡例 legend.background: (element_rect; inherits from rect) legend.margin: (margin) legend.spacing:(unit)\n__.x, __.y legend.key: (element_rect; inherits from rect) legend.key.size: (unit) legend.key.height: (unit; inherits from legend.key.size) legend.key.width: (unit; inherits from legend.key.size) legend.text: (element_text; inherits from text) legend.text.align: (number from 0 (left) to 1 (right)) legend.title: (element_text; inherits from title) legend.title.align: (number from 0 (left) to 1 (right)) legend.position: (\u0026quot;left\u0026quot;, \u0026quot;right\u0026quot;, \u0026quot;bottom\u0026quot;, \u0026quot;top\u0026quot;, \u0026quot;none\u0026quot; c(0, 1)) legend.direction: (\u0026quot;horizontal\u0026quot; or \u0026quot;vertical\u0026quot;) legend.justification: (\u0026quot;center\u0026quot; or c(0, 1) のような数値でアンカー位置を指定) legend.box: (\u0026quot;horizontal\u0026quot; or \u0026quot;vertical\u0026quot;) legend.box.just: (\u0026quot;top\u0026quot;, \u0026quot;bottom\u0026quot;, \u0026quot;left\u0026quot;, or \u0026quot;right\u0026quot;) legend.box.margin: (margin) legend.box.background: (element_rect; inherits from rect`) legend.box.spacing:(unit) プロット領域の背景、余白、格子 panel.background: (element_rect; inherits from rect) panel.border: (element_rect; inherits from rect; should be used with fill = NA) panel.spacing: (unit; facet_* の間隔)\n__.x, __.y panel.grid: (element_line; inherits from line) panel.grid.major: (element_line; inherits from panel.grid)\n__.x, __.y panel.grid.minor: (element_line; inherits from panel.grid)\n__.x, __.y 全体の背景、タイトル、余白 plot.background: (element_rect; inherits from rect) plot.title: (element_text; inherits from title) plot.subtitle: (element_text; inherits from title) plot.caption: (element_text; inherits from title) plot.margin: (unit with the sizes of the top, right, bottom, and left margins) facet したときのラベル strip.background: (element_rect; inherits from rect) strip.placement: (\u0026ldquo;inside\u0026rdquo;, \u0026ldquo;outside\u0026rdquo;) strip.text: (element_text; inherits from text)\n__.x, __.y その他 complete: 部分的な変更か、完全なテーマか (FALSE) validate: 毎回チェックするか (TRUE) エレメント element_rect(fill, color, size, linetype, inherit.blank) \u0026mdash; 長方形 fill: 塗りつぶしの色 color: 枠の色 element_line(color, size, linetype, lineend, arrow, inherit.blank) \u0026mdash; 線\nelement_text(family, face, color, size, hjust, vjust, angle, lineheight, margin) \u0026mdash; 文字 family: フォントファミリー。 空なら base_family を継承。 face: (\u0026quot;plain\u0026quot;, \u0026quot;italic\u0026quot;, \u0026quot;bold\u0026quot;, \u0026quot;bold.italic\u0026quot;) hjust, vjust: 水平位置と垂直位置の寄せ方をそれぞれ [0, 1] の実数で。 angle: 角度 [0, 360] margin: スペース調整を関数 margin(top, right, bottom, left) 越しに。 element_blank() \u0026mdash; 空 消したい要素にはこれを指定する rel(x) デフォルトからの相対値で size 引数を指定したいときに。 grid::unit(x, units, data = NULL) こちらは絶対指定。grid パッケージに入ってる。 units で使いそうなのは cm, mm, inches, points, lines, native grid::arrow(angle, length, ends, type) axis.line の element_line() にこれを与えて軸を矢印にするとか。 margin(t = 0, r = 0, b = 0, l = 0, unit = \u0026quot;pt\u0026quot;) marginクラス calc_element(element, theme, verbose = FALSE) 継承などを考慮した上でelementがどんな値にセットされるか確かめる。 ファイルに書き出す RStudioやQuartzの保存ダイアログを利用して書き出すこともできるけど、 それだとサイズ調整やファイル名入力を手作業でやることになってしまう。 ggsave() をスクリプトに書いておけば何回でも同じ設定で出力できる。\nggsave(filename, plot = last_plot(), device = NULL, path = NULL, scale = 1, width = NA, height = NA, units = c(\u0026#34;in\u0026#34;, \u0026#34;cm\u0026#34;, \u0026#34;mm\u0026#34;), dpi = 300, limitsize = TRUE, ...) 画像形式はファイル名の拡張子から自動的に判別される (e.g., diamonds.png, diamonds.pdf)。 widthやheightを小さくするほど、文字・点・線などの要素が相対的に大きくなる。 dpiを変えることで、見た目のバランスを保ったまま解像度を変えられる。 (これはPNGなどラスタ形式だけの話。PDFなどのベクタ形式なら気にしなくていい) タイトルや軸ラベルの文字サイズを変えたいときはテーマの theme_bw(base_size = 42) や各要素の element_text(size = 42) を使う。 scale = や units = を使うのはよほど必要になったときだけ。 PDF出力でフォント関連の問題が生じたら device = cairo_pdf を試す。 # 7inch x 300dpi = 2100px四方 (デフォルト) ggsave(\u0026#34;mpg1.png\u0026#34;, p3) # width = 7, height = 7, dpi = 300 # 4 x 300 = 1200 全体7/4倍ズーム ggsave(\u0026#34;mpg2.png\u0026#34;, p3, width = 4, height = 4) # dpi = 300 # 2 x 600 = 1200 全体をさらに2倍ズーム ggsave(\u0026#34;mpg3.png\u0026#34;, p3, width = 2, height = 2, dpi = 600) # 4 x 300 = 1200 テーマを使って文字だけ拡大 ggsave(\u0026#34;mpg4.png\u0026#34;, p3 + theme_bw(base_size = 22), width = 4, height = 4) プロットの種類 散布図 geom_point(size = 2, alpha = 0.3) 重なった点をランダムにばらかしたいときは geom_jitter() 点の形(shape)一覧 デフォルトの shape = 19 の丸は半透明にすると縁取りしたように見えてしまう。 shape = 16 の丸は均一なのでこっちを常に使うように設定できればいいんだけど。 折れ線グラフ geom_path(size = 2, linetype = \u0026quot;dashed\u0026quot;) データ順に結ぶ geom_line() x軸上の順で結ぶ geom_step() 階段状に結ぶ 線の種類(linetype)一覧 面グラフ geom_ribbon() \u0026mdash; yminからymaxの面 geom_area() \u0026mdash; 0からyの面 ヒストグラム、密度曲線 geom_histogram() \u0026mdash; 棒グラフ(連続値をstat_bin() で区切って) geom_bar() \u0026mdash; 棒グラフ(離散値をstat_count()で数えて) geom_freqpoly() \u0026mdash; 折れ線 geom_density() \u0026mdash; 密度推定されたスムーズな線 geom_bin2d() \u0026mdash; 二次元ヒストグラム geom_hex() \u0026mdash; 六角形版二次元ヒストグラム 棒グラフ geom_col() グループ分けする場合のオプション: position = \u0026quot;stack\u0026quot;: 縦に積み重ねる (デフォルト) position = \u0026quot;dodge\u0026quot;: 横に並べる position = \u0026quot;fill\u0026quot;: 縦に積み重ね、高さを1に揃えて割合を示す 箱ひげ図とその代替案 geom_boxplot(): 生データの分布に関する情報量が落ちすぎるので要注意。 geom_violin(scale = \u0026quot;count\u0026quot;): 箱ひげ図よりは密度が分かりやすいけど、推定であることとスケールに注意。 ggridges::geom_density_ridges(): violinの片側を横にしたようなもの。 geom_dotplot() or ggbeeswarm::geom_beeswarm(): 生データの分布が読み取りやすい。 ヒートマップ geom_tile(aes(fill = z)) geom_raster(aes(fill = z)) \u0026mdash; 各タイルの大きさを揃える制約のため高速 エラーバー geom_errorbar(aes(ymax = y + se, ymin = y - se), width = 0.1) geom_linerange(...) geom_pointrange(...) 関数 ggplot(data.frame(x = c(-4, 4))) + aes(x) + stat_function(fun = dnorm, args = c(0, 1), n = 200) 回帰曲線 geom_smooth(method = glm, method.args = list(family = poisson), se = FALSE) 始点と終点で曲線や矢印を描く geom_curve(aes(x, y, xend, yend), curvature = -0.2) geom_segment(aes(x, y, xend, yend), arrow = arrow(type = \u0026quot;closed\u0026quot;), linejoin = \u0026quot;mitre\u0026quot;) 矢印の調整はgrid::arrow()。 普通の線より矢尻の分だけ長くなることに注意。 切片と傾きで直線を描く geom_abline(intercept = 3, slope = 5) geom_hline(yintercept = 7) + geom_vline(xintercept = 11) これらは annotate() 的な位置づけであり、ほかの geom_ 関数とはかなり挙動が違う。 そのためか annotate(\u0026quot;hline\u0026quot;, yintercept = 7) なども思い通りの結果にはならない。 文字列や図形を書き加える annotate(\u0026quot;text\u0026quot;, x = 1:4, y = 4:1, label = sprintf(\u0026quot;x = %d\u0026quot;, 1:4)) テーマの base_family は引き継がれないので family = で指定すべし。 数式を表示するには label = \u0026quot;italic(N[t])\u0026quot; のような文字列で渡して parse = TRUE。 データ範囲によらず相対位置を指定したいとき、公式のオプションは無いが、 annotate(\u0026quot;text\u0026quot;, x = -Inf, y = Inf, hjust = 0, vjust = 1, label = \u0026quot;top-left\u0026quot;) のように Inf とjustificationを駆使すれば端付近に置くことができる。 データ点に対応する文字列を添えるには geom_text(aes(label = foo)) のほうが適している。 オプションで nudge_x = 2, nudge_y = 2 などとすれば点と重ならないようにずらせる。 position_nudge() Extensions https://ggplot2.tidyverse.org/articles/extending-ggplot2.html https://exts.ggplot2.tidyverse.org/ ggplotを拡張するための仕組みがversion 2.0から正式に導入され、 ユーザーが独自の stats や geom を作って登録することが容易になった。\ngridExtra https://github.com/baptiste/gridextra/wiki\nデータによって自動的にパネルを分割するには facet_grid() や facet_wrap() を使えばよいが、 関係ない複数の図を1枚に描きたい場合は grid や gtable の機能を使う必要がある。 gridExtra はそのへんの操作を手軽にできるようにしてくれるパッケージ。 残念ながら開発中止？\n\u0026ldquo;grob\u0026rdquo; は \u0026ldquo;grid graphical object\u0026rdquo; の略。 ggplotオブジェクトと同じように ggsave() に渡して保存可能。\ngrob = gridExtra::arrangeGrob(p1, p2, nrow = 2, ncol = 1, bottom = \u0026#34;Time\u0026#34;) grid.newpage() grid.draw(grob) 複数ページのPDFに書き出したい場合は purrr などを使って list of ggplots を作っておき、 marrangeGrob() に渡す。\n.grobs = purrr::map(.dataframes, my_ggplot_func) .gtable = gridExtra::marrangeGrob(.grobs, nrow = 4, ncol = 3) ggsave(\u0026#34;multi_page.pdf\u0026#34;, .gtable, width = 7, height = 9.9) cowplot https://github.com/wilkelab/cowplot https://cran.r-project.org/web/packages/cowplot ggplotを学術論文向けにカスタマイズしやすくする。 主な利用目的はgridExtraと同じでggplotを並べる機能。 論文figureのようなA, B, Cラベルをオプションで簡単に付けられるのが良い。\ncowplot::plot_grid() facet_wrap()のように、ざっと並べるのに便利。 もちろん入れ子も可能。 cowplot::plot_grid(..., plotlist = NULL, align = c(\u0026#34;none\u0026#34;, \u0026#34;h\u0026#34;, \u0026#34;v\u0026#34;, \u0026#34;hv\u0026#34;), nrow = NULL, ncol = NULL, scale = 1, rel_widths = 1, rel_heights = 1, labels = NULL, label_size = 14, hjust = -0.5, vjust = 1.5) さらに細やかな制御をしたいときは以下の関数を個別に重ねていく。\ncowplot::ggdraw(plot = NULL, xlim = c(0, 1), ylim = c(0, 1)) これの後ろに + 演算子で draw_***() を足していく。 cowplot::get_legend(plot) 凡例が共通する図を並べるとき、代表のやつをこれで取っておいてあとで並べる。 draw_figure_label() draw_grob() draw_label() draw_line() draw_plot() draw_plot_label() draw_text()\ntheme_cowplot() ggsave2()\nパッケージ読み込みと同時に勝手にテーマを変更したり、 ggsave() 関数を上書きしたりという問題が過去にはあったが、今は大丈夫。\nできあがった図を並べるための新しいパッケージとして patchwork がすごくエレガントで期待大。 ただし、演算子を多用するスタイルは忘れやすく検索しにくい諸刃の剣。\n関連書籍 ",
  "href": "/rstats/ggplot2.html",
  "tags": [
   "r",
   "graph",
   "tidyverse"
  ],
  "title": "ggplot2",
  "type": "rstats"
 },
 {
  "content": "https://git-scm.com/\nGitは分散型バージョン管理システムの代表格。 プログラムのソースコードはもちろんのこと、 研究ノートや論文の原稿などあらゆるテキストの管理に使える。\nGitHubはGitをより便利に使うためのオンラインサービス。 個人的なリポジトリ置き場としてはもちろんのこと、 ほかの人と共有・協力してプロジェクトを進めるプラットフォームとしても使える。\nGitのライバルとしてMercurialもあるが、 BitBucket (GitHubのライバル) がGit対応した今となってはMercurialを積極的に使う理由は無い気がする。\n基本 準備 ローカルマシンにGitをインストールする。 最初から入ってる場合も多いけど、それが古すぎる場合は Homebrew で新しいのを入れる。\nGitHubに個人アカウントを作る。\nSSH公開鍵を作ってマシンとGitHubに登録する。 https://help.github.com/articles/connecting-to-github-with-ssh/\n~/.gitconfig にユーザ名やアドレスを登録する。 https://git-scm.com/docs/git-config\ngit config --global user.name \u0026#34;Watal M. Iwasaki\u0026#34; git config --global user.email \u0026#34;heavywatalあmail.com\u0026#34; less ~/.gitconfig ついでに pushinsteadof の設定をしておく。 httpsで高速にclone/fetch/pullして、 sshでパスワード無しでpushする、というのが楽ちん。\n設定例: heavywatal/dotfiles/.gitconfig\n手元の変更を外に伝える 📁 working directory (working tree) 手元のファイルの変更はまだリポジトリに登録されていない ↓ add staging area (index) 次のコミットに含めるファイルをマークする段階 ↓ commit local repository 変更履歴が .git/ 内に記録されている ↓ push remote repository GitHubなど別マシンのリポジトリに反映 外部の変更を手元に取り込む remote repository ↓ fetch local repository 変更が .git/ に取り込まれたが、見えてるファイルには反映されてない ↓ merge or rebase 📁 working directory 手元のファイルが最新版に同期されている 用語 blob git内部で1つのファイルを指すオブジェクトで、add時に作られる。 ファイル名などのメタデータは持たず、 ファイルの内容にのみ依存したハッシュIDを持つ。 tree git内部で1つのディレクトリを指すオブジェクトで、commitした時に作られる。 blobやファイル名などのメタデータに依存したハッシュIDを持ち、 その変化は親に伝播する。 commit git内部でroot treeのsnapshotを指すオブジェクト。 root treeのハッシュID、著者、コメントなどの情報を持つ。 動詞としては、staging areaの情報をひとつのcommitとしてリポジトリに登録することを指す。 repository commitの履歴を保持する拠点。 git init で手元に新規作成するか、git clone でリモートから複製する。 origin remoteリポジトリの典型的なshortname。 clone時に自動的に追加され、 push先やfetch元を省略したときにデフォルトで使われる。 git remote -v で確認。 他の人のリポジトリをforkして使う場合、 自分のを origin, 元のを upstream と名付けるのが一般的。 master, main デフォルトのブランチの典型的な名前。 HEAD, @ 現在参照しているbranch/commitを指すポインタ。 基本的にはmasterの最新commitを指していることが多い。 1つ前は HEAD^ か HEAD~、 2つ前は HEAD^^ か HEAD~~ か HEAD~2。 (HEAD^2 は merge で複数の親がある場合の2番目) zshのEXTENDED_GLOBが有効になってる場合は HEAD^ がパターン扱いされてエラーになるので、 HEAD\\^ のようにエスケープするか unsetopt NOMATCH しておいたほうがいい。\nよく使うコマンド reset git reset \u0026lt;DESTINATION\u0026gt; は HEAD の位置を戻す処理で、 オプションによってindexとworing treeもそこに合わせるように変更される。 --soft なら HEAD 移動のみ。 --mixed なら移動した HEAD にindexも合わせる。 --hard なら移動した HEAD にindexとworiking treeも合わせる。 直前の動作を取り消す用途に絞って使うのが無難:\n# commit直後、それを取り消す (indexとworkingはそのまま) git reset --soft HEAD^ # add直後、それを取り消す (workingとHEADはそのまま) git reset --mixed HEAD # 変更したファイルをHEADの状態に戻す (DANGEROUS!) git reset --hard HEAD # reset直後、それを取り消す git reset --hard ORIG_HEAD # divergedになってしまった手元のbranchを破棄 (DANGEROUS!) git reset --hard origin/master 直前のcommitをちょっと修正したいだけなら git commit --amend が簡単。 それより前のを修正するには git rebase -i HEAD~3 とかで戻ってrewordやedit。\ndiff 差分を表示:\n# HEAD vs working (staging前のファイルが対象) git diff # HEAD vs index (staging済みcommit前のファイルが対象) git diff --staged # HEAD vs working+index (commit前の全ファイルが対象) git diff HEAD # 特定コミットの変更点 (diffじゃない...) git show [revision] rm, clean tracking対象から外して忘れさせる(手元のファイルはそのまま):\ngit rm --cached \u0026lt;file\u0026gt; .gitignore で無視されてるuntrackedファイルを消す:\ngit clean -fdX 無視されていないuntrackedファイルも消したい場合は小文字の -fdx (危険)。\ntag 特定のコミット(省略するとHEAD)にタグ付けする。 lightweightとannotatedの2種類が存在し、後者にはメッセージなどが紐付く。\ngit tag v0.1.0 [revision] git tag -a v0.1.0 -m \u0026#34;Message!\u0026#34; GitHubリポジトリに git push --tags するとアーカイブが作られ、 Releasesページに反映される。 annotated tagであれば git push --follow-tags でcommitとtagを同時にpushできる。\nhttps://git-scm.com/book/en/Git-Basics-Tagging\nrebase ブランチの根本を別のコミットに付け替える。 よく使うのは、開発ブランチを master の先頭に追従させるとき。\ngit switch some-branch git rebase master 最も近い共通祖先(MRCA)コミットから先を丸ごと移すだけならこのように単純だが、 ブランチのブランチとか、ブランチの一部だけを移したい場合は次のようにする。\ngit rebase --onto \u0026lt;newbase\u0026gt; \u0026lt;base\u0026gt; \u0026lt;tip\u0026gt; これで base–tip 間のコミットがnewbaseから伸びる形になる。\nSubmodule 既存のリポジトリをsubmoduleとして追加する git submodule add https://github.com/mbostock/d3.git # ブランチを指定する場合: git submodule add -b gitsubmodule_https https://github.com/heavywatal/x18n.git gh-pages で公開する場合は参照プロトコルを git:// ではなく https:// にする必要がある。\nsubmoduleを含むメインリポジトリを使い始めるとき 最初にclone/fetchしてきた時submoduleたちは空なのでまず:\ngit submodule update --init # 使いたいbranchがmasterではない場合は --remote git submodule update --init --remote x18n # 歴史があって重いリポジトリはshallowに git submodule update --init --depth=5 d3 submoduleを更新 更新分をまとめて取得:\ngit submodule foreach git fetch 好きなコミット/タグまで移動 (旧git checkout):\ncd d3/ git switch --detach v3.5.6 \u0026ldquo;detached HEAD\u0026rdquo; 状態になる。\nメインリポジトリでその変更をコミット:\ncd .. git commit GitHub Pages https://help.github.com/articles/user-organization-and-project-pages/\nユーザーサイトを作る USERNAME.github.io という名前のリポジトリをGitHub上で作成 公開したいウェブサイトをmasterブランチとしてpush https://USERNAME.github.io にアクセスしてみる。 例えば本ウェブサイトは heavywatal.github.io というリポジトリの sourceブランチでMarkdownテキストを書き、 Hugo で変換・生成したHTMLファイルをmasterブランチに書き出している。\nGitHubが勝手にJekyll処理しようとすることがあるので、 .nojekyll という空ファイルを作っておく。\nプロジェクトサイトを作る https://help.github.com/articles/configuring-a-publishing-source-for-github-pages/\nリポジトリの内容を https://USERNAME.github.io/PROJECT/ に公開することができる。 方法は複数あり、リポジトリの設定画面から選択できる。\ngh-pages ブランチの内容を公開 (古い方法) master ブランチの内容を公開 master ブランチの /docs ディレクトリのみを公開 前は1番の方法しかなくてブランチの扱いがやや面倒だったが、 今ではmasterだけで簡単に済ませられるようになった。\nPull Request (PR) 大元のリポジトリをupstream、フォークした自分のリポジトリをoriginと名付ける。 デフォルトブランチ(masterとかdevelopとか)は更新取得のためだけに使い、変更は新規ブランチで行う。 push済みのcommitをrebaseするとIDが変わっちゃうのでダメ。 基本の流れ 例えば USER さんの PROJECT のコード修正に貢献する場合。\ngithub.com/USER/PROJECT のForkボタンで自分のGitHubリポジトリに取り込む\nforkした自分のリポジトリからローカルにclone:\ngit clone https://github.com/heavywatal/PROJECT.git cd PROJECT/ 大元のリポジトリにupstreamという名前をつけておく:\ngit remote add upstream git://github.com/USER/PROJECT.git PR用のブランチを切って移動:\ngit switch -c fix-typo コードを変更してcommit:\nvim README.md git diff git commit -a -m \u0026quot;Fix typo in README.md\u0026quot; この間にupstreamで更新があったか確認:\ngit fetch upstream 必要ならそれを取り込む:\ngit switch master git merge --ff-only upstream/master git switch fix-typo git rebase master 自分のリポジトリにpush:\ngit push [-f] origin fix-typo PR用のURLが表示されるのでそこから飛ぶ。 もしくはGitHub上に出現する\u0026quot;Compare \u0026amp; pull request\u0026quot;ボタンを押す。\n差分を確認し、コメント欄を埋めて提出\n修正を求められたらそのブランチで変更し、自分のリポジトリにpushすればPRにも反映される\nマージされたらブランチを消す\n問題と対処 Trailing whitespace 以外の変更だけ add する 大概のIDEには保存時に行末の空白を自動削除するオプションがある。 それによって自分のソースコードは常にきれいに保てるが、 他人の汚いコードやknitrの結果などを編集するときに余計な差分を作ってしまう。 VSCode なら \u0026ldquo;Save without Formatting\u0026rdquo; で設定を変えずに済ませられることは覚えていても、 \u0026ldquo;Find in Files\u0026rdquo; で一括編集したときにも空白が削られることは忘れがち。\ngit diff --ignore-space-at-eol | git apply --cached 上記ワンライナーで大概うまくいくが、 変更箇所が近かったりすると error: patch failed と蹴られる。 その場合は次のようにworkaround:\ngit diff --ignore-space-at-eol \u0026gt; tmp.diff git stash git apply --cached tmp.diff git stash drop 用済みブランチの掃除 PRマージ済みのリモートブランチを消したい。 一番簡単なのは、GitHub PR画面のdelete branchボタンを押すこと。 手元のgitからやるには、明示的に --delete するか、空ブランチをpushするか:\ngit push --delete origin issue-42 git push origin :issue-42 リモートブランチを消しても手元のリポジトリには残る。 まず確認:\ngit branch -a git remote show origin こうしたstaleなブランチを刈り取る方法には二通りある:\ngit fetch --prune git remote prune origin それでも消えないローカルブランチは手動で消す:\ngit branch -d issue-666 error: The branch \u0026#39;issue-666\u0026#39; is not fully merged. If you are sure you want to delete it, run \u0026#39;git branch -D issue-666\u0026#39;. マージ済みのブランチじゃないと上記のように怒ってくれる。 消してもいい確信があればオプションを大文字 -D にして強制削除。\ndetached HEAD からの復帰 submoduleなどをいじってると意図せずdetached HEAD状態になることがある。 その状態でcommitしてしまった変更をmasterに反映したい。\npushしようとして怒られて気付く\ngit push fatal: You are not currently on a branch git status HEAD detached from ******* masterに戻ると道筋を示してくれる:\ngit switch master Warning: you are leaving 2 comits behind, not connected to any of your branches If you want to keep them by creating a new branch, this may be a good time to do so with: git branch \u0026lt;new-branch-name\u0026gt; ******* 言われたとおりbranchを作ってmerge\ngit branch detached ******* git merge detached 不要になったbranchを消す\ngit branch -d detached サブディレクトリを別リポジトリに切り分ける 新しく作りたいリポジトリ名で元リポジトリをクローン:\ngit clone https://github.com/heavywatal/hello.git bonjour filter-branch でサブディレクトリの歴史だけ抜き出す:\ncd bonjour/ git filter-branch filter-branch --subdirectory-filter subdir git log ls # サブディレクトリの中身がルートに来てる GitHubなどリモートにも新しいリポジトリを作って登録、プッシュ:\ngit remote set-url origin https://github.com/heavywatal/bonjour.git git push -u origin master 別のリポジトリをサブディレクトリとして取り込む Subtree Merging\nオプション -X subtree=${subdir} を利用してサブディレクトリに入れると、 全体の git log では統合されてるように見えるけど、 各ファイルの履歴は途絶えてしまって git log --follow ${subdir}/hello.cpp などとしても統合前までは辿れない。 予め全ファイルをサブディレクトリに動かすだけのcommitをしておいて、 ルート同士でmergeすると --follow が効く状態で取り込める。\ncd /path/to/${subrepo}/ mkdir ${subdir} git mv $(git ls-tree --name-only master) ${subdir}/ git commit -m \u0026#34;:construction: Move all to ${subdir}/ for integration\u0026#34; cd /path/to/${mainrepo}/ git remote add ${subrepo} /path/to/${subrepo} git fetch ${subrepo} git merge --no-commit --allow-unrelated-histories ${subrepo}/master git commit fetchせずにmergeしようとするとブランチ情報が無くて怒られる: not something we can merge\n異なる起源をもつリポジトリのmergeは危険なので --allow-unrelated-histories を明示しないと拒否される: fatal: refusing to merge unrelated histories\n",
  "href": "/dev/git.html",
  "tags": [
   "vcs",
   "writing"
  ],
  "title": "Git",
  "type": "dev"
 },
 {
  "content": "Unixツールをパッケージとして手軽にインストールできるパッケージ管理ソフト。\nhttps://brew.sh/\nInstallation Command Line Tools をインストールする。 cf. /dev/devenv:\nxcode-select --install Xcodeを丸ごとインストールしてある場合でも独立CLTが必要らしい。\nターミナルから下記のコマンドを実行し、指示に従う:\n/bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\u0026#34; ちゃんと入ったか確認:\nbrew doctor https://docs.brew.sh/Installation.html\nデフォルトの /usr/local/ にインストールするのが嫌なら、 例えばホーム以下の ~/.homebrew/ にインストールすることもできる:\nmkdir ~/.homebrew curl -L https://github.com/Homebrew/brew/tarball/master | tar xz --strip 1 -C ~/.homebrew が、/usr/local/ 以外にインストールすると、 bottle機能を封じられて毎回自前ビルドすることになるみたいなので、 非力なラップトップとかでは結構厳しい。\nUsage https://docs.brew.sh/FAQ.html\nHomebrew本体とカタログをアップデートし、アップグレード可能なパッケージを表示:\nbrew update \u0026amp;\u0026amp; brew outdated outdated なものを全てアップグレード:\nbrew upgrade パッケージのバージョンを固定し、一括 brew upgrade の適用外にする。 頻繁に更新され、やたらCPUを使うやつらに。\nbrew pin imagemagick パッケージ検索:\nbrew search text パッケージ情報の表示:\nbrew info formula パッケージのインストール・アンインストール:\nbrew install formula brew uninstall formula インストール済みパッケージ、またはパッケージ内ファイルの一覧:\nbrew list [formula] brew install 公式リポジトリから明示的にインストールしたものメモ:\naspell autoconf automake binutils boost cmake coreutils diffutils doxygen eigen emacs exiftool findutils fswatch git gnu-sed gnu-tar go grep imagemagick less lftp make nano nkf pandoc parallel pyenv qpdf rbenv rsync sshfs tmux vim webp wget xz zsh zsh-completions Rをここからインストールするとバイナリ版のパッケージが利用できず、 毎回ソースからビルドすることになるので、 後述のcaskのほうの r を入れるほうが簡単。\ncoreutils, gnu-sed, gnu-tar, grep などは既存のコマンドとごっちゃにならないよう頭に g を付けてインストールしてくれる。 元の名前でアクセスする方法はいくつかあるが、 $(brew --prefix)/opt/{coreutils,gnu-sed,gnu-tar,grep}/libexec/gnubin に PATH を通すのが楽ちん。 brew unlink coreutils gnu-sed gnu-tar grep してもそれらのディレクトリは残る。\nbrew tap https://docs.brew.sh/brew-tap.html https://docs.brew.sh/Interesting-Taps-and-Forks.html 明示的にリポジトリを追加する:\nbrew tap brewsci/bio brew install libsequence 暗黙に brew tap しつつ直接インストールも可能: brew install brewsci/bio/libsequence\nバイオインフォマティクスなど科学計算のツール群はHomebrew公式タップ homebrew/science に収録されていたがdeprecatedになった。 brewsci/science がフォーミュラを一旦引き継いで、 homebrew/core と brewsci/bio に振り分けて移行を進めている。\nTapを作る https://docs.brew.sh/How-to-Create-and-Maintain-a-Tap.html\nGitHubに homebrew-nicetap のような名前のリポジトリを作成し、 ルート直下の Formula/ に goodtool.rb のようなファイルを置くだけ。 使うときは brew install \u0026lt;username\u0026gt;/nicetap/goodtool のようにリポジトリ名から homebrew- を削った形で指定する。\nFormulaを作る https://docs.brew.sh/Formula-Cookbook.html\n新規作成するには brew create \u0026lt;URL\u0026gt; コマンドが便利。\nurl には git tag でGitHubに作られるバージョン付きアーカイブを指定してやるのが楽ちん。\nhead にリポジトリを登録してバージョン無しで運用することも可能。 ただし brew upgrade ではHEADの更新をチェックしてくれないので要注意。 brew reinstall するしかないのかな？\nCask GUIアプリケーションもHomebrewで管理してしまおうという野心的な拡張機能。 昔は brew cask install のような形で使っていたが、 今は brew 本体に統合されている。 同名のformulaがある場合などは --cask で限定できる:\nbrew install --cask r rstudio brew list --cask アプリ側でアップデートを実行するとCask内でのバージョンと食い違っちゃうけど使用上は問題ないらしい。\nadobe-acrobat-reader amazon-photos basictex bibdesk discord docker drawio dropbox firefox gimp gitter google-chrome google-drive google-japanese-ime inkscape iterm2 joplin julia kindle macfuse megasync menumeters monitorcontrol quarto r rstudio skim skype slack spideroakone the-unarchiver virtualbox visual-studio-code vlc whatsapp xquartz zoom\nQuicklook See Quicklook.\nFonts brew tap homebrew/cask-fonts font-ubuntu font-ubuntu-mono font-ubuntu-mono-nerd-font font-noto-sans font-noto-serif font-noto-sans-mono font-source-sans-3 font-source-serif-4 font-open-sans font-roboto font-dejavu font-libertinus font-lora font-merriweather\n",
  "href": "/mac/homebrew.html",
  "tags": [
   "mac",
   "package"
  ],
  "title": "Homebrew",
  "type": "mac"
 },
 {
  "content": "Markdown記法のテキストをHTMLに変換する、静的ウェブサイト生成プログラム。 公式ドキュメントが充実しているので基本的にそれに従えば簡単にできる。\nhttps://gohugo.io/documentation/ https://github.com/gohugoio/hugo 高速さとシンプルさに惹かれてSphinxから移行し、 本サイトもHugoでビルドしている。 オフラインの研究ノートとしても有用。\nQuickstart https://gohugo.io/getting-started/quick-start/\nHugo本体をインストール。 方法はいろいろ用意されてる。\n手動でよければOSに合った公式prebuilt binaryをダウンロードしてPATHを通すのが簡単。 コマンドで管理するならHomebrewで一発: brew install hugo ソースコードを改変したりしたい場合は git から: export GOPATH=${HOME}/.go export PATH=${PATH}:${GOPATH}/bin mkdir ${HOME}/src cd ${HOME}/src git clone https://github.com/gohugoio/hugo.git cd hugo go install -v SCSSのための --tags extended オプションは不要になった。 SCSSを使う場合は Dart Sass を別途インストールする。 この方法もいろいろあるけど 公式prebuilt binary を使うのが簡単。適当に落としてPATHを通す。 wget -O- https://github.com/sass/dart-sass/releases/download/1.69.5/dart-sass-1.69.5-macos-x64.tar.gz | tar xz\nちゃんとインストールできているか確認: hugo env\n骨組みを作る:\ncd path/to/site hugo new site . ページをMarkdownで書く:\nhugo new about.md +++ date = 2016-02-26T19:10:22+09:00 title = \u0026#34;About\u0026#34; +++ ## Heading normal *italic* **bold** テーマをとりあえず全部インストール:\ngit clone --depth 1 --recursive https://github.com/gohugoio/hugoThemes.git themes 適当なテーマでウェブサーバーを走らせる:\nhugo server --theme blank ブラウザから http://localhost:1313/about にアクセスしてみる。 hugo server, hugo -w はファイルを監視していて変更をすぐに反映する。\n設定 https://gohugo.io/getting-started/configuration/\n長らく config.toml だったが今は hugo.toml がデフォルト。 config/_default/hugo.toml に置いても同じ。\nconfig/ 直下のディレクトリ名と -e/--environment オプションで切り替え可能。 ただしデフォルトの挙動がわかりにくい罠なので注意。例えば config/_default/ と config/production/ を持って hugo を実行するとproduction環境になってしまう。 production環境を作らず config/public/ とかにしておけば明示的に -e public を渡さない限り常にデフォルトのdevelopment環境になるので分かりやすい。\nhugo # -e production (confusing!) hugo -w # -e development hugo server # -e development Theme https://themes.gohugo.io/\nデフォルトのテーマというものが存在しないのがちょっと厳しい。 ユーザーによっていろいろ投稿されてるけどほとんどがブログ用途。 ということで非ブログ用に簡単なものを自作して使っている:\nhttps://github.com/heavywatal/hugo-theme-nonblog\nPerformance https://gohugo.io/troubleshooting/build-performance/\nページによって内容が変わらないテンプレートは partial の代わりに partialCached を使う。 ビルドするときに --templateMetrics --templateMetricsHints オプションを付けるとどのへんを変えたら良いか教えてくれる。\nContent Markdown CommonMark \u0026ldquo;Markdown\u0026quot;の正式な仕様というものが存在せず、 いくつかの方言(flavor)が乱立していたが、 現在ではこれが事実上の標準仕様となりつつある。 2017年からGFMがこれに準拠することになったのもよかった。 GitHub Flavored Markdown (GFM) CommonMarkに準拠しつついくらかの機能を追加したもの。 基本的な書き方はGitHub Helpのページが読みやすい。 Blackfriday HugoのMarkdownエンジンは長らくこれだった。 CommonMark準拠じゃないし、 リストまわりでの不具合が放置されてるし、 などなど不満が募るうちにGoldmarkに取って代わられた。 Goldmark 2019年末からHugoはこっちに移行した。 基本的にはCommonMark準拠だけど、 デフォルト設定での生HTMLコードの扱いがちょっと変。 Front matter https://gohugo.io/content-management/front-matter/\nタイトルや日付などのメタデータをファイルの先頭で記述する。 YAMLやJSONでもいいけど、 TOMLのほうが将来性ありそう。\n閲覧・公開方法 Hugo Server 付属の簡易サーバーを起動。\nhugo server open http://localhost:1313/ localhost (Mac) public/ 以下に生成されるファイルをシムリンクやコピーで /Library/WebServer/Documents/ に配置すれば http://localhost で閲覧できる。 ユーザーの ~/Sites/ をドキュメントルートにする方法でもいいが、 単にシムリンクを張るほうが楽ちん。\nls /Library/WebServer/ sudo mv /Library/WebServer/Documents /Library/WebServer/Documents.orig sudo ln -s ~/path/to/public /Library/WebServer/Documents open http://localhost/ 5秒待たされる問題 を避けるため KeepAlive Off を設定しておく。 /etc/apache2/httpd.conf を直に書き換えるか、 /etc/apache2/other/keepalive.conf のようなファイルを作り、 sudo apachectl graceful で変更を反映。\nGitHub Pages public/ 以下に生成されるファイルをしかるべきrepository/branchに置くだけ。\nSee Git\n",
  "href": "/misc/hugo.html",
  "tags": [
   "writing"
  ],
  "title": "Hugo",
  "type": "misc"
 },
 {
  "content": " https://ipython.org/ https://ipython.readthedocs.io/ 対話型実行環境 起動 ターミナルから ipython を実行するか、 普通の対話Pythonから:\nimport IPython IPython.start_ipython(argv=[]) とりあえずドキュメントを読む:\n? %quickref 文脈を考慮したタブ補完 定義済み変数などはある程度 rlcompleter でも補完できるが、 IPythonはさらに文脈を考慮して賢い補完をしてくれる:\nimport o[TAB] import os.[TAB] open(\u0026#39;~/.[TAB]\u0026#39;) 履歴 上下キーで単純に遡る 途中まで入力して control-p で前方一致する履歴のみ遡る control-r から部分一致する履歴を検索 %hist input cache: _i, _ii, _iii, _ih[n], _i\u0026lt;n\u0026gt; output cache: _, __, ___, _oh[n], _\u0026lt;n\u0026gt; directory: _dh Object introspection https://ipython.readthedocs.io/en/stable/interactive/reference.html#dynamic-object-information\n関数がどんな引数をとるか、 クラスがどんなメンバを持っているか、 などをパッと覗くことができる。 help() コマンドの強力版。\n先頭(か末尾)にクエスチョンをつけるだけ:\n?os ??os %pdoc \u0026lt;object\u0026gt;: docstring %pdef \u0026lt;object\u0026gt;: ? %psource \u0026lt;object\u0026gt;: 定義しているソースコード %pfile \u0026lt;object\u0026gt;: 定義しているファイル全体 システムコマンド実行 頭にエクスクラメーションをつけるだけ:\n!pwd files = !ls マジックコマンド https://ipython.readthedocs.io/en/stable/interactive/magics.html\n一行単位の line magic は % で始める。 複数行うけつける cell magic は %% で始める。 デフォルトの automagic = True では % が省略可能で怖い。 環境設定 https://ipython.readthedocs.io/en/stable/config/intro.html\nipython help profile ipython profile create [name] ipython profile list ipython profile locate ipython --profile=\u0026lt;name\u0026gt; ~/.ipython/profile_default/ipython_config.py\n起動時に自動的に pandas や matplotlib を読み込むとか。 automagicを切るとか。\nJupyter https://jupyter.org/\nウェブブラウザ上で動く対話的実行環境。 元はIPython Notebookだったが、 カーネルを入れ替えることで他言語を扱うことが可能になり、 汎用Jupyter Notebookとして生まれ変わった。 それをさらに開発環境として洗練させたのがJupyterLab。\nノートブック形式 .ipynb は Markdown/LaTeX記法による見出し・コメント・数式とともに ソースコードと実行結果をひとまとめに保存しておけるので、 研究ノートのような使い方に向いている。 Mathematica/Mapleの使い勝手に似ている。 GitHub上でも直接閲覧できるし、VSCode でも扱える。 ターミナルやテキストエディタに馴染みのない非プログラマ向けには便利だろう。\nファイルがインプット・アウトプット混在の複雑なテキストになるので、 コマンドラインやGit上での取り回しが悪いのは致命的な欠点。 後述のJupytextがこれを回避する救世主かもしれない。\n始め方 https://jupyterlab.readthedocs.io/en/stable/getting_started/starting.html\n適当な作業ディレクトリを作って移動: mkdir -p ~/jupyter; cd $_ ターミナルから起動: jupyter lab [file or directory] ウェブブラウザで http://localhost:8888/lab/ が立ち上がる Launcher内のNotebookにある適当なカーネル (e.g., Python 3) を選択 [ ]: の右の箱に適当なコマンド print('Hello, world!') を入れて shift-return 適当に保存してブラウザを閉じる ターミナルに戻って control-c で終了 キーボードショートカット key action esc enter command mode enter enter edit mode h show keyboard shortcuts y to code m to markdown a insert cell above b insert cell bellow dd delete selected cells ctrl-return run selected cells 出力 例えばある Pandas DataFrame df を表示したいとき、 単に df で評価するのと明示的に print(df) を実行するのとでは結果が異なる。 前者はコードセルの出力として扱われ、HTML+CSSで描画される。 後者は副作用扱いで出力番号も与えられず、普通のテキストで表示される。 テキストで出力するオプションが欲しいけど見つからない。\nnbconvert による変換時は ~/.jupyter/jupyter_nbconvert_config.py に以下のように書いておけば print() 無しでもテキスト表示できる。\nc.NbConvertBase.display_data_priority = [\u0026#39;text/plain\u0026#39;] Jupyter’s Common Configuration Approach によれば代入ではなく .prepend() でも行けそうだが、 それだとなぜか text/html が優先されてしまう。 また、いくつもある display_data_priority の中でなぜ NbConvertBase だけが効くのか、というのもよく分からない。\nJupytext https://jupytext.readthedocs.io/\n.ipynb, .py, .md などの形式を相互に変換してくれる。 例えば、手元のソースコードは .py としてバージョン管理・編集し、 配布時に .ipynb で出力といった使い方ができる。\nJupytextではソースコードを同期することが主眼なので、 knitrのようにコードセルの実行結果を含むMarkdownを書き出す機能は無い。 それは jupyter nbconvert --execute --to markdown とかでやるべき仕事っぽい。 jupytext/issues/220 で議論はある。\nFormat md (Jupytext Markdown) YAMLヘッダーにメタデータを保存する。 コードセルにオプションを付けられる。 それ以外はほぼ普通のMarkdown。 現状のVSCodeではコードセル内での補完が貧弱なのは残念だが、 リポート目的で非コードが多いならこれが扱いやすそう。 rmarkdown (R Markdown) コードセルに波括弧を付ける: {python} Rからknitrを使えば結果を含むMarkdownを出力できる。 RとPythonの橋渡しはreticulateが担う。 md:myst (MyST Markdown; Markedly Structured Text) CommonMarkに準拠しつつreStrucutredTextやSphinxの機能をサポートするリッチなMarkdown。 コードセルの中にメタデータを埋め込むのが難点。 md:pandoc (Pandoc Markdown) Pandoc divs ::: という謎要素でセルを区切るらしい。 py:light # コメント のかたまりをテキストセルとして扱う コードと隣接するコメントはコードセル内に含められる。 py:percent セルの頭に明示的に # %% を書く。 VSCode CodeLens でも認識されるが、むしろ表示が邪魔なのでオフにする。 現状のVSCodeでコードを書くにはこれが一番扱いやすいか。 py:nomarker 情報が落ちすぎて round-trip 不可。 Config ${XDG_CONFIG_HOME}/jupytext/jupytext.toml\n書籍 ",
  "href": "/python/ipython.html",
  "tags": [
   "python"
  ],
  "title": "IPython",
  "type": "python"
 },
 {
  "content": "Shortcuts https://support.apple.com/en-us/HT201236\nスクリーンショット（全画面） ⇧shift⌘command3 スクリーンショット（範囲指定） ⇧shift⌘command4 スクリーンショット（ウィンドウ選択） ⇧shift⌘command4 then space スクリーンショット（メニューから選択。動画も撮れる） ⇧shift⌘command5 Dialog (Restart/Sleep/Shut Down) ⌃control⏏︎eject ⌃control⏻power Sleep Display ⇧shift⌃control⏏︎eject ⇧shift⌃control⏻power Sleep ⌥option⌘command⏏︎eject ⌥option⌘command⏻power Restart ⌃control⌘command⏏︎eject Shut Down ⌃control⌥option⌘command⏏︎eject Startup key combinations 機種によって違うので要確認。 Safe mode: キャッシュ削除、諸々のチェックと修復など Reset NVRAM/PRAM: 画面や音の設定 Reset SMC: 電源、ファン、内蔵カメラなどハードウェア関係 Display Brightness 明るさ調整の🔅F1🔆F2キーは通常iMacやMacBookにのみ有効で、外部ディスプレイには効かない。 MonitorControl というアプリを用いることで外部ディスプレイもキーボードで調節できるようになる。\nSystem Preferences \u0026gt; Keyboard Key Repeat \u0026amp; Delay Until Repeat を最速にする Keyboard \u0026gt; スライダーを両方とも右端に 自動修正をオフにする Text \u0026gt; 右のやつ全部チェック外す ショートカットはデフォルト設定を使う Shortcuts \u0026gt; Restore Defaults (左の項目ごとに、全部) まっさらセットアップ直後に Restore Defaults を押して変わるところがあるのは謎。 US Keyboard 左手小指Aの隣、⇪caps lockを⌃controlとして使う Keyboard \u0026gt; Modifier Keys\u0026hellip; 入力言語切り替え ⌃controlspace 半角英数入力 ⌃control⇧shift; ひらがな入力 ⌃control⇧shiftj Karabiner-Elements を使うことで左右の⌘command単体押下を 英数かなとして扱えるようになる。 が、⌃controlspaceも慣れればそれほど面倒じゃなくなる。\nInput Sources Google日本語入力を使う。 brew install google-japanese-ime\nmacOS標準の日本語入力がどうしても使いにくいポイント:\n変換候補ウィンドウを開いた場合 return を2回押さないと確定されない。 \u0026ldquo;Windows-like shortcuts\u0026rdquo; をオンにすれば return1回で確定できるようになるが、 そうすると ⌃n, ⌃p, ⌃k などのEmacs/Cocoaキーバインドが崩れる。 特殊な文字・記号の入力 ウムラウト、アクセント符号 Umlaut (ä ï ü ë ö ÿ) ⌥optionu + 母音 Acute accent (á í ú é ó) ⌥optione + 母音 Grave accent (à ì ù è ò) ⌥option` + 母音 Circumflex (â î û ê ô) ⌥optioni + 母音 Tilde (ã õ ñ) ⌥optionn + 母音 Chinese Pinyin System Preferences \u0026gt; Keyboard \u0026gt; Input Sources \u0026gt; Add \u0026ldquo;ABC - Extended\u0026rdquo;\n1st tone (ā ē ī ō ū ǖ) ⌥optiona + vowel 2nd tone (á é í ó ú ǘ) ⌥optione + vowel 3rd tone (ǎ ě ǐ ǒ ǔ ǚ) ⌥optionv + vowel 4th tone (à è ì ò ù ǜ) ⌥option` + vowel u with umlaut and tone (ǖ ǘ ǚ ǜ) ⌥option` + v ⌥ option ⌥option ⌥option⇧shift 8 • bullet ° degree - – en dash — em dash = ≠ not equal ± plus minus [ “ left double quotation ” right double quotation ] ‘ left single quotation ’ right single quotation \\ « left double angle quotation » right double angle quotation \u0026lt; ≤ less than or equal to \u0026gt; ≥ greater than or equal to 絵文字・記号 ⌃control⌘commandspace でポップアップした窓から検索・入力すればキーボードから手を離さずに済む。\ne.g., beer🍺🍻, metal🤘, muscle💪, thumb👍, smile😁🤣, option ⌥, schwa ə\nEntering Unicode Text and Symbols System Preferences \u0026gt; Keyboard \u0026gt; Input Sources で Unicode Hex Input にチェック 右上メニューバーから Unicode Hex Input を選択 ⌥option を押しつつ番号を入力 (e.g. ⌥option2318 で ⌘ が入力される) Magic Trackpad 応答しなくなった場合に試すこと スイッチ off → on (触覚フィードバックの有無) System Preferences → Bluetooth (\u0026ldquo;Not Connected\u0026rdquo; or \u0026ldquo;Connection Rejected\u0026rdquo;?) 有線接続 一旦デバイスを削除して再登録。 System Preferences はキーボード操作で完結できないので、 有線マウスが無い場合はblueutilで操作。 brew install blueutil blueutil --paired blueutil --unpair 00-00-00-00-00-00 セーフモード、SMCリセット、PRAM/NVRAMリセット ",
  "href": "/mac/keyboard.html",
  "tags": [
   "mac",
   "writing"
  ],
  "title": "Keyboard",
  "type": "mac"
 },
 {
  "content": " 特殊Markdownファイル (.Rmd, .qmd) に含まれるRコードを実行し、 結果を編み込んで汎用Markdownファイル (.md) に変換する。\n一般的には Quarto や R Markdown といったパイプラインの中で利用されることが多く、 直接触れる必要性はあまりない。 私はMarkdown→HTML処理をPandocではなくHugoに任せたいので .Rmd |\u0026gt; knitr |\u0026gt; Hugo という流れで使う。\n## Heading Paragraph. ```{r, example-code-chunk} #| fig.height: 5 #| fig.width: 6 answer = 6 * 7 ggplot(mpg) + aes(displ, height) + geom_point() ``` Answer to the ultimate question is `r answer`. https://yihui.org/knitr 逆順で理解する R Markdown Presentation 2022-10 Tokyo.R #102 Options https://yihui.org/knitr/options/\nFormat chunk header {r, tag=value, tag=value} 形式 knitrではこれが基本という扱い。 値はRコードとして評価される。文字列にはquoteが必要。 改行は許されない。 {r 直後の , はあってもなくてもいい。 Yihuiは付けているが、RStudioの新規作成RmdやPosit社のcheatsheetでは付いてない。 先頭のオプションはtag無しquote無しで label 扱いされる。 明示的に label の値を設定してもよい。 chunk内 #| tag=value, tag=value 形式 Rコードも書けるし改行も許される。 改行はカンマの代わりにはならない。 chunk内 #| tag: value 形式 Quartoではこちらを推奨。 一行一項目で改行する前提。 区切り文字としてピリオドよりハイフンが好まれる。 ^(fig|out)- は \\1. に置換された上でheader形式のオプションとmergeされる。 値はYAMLの型である必要がある。 論理値は小文字: true, false 文字列はquote無しでもいいが付けておいたほうがたぶん安全。 \u0026ldquo;double\u0026rdquo; では \\n が改行扱いされるなどエスケープシーケンスが有効。 \u0026lsquo;single\u0026rsquo; ではそういうのが起こらず文字通り。 Rコードを渡すにはひと手間: #| message: !expr 'NA' 一括 knitr::opts_chunk$set(tag = value) 形式 文書内、それより後ろのchunkのデフォルトを変更する。 Chunk label 区切りにはなぜかハイフンが推奨。ピリオドやアンダースコアではなく。\nfigureやcacheの出力先に使われるのでuniqueになるように注意する。 文書内での重複はknitrがチェックして怒ってくれるが、 ディレクトリ内の文書間での重複までは見てくれない。\n省略すると unnamed-chunk-%d のような形で自動的に割り振られる。 入力ファイルによって変わるように unnamed.chunk.label オプションを変更しておいたほうが安全。\nGeneral knitr::opts_chunk$set(error = FALSE): knitr単独で使う場合、デフォルトではchunk内でエラーが生じても止まらず危険。 あえてエラーを表示したいchunkでのみ明示的に error = TRUE とするように。 knitr::opts_chunk$set(comment = \u0026quot;\u0026quot;): 出力結果の行頭に何も書かないようにする。 NA でも NULL でも同じ。 デフォルトでは ## が入ってしまう。 knitr::opts_chunk$set(message = NA): message() からの出力をknitrで捕まえずにconsoleにそのまま流す。 knitr 1.42 から message = FALSE はどこにも出力しないようになって危険。 utils::capture.output(..., type = \u0026quot;message\u0026quot;) したいときとかも。 knitr::opts_chunk$set(warning = NA): 同上。 echo: false: コードを表示しない。 echo: -1 のように数値で特定の行だけを除外したりもできる。 results: 'markup': 標準出力があるたびにコードフェンスに入れて編み込む。(デフォルト)\nresults: 'hide': 表示しない。FALSE でも。\nresults: 'asis': コードフェンスに入れず生の文字列として編み込む。\nresults: 'hold': 標準出力をchunk最後まで溜めてまとめて編み込む。\neval: false: 評価しない。 include: false: コードも結果も出力しない。実行だけする。 空行が1行だけ追加されるのを防ぐのは難しそう。 collapse: true: 実行結果をコードと同じブロックに入れる。 ref.label: ほかのchunkをソースコードとして読み込む。 順番は関係なく、後ろのコードを前で参照することもできる。 複数のchunkに同じlabelをつけて1つだけeval=TRUEにする という手もある。 \u0026lt;\u0026lt;label\u0026gt;\u0026gt; で埋め込むのはちょっとお行儀が悪い気がするので使わない。 opts.label: ほかのchunkのオプションを継承する。 External file: 'setup.R': 外部ファイルをchunkのソースコードとして読み込む。 code: expr! 'readLines(\u0026quot;setup.R\u0026quot;)' も同様。 {r} 以外のengineも指定できる。 {embed} なら拡張子を考慮しつつコードフェンスに入れて埋め込み。 {asis} なら文字通りそのまま。 {cat, engine.opts = list(file = \u0026quot;hello.py\u0026quot;)}: chunkの内容をファイルに書き出す。 そのままでは表示されず echo も効かない。 lang を追加設定することで言語付きコードフェンスになる。 Cache knitr::opts_chunk$set(cache = TRUE): 実行結果を保存しておき、chunk内容に変更が無ければ再利用する。 乱数生成やchunk間の依存関係などによって危険性が跳ね上がる諸刃の剣。 invalidationにはchunkコードとオプションのMD5 digest値が使われる。 knitr::opts_chunk$set(autodep = TRUE): グローバル変数の依存関係を自動で解決。 knitr::opts_chunk$set(cache.path = glue(\u0026quot;.cache/{inputname}/\u0026quot;)): 名前からの予想に反してpathそのものではなくprefix。 相対パスの基準はchunk内の getwd() ではなく knit() を呼び出す側の getwd() (i.e., 推奨に従えばinputではなくoutput側のディレクトリ)。 デフォルトの cache/ ではhugoに拾われたりして邪魔なので、 .cache/ に変えるとか絶対パスで別の場所を指定するとか。 複数の文書を同一ディレクトリで扱う場合は入力ファイル名を含めたほうがいい。 chunk labelの重複を自力で避けたとしても autodep の __global などが衝突してしまうので。 cache.vars: 保持する変数を明示的に制限する。 cache.globals: そのchunkで作られないグローバル変数を明示的に指定して autodep を助ける。 cache.rebuild: !expr '!file.exists(\u0026quot;some-file\u0026quot;)': 強制的にchunkを評価してcacheを作り直す。 外部ファイルの存在などに依存させたいとき便利。 cache.extra: invalidationの依存関係を追加する。 tools::md5sum(\u0026quot;some-file\u0026quot;) とか file.mtime(\u0026quot;some-file\u0026quot;) とかで外部ファイルの内容や変更時間を参照できる。 そもそもすべてのchunkオプションがinvalidation評価対象なので、 tagが cache.extra である必要はない。 それゆえにknitrコードにもドキュメントにもちゃんと書かれていなくてわかりにくい。 yihui/knitr#405 dependson: 依存するchunkをlabelや数値で指定。 Figure fig.path: cache.path と同様の挙動。 fig.width: 10 fig.height: 5 fig.show: hold fig.show: animate animation.hook: gifski https://github.com/r-rust/gifski interval: 0.25 dpi: デフォルトは72。 fig.retina: dpi に掛け算、 out.width に割り算をして 見かけサイズ据え置きで解像度を変更する。 \u0026lt;img\u0026gt; タグに width attribute が追加されることになる。 knitr::opts_chunk$set(fig.process = wtl::oxipng): PNGを圧縮したり WebPに変換したりできる。 knitr::opts_chunk$set(dev = \u0026quot;ragg_png\u0026quot;): 日本語ラベルが文字化けしない、描画が速い、などと言われる ragg を使う場合。 XQuartzのPNGに比べるとなんとなく文字がガチャガチャで美しくない気がするので、 日本語を使うchunkでだけ設定するほうがいいかも。 Package Options knitr::opts_knit https://yihui.org/knitr/options/#package-options\nknitrロード前に options(knitr.package.verbose = TRUE) とするか、 knit() 実行前に knitr::opts_knit$set(verbose = TRUE) とするか。 chunk内からの実行では遅い。 knitr::opts_chunk とは別であることに注意。\nverbose = TRUE: 帯に短し襷に長し。 progress = TRUE のときはchunkの内容をいちいち表示するので量が多すぎ、 progress = FALSE のときはキャッシュを使った場合にその旨が表示されるだけで情報不足。 unnamed.chunk.label = glue(\u0026quot;_{inputname}\u0026quot;): 複数の文書を同一ディレクトリで扱う場合を考えるとデフォルトの unnamed-chunk ではやや不安。 root.dir: コード評価のディレクトリ。デフォルト(inputファイルと同じ)を維持。 base.dir: plotの出力先を絶対パスで指定したいことがもしあれば。 chunk optionではないことに注意。 Global R Options https://yihui.org/knitr/options/#global-r-options\nなぜ knitr::opts_knit とまた違うくくりがあるのかは謎。歴史的経緯？\nknitr.progress.fun: デフォルトではchunk名の確認さえままならないほどコンパクトなので、 例を参考にして適当に差し替える。 Functions knitr::knit(input, output = NULL, ...) input: 呼び出し元とは違うディレクトリにあるファイルを指定しても、 chunk内の getwd() はこのファイルが基準となる。 output: 可能な限り NULL のままにしておく。 input とは違うディレクトリに書き出したい場合、 出力先に予め setwd() しておくことが強く推奨されている。 (個人的には作業ディレクトリをinput側に統一するほうが簡単そうに思えるけど。) Dangerous: knit(\u0026quot;report.Rmd\u0026quot;, \u0026quot;outdir/report.md\u0026quot;) Redundant: setwd(\u0026quot;outdir\u0026quot;); knit(\u0026quot;../report.Rmd\u0026quot;, \u0026quot;report.md\u0026quot;) Good: setwd(\u0026quot;outdir\u0026quot;); knit(\u0026quot;../report.Rmd\u0026quot;) knitr::fig_chunk(label, ext, number, ...) chunk名から図のパスを取得。 離れたところで図を使い回せる。 knitr::current_input() 処理中の入力ファイル名を取得。 knitr::knit_exit() 文書の途中で終了。 Hooks https://yihui.org/knitr/hooks/ https://bookdown.org/yihui/rmarkdown-cookbook/chunk-hooks.html Chunk hooks chunk前後に評価するコードをsetできる。 setしたオプションが NULL でなければ(FALSE でも)評価される。 cacheによってchunk本体が評価されない場合は評価されない。 オプションの名前は output hooks と被らなければ何でも。 cache invalidation より後なのでここで options の値を変更しても遅い。 この関数が返す文字列はasisで埋め込まれる。 getwd() は knit() 実行元と同じであり、 chunk内と同じとは限らないことに注意。 knitr::knit_hooks$set(foo = \\(before, options, envir) { if (before) { # evaluated before chunk code options$cache.rebuild = TRUE # too late! } else { # evaluated after chunk code } }) Output hooks コード評価後、コードや実行結果の文字列をいじる。 source, output, warning, message, error, plot, inline, chunk, document. getwd() は場合によって異なる謎仕様なので注意。 Rの場合は knit() 実行元と同じ、 engineを変えた場合はchunkと同じ、、、？ knitr::knit_hooks$set(source = \\(x, options) { message(\u0026#34;hook source \u0026#34;, getwd()) paste0(\u0026#34;src: \u0026#34;, x) }) Option hooks options を受け取り、変更し、返す関数をsetしておく。 setしたオプションの値が NULL でさえなければ評価される。 つまり、元々値を持つ echo などにsetすると毎回評価される。 cache invalidation前に評価される唯一のhookか。 getwd() は knit() 実行元と同じであり、 chunk内と同じとは限らないことに注意。 knitr::opts_hooks$set(fig.square = \\(options) { stopifnot(is.numeric(fig.square)) options$fig.width = options$fig.square options$fig.height = options$fig.square options }) Quarto https://quarto.org/docs/output-formats/hugo.html\n",
  "href": "/rstats/knitr.html",
  "tags": [
   "r",
   "tidyverse"
  ],
  "title": "knitr",
  "type": "rstats"
 },
 {
  "content": "基本操作 TeX Live をMacにインストール とにかく動く環境が欲しい人は、素直にフルのMacTeXをインストールするだけ。 ただし2GB以上の大きなインストーラをダウンロードする必要があるので注意。 BibDesk, LaTeXiT, TeX Live Utility, TeXShop などのGUIアプリが不要で、 必要なパッケージをコマンドラインからインストールできる人は下記の手順で小さくインストールできる。\nhttps://www.tug.org/mactex/ から BasicTeX.pkg を入手してインストール。 あるいは brew install --cask basictex\n/Library/TeX/texbin/ にパスを通す。 基本的には /etc/paths.d/TeX 越しに自動的に設定されるはず。\n今後の tlmgr 操作で管理者権限を使わなくて済むようにパーミッション設定: sudo chown -R $(whoami):admin /usr/local/texlive/\ntlmgr update --self --all で諸々アップデート。\nTeX Liveの新パージョンが年1回リリースされるので、そのときはインストールからやり直す。 (tlmgr update では更新できない)\nTeXをPDFにコンパイル ソースコードを書く\n\\documentclass[a4paper]{article} \\begin{document} Hello, World! \\end{document} ターミナルからコンパイル:\npdflatex hello.tex open hello.pdf MacTeXに含まれるTeXShopというアプリを使えば、 原稿書きからコンパイルまで面倒を見てもらえるので、 ターミナルにコマンドを打ち込む必要はないらしい。\nlatexmk 引用文献や図表への参照などを入れたりすると pdflatex を一度実行するだけではPDFが完成せず、 コマンドを何度か繰り返し実行しなければならなくなる。 latexmk はそのへんをうまくお世話してくれる。\n対象の .tex ファイルを明示的に指定してもいいし、 省略してディレクトリ内のすべてのファイルを対象にしてもいい。\nlatexmk -h # print help latexmk # generate document latexmk -pv # preview document after generation latexmk -pvc # preview document and continuously update latexmk -C # clean up 設定を ~/.latexmkrc とかに書いておける:\n$pdf_mode = 1; $pdflatex = \u0026#39;pdflatex -file-line-error -halt-on-error -synctex=1 %O %S\u0026#39;; $lualatex = \u0026#39;lualatex -file-line-error -halt-on-error -synctex=1 %O %S\u0026#39;; https://www.ctan.org/pkg/latexmk\nSyncTeX ソースコードとPDFの対応箇所を行き来するための仕組み。\nSkim to source: shift-cmd-click VSCode to PDF: alt-cmd-j tlmgr でパッケージ管理 BasicTeXの場合は最小限のパッケージしか付いてこないので、必要なものを別途インストールする。 \u0026ldquo;By default, installing a package ensures that all dependencies of this package are fulfilled\u0026rdquo; と言っていて実際tlmgr自体はそういう作りになっているが、 多くのパッケージで依存関係がちゃんと記述されていないので、 実際にはエラーを読みながら依存パッケージを手動でインストールすることになる。\ntlmgr update --self --all tlmgr search --global japanese tlmgr search --global --file zxxrl7z tlmgr info --list newtx tlmgr install chktex latexmk 管理者権限不要の --usermode も用意されているが、 なんかうまくいかないので必要に迫られない限り使わないほうがよい。\nパッケージをアンインストールしようと思って tlmgr uninstall some-package などとするとTeX Live全体が消えてしまうので注意。 正しくは tlmgr remove some-package\n基本要素 数式 とりあえず Short Math Guide for LaTeX (PDF)を読むべし。\n基本的に {amsmath} を使う。 数式環境のデファクトスタンダード。 アメリカ数学会(AMS)が開発したらしいが、 ガチ数学じゃなくても数式を書く場合はこれらしい。\n\\usepackage{amssymb,amsmath} \\usepackage[all,warning]{onlyamsmath} Inline math:\nif $N_e u \\ll 1$, then the population is monomorphic most of the time, $ ... $ はTeXの古いやり方で、 新しいLaTeXでは \\( ... \\) を用いるべし、という流れもある。 けどダラーのほうが書きやすいし読みやすいので、 しばらくは chktex -n46 で様子を見る。\nDisplay math:\n\\begin{equation*}\\label{eq:growth} N_t = N_0 e^{rt} \\end{equation*} 生TeXの $$ ... $$ を使ってはいけない。 \\[ ... \\] はOK。\n改行したり、等号で揃えたり\n\\begin{equation}\\label{eq:growth} \\begin{split} N_t \u0026amp;= N_0 e^{rt} \\\\ \u0026amp;= N_0 \\lambda^t \\end{split} \\end{equation} \\begin{align} S \u0026amp;= 4 \\pi r^2\\label{surface}\\\\ V \u0026amp;= \\frac {4 \\pi r^3} 3\\label{volume}\\\\ \\end{align} {split} 環境は {equation} 環境内部で1つの数式を複数行に分ける。 数式番号は1つのまま。 \u0026amp; を1つまでしか使えない。 {aligned} 環境なら \u0026amp; を複数使えるが上位互換というわけでもないらしい。\n{align} は独立した数式環境を作り、行ごとに数式番号を生成する。 {align*} としたり行末に \\notag を置くことで数式番号を抑制できる。\n古い {eqnarray} 環境でも似たようなことはできるが問題があるらしく非推奨。\n場合分け\n\\begin{equation}\\label{eq:heaviside} H(x) = \\begin{cases} \u0026amp; 0 \\text{if $x \\le 0$} \\\\ \u0026amp; 1 \\text{if $x \u0026gt; 0$} \\end{cases} \\end{equation} 記号: https://www.ctan.org/tex-archive/info/symbols/comprehensive/ に網羅されてるけど、 だいたい Short Math Guide for LaTeX にまとめられてるやつで足りるはず。\nカッコの大きさを変えたいときは \\left( と \\right) を使っておけば、 前後のサイズに応じて自動的にうまいことやってくれる。 e.g., 分数を挟むとか、カッコの入れ子とか \u0026ldquo;given that\u0026rdquo; を示す縦棒はパイプ記号 | ではなく \\mid を使うのが正しいし適度なスペースが入って読みやすい。 絶対値もパイプではなく \\lvert x \\rvert のようにする。 斜体にしたくない文字を普通にするには \\mathrm dt 。 記号じゃないテキストには \\text{otherwise} 。 よく使われるやつは定義済み e.g., \\log, \\exp 図 \\usepackage[final]{graphicx} %%% \\begin{figure} \\includegraphics[width=\\columnwidth]{awesome.pdf} \\caption{some description}% \\label{fig:awesome} \\end{figure} \\begin{figure}[htbp] のようにして配置場所の優先順位を指定できるが、 結局はいろんな兼ね合いでコンパイラが決める。 デフォルトは [tbp]。 複数指定した場合、どの順序で書いても下記の順に優先される。\nhere: ソースと同じ位置に top: 上部に bottom: 下部に float page: 図だけを含む独立したページに 幅の指定には \\textwidth, \\columnwidth, \\linewidth などの変数が使える。 twocolumnの文書内で \\textwidth まで広げたい場合は アスタリスク付きの \\begin{figure*} 環境を用いる。\nラベル \\label{} は \\caption{} 直後(または内部)じゃないとおかしくなるらしい。 間に改行やスペースが入るのもダメっぽいので \\caption{} 行末に % を置いておく。 本文からは \\ref{fig:awesome} のようにラベルで参照しておけば番号に置き換えられる。\nキャプションをカスタマイズするには {caption} の \\captionsetup{...} を用いる。\nひとつの領域に複数の図を貼るには {subfig} の \\subfloat[caption]{filename} を用いる。\n{epstopdf} でEPSを取り込もうとすると(e.g., PLOS)、 ファイルもパッケージも揃ってるはずなのに ! Package pdftex.def Error: File '*-eps-converted-to.pdf' not found. というエラーが出る。 変換プログラム本体である ghostscript をHomebrewか何かで入れる必要がある。\nGIFアニメをそのまま埋め込むことはできないので、 {animate}で連番PNGを読み込む。\n% tlmgr install animate media9 ocgx2 \\usepackage[autoplay,final,controls=all,type=png]{animate} \\begin{document} % \\animategraphics[options]{frame rate}{file basename}{first}{last} \\animategraphics[scale=0.5]{10}{dir/basename_}{1}{9} \\end{document} 表 https://en.wikibooks.org/wiki/LaTeX/Tables\nキャプションやラベルなどをまとめるのが table 環境、 表本体が tabular 環境。\n\\begin{table} \\caption{Parameters}% \\label{table:parameters} \\begin{tabular}{rl} \\toprule Symbol \u0026amp; Description \\\\ \\midrule $\\mu$ \u0026amp; mutation rate per division \\\\ $N_0$ \u0026amp; initial population size \\\\ \\bottomrule \\end{tabular} \\end{table} 罫線は標準の \\hline だと上下スペースが狭苦しいので、 {booktabs} の toprule, midrule, bottomrule を使う。\nページに合わせて幅をうまいことやるには {tabulary} 。\n複数ページにまたがる表を作るには標準 {longtable} やその派生。\n箇条書き {enumitem} を使うといろいろなオプションが設定可能になる。\n\\usepackage{enumitem} %%% \\begin{itemize} \\item Judas Priest \\item Iron Maiden \\end{itemize} \\begin{enumerate}[nosep,leftmargin=*] \\item Judas Priest \\item Iron Maiden \\end{enumerate} \\begin{description}[nextline] \\item[key1] value1 \\item[key2] value2 \\end{description} 引用 https://en.wikibooks.org/wiki/LaTeX/More_Bibliographies\nBibdesk などの文献管理アプリでbibtex形式の文献リストを作る。 e.g., mybibdata.bib\nプリアンブルで {natbib} を呼び出し、スタイルを指定。\n\\usepackage[authoryear,round]{natbib} \\bibliographystyle{abbrvnat} スタイルとして {natbib} の abbrvnat や unsrtnat をそのまま使うことはまれで、 各Journalの提供する、あるいは有志の作る .bst ファイルをダウンロードして使う。\nlatex makebst コマンドから質問に答えて新規作成することも可能。 後で間違いに気付いた場合、イチからやり直すより中間ファイルの .dbj ファイルを編集して latex *.dbj で .bst を生成すると早い。 .bst そのものを読み解いて編集することも不可能ではない。\nLaTeX本文にcite keyを挿入。標準の \\cite は使わない。\ncomand authoryear,round numbers \\citet{hudson1987g} Hudson et al. (1987) Hudson et al. [42] \\citep{hudson1987g} (Hudson et al., 1987) [42] \\citealt{hudson1987g} Hudson et al. 1987 Hudson et al. 42 \\citealp{hudson1987g} Hudson et al., 1987 42 \\citeauthor{hudson1987g} Hudson et al. Hudson et al. \\citeyear{hudson1987g} 1987 1987 \\citenum{hudson1987g} 42 42 \\citep[eq. 5]{hudson1987g} (Hudson et al., 1987, eq. 5) [42, eq. 5] \\citep[see][]{hudson1987g} (see Hudson et al., 1987) [see 42] \\citet*{hudson1987g} Hudson, Kreitman, and Aguadé (1987) Hudson, Kreitman, and Aguadé [42] \\citep*{hudson1987g} (Hudson, Kreitman, and Aguadé, 1987) [42] デフォルトは authoryear,round,semicolon だが \\bibliographystyle{} 次第で括弧が勝手に square になったりする。\n\u0026ldquo;et al., 1987\u0026rdquo; の間のカンマを取りたい、といった微調整は \\bibpunct{(}{)}{;}{a}{}{,} のようにする。\n最後の方に文献リストを挿入:\n\\bibliography{mybibdata} 元の .tex をコンパイルして .aux を生成\nbibtex に .aux を渡して .bbl を生成\n再び .tex をコンパイルすると .bbl を踏まえて .aux が更新される。 (このときPDF出力すると、文献リストはできるけど引用部分はハテナ?になる)\nさらにもう1回コンパイルして完成\n最初の2回は pdflatex -draftmode としてPDF出力を省略すると早い。 適切な Makefile を書いて自動化すると楽で、 latexmk を使うともっと楽ちん。\n文字の修飾 https://en.wikibooks.org/wiki/LaTeX/Fonts#Font_styles https://en.wikibooks.org/wiki/LaTeX/Colors \\emph{emphasis} \\textit{italic} \\textbf{bold} \\texttt{monospace} {\\huge huge text} \\tiny, \\scriptsize, \\footnotesize, \\small, \\normalsize, \\large, \\Large, \\LARGE, \\huge, \\Huge\n\\usepackage[normalem]{ulem} % \\uline{}, \\sout{} \\usepackage{color} % \\textcolor{} \\usepackage{soul} % \\hl{} using {color} %%% \\uline{underlined text} \\sout{strikethrough} \\textcolor{red}{colored text} {\\text{red} colored text} \\hl{highlighted text} {ulem} は [normalem] オプションを付けて読まないと \\emph が下線に変更されてしまうので注意。\n{soul} のドキュメントによれば \\hl{環境} に $数式$ を入れられるはずだが \u0026ldquo;Extra }, or forgotten $\u0026rdquo; というエラーで弾かれる。\n行番号 共著者や査読者との議論をスムーズにするため各行に番号を振る。 {amsmath} の fleqn オプションを使うと行番号が消えるとか、 twocolumn のときに \\pagewiselinenumbers{} がページワイズにならないとか、 いろいろ不具合はあるものの {lineno} を使うしかなさそう。\n\\usepackage[mathlines,pagewise,switch]{lineno} \\renewcommand\\linenumberfont{\\normalfont\\scriptsize\\sffamily\\color[gray]{0.5}}% \\setlength\\linenumbersep{4truemm} \\linenumbers{} Tips ダメな使い方を警告してもらう いろんなパッケージを使ったいろんな書き方がネット上にあふれているが、 中には古すぎたりするため避けたほうがよいものもある。 ファイルの先頭で nag を読み込むことで、 コンパイル時にそういうのを警告してもらえる。\n\\RequirePackage[l2tabu,orthodox]{nag} \\documentclass[a4paper]{article} % これよりも前 chktex コマンドを使えばコンパイルよりも手軽にチェックできる。 VSCode などのリッチなエディタなら、 自動で実行してコード付近に警告を出してくれる。\nligature問題 表示の美しさという点でリガチャは素晴らしいけど、 PDF内の検索やPDFからのコピペ時に問題が発生する。 例えば fi が合字になるため definition が検索でひっかからない。 definition をコピペすると de nition になってしまう。\n次のコードをプリアンブルの頭の方に記述するといいらしいが、うまく機能しない。。。\n\\input{glyphtounicode.tex} \\pdfgentounicode=1 % あるいは \\usepackage{mmap} 日本語を使う LuaLaTeX OS上にあるOTFフォントがそのまま使える pdfTeXの後継として、今後のスタンダードと目される {luatexja} が精力的に開発されている 動作が遅い XeLaTeX OS上にあるOTFフォントがそのまま使える とにかく日本語入りでコンパイルできればいい、というのであればこれが早い 日本語に特化したツールは開発されていないので細かい制御がイマイチらしい upLaTeX 日本語を使えるように LaTeX を改良したもの 歴史が長いので日本語組版のための便利な道具が揃ってるらしいけど未来は無さそう フォント Computer Modern Knuth先生が作ったデフォルトフォント。 {lmodern} \u0026mdash; Latin Modern Computer Modern の改良版。 {times} ローマンとサンセリフにそれぞれ Times と Helvetica を割り当てる。 数式は Computer Modern のまま。 {txfonts} {times} の改良版？ 数式も Times にする。 直接は使わない。 {newtx} {txfonts} の後継で現役。 本文と数式を別々に指定できる。 \\usepackage[libertine]{newtxmath} とすると Libertine を数式に使える。 インストールするときは newtx だけでなく txfonts と boondox も入れないと Unable to find TFM file と怒られる。 {newpx} {newtx}と同等の機能を美しいPalatinoで。 {palatino}, {pxfonts}, {newtx}, {tex-gyre-pagella}, {tex-gyre-math-pagella} も入れておく。 TeX Gyre Pagella はOpenType志向のPalatinoクローン。 {libertinus} 美しいLinux Libertineの後継プロジェクト。 type1もOTFも数式もサポートしていて便利だがひと回り小さいことに注意。 使うときは \\usepackage{libertinus} でよしなにやってくれるらしいが 依存パッケージのインストールは例によって手動: libertibnus libertinus-fonts libertinus-type1 libertinust1math libertinus-otf LuaTeX/XeTeXならOSのフォントをフルネームで指定して使えるが、 共同執筆とかを考えるとTeX Liveパッケージやプリセットに含まれるものを使うのが安全。\nTeX Liveから入れたフォントをOSに認識させるにはシムリンクを張るだけ: ln -s /Library/TeX/Root/texmf-dist/fonts/opentype ~/Library/Fonts/texlive-opentype\n\\usepackage{amssymb,amsmath} % must be called ahead of mathspec \\usepackage[all,warning]{onlyamsmath} \\usepackage{iftex} \\iftutex \\usepackage[math-style=TeX,bold-style=TeX]{unicode-math} \\usepackage[no-math]{fontspec} \\setmainfont{TeX Gyre Pagella} \\setmathfont{TeX Gyre Pagella Math} \\setsansfont{TeX Gyre Heros} % \\usepackage{luatexja} % \\usepackage[hiragino-pron,scale=0.92,deluxe,jis2004,match,nfssonly]{luatexja-preset} \\else \\usepackage[T1]{fontenc} \\usepackage[utf8]{inputenc} \\usepackage{newpxtext} \\usepackage{newpxmath} \\usepackage{textcomp} % \\usepackage[uplatex,deluxe,jis2004]{otf} \\fi フォント関連をいじったあと明示的にマップを更新するには updmap-sys\n",
  "href": "/misc/latex.html",
  "tags": [
   "writing"
  ],
  "title": "LaTeX",
  "type": "misc"
 },
 {
  "content": "簡単・便利 open Finder でダブルクリックするのと同じように、 Terminal から一発でファイルを開くことができる。\n# 関連付けられたデフォルトのアプリケーションで開く open hudson1992g.pdf # アプリケーションを指定して開く open -a Skim hudson1992g.pdf # カレントディレクトリをFinderで開く open . pbcopy, pbpaste ターミナルからクリップボード(pasteboard)を操作する。\npwd | pbcopy pbpaste | wc say 音声読み上げ say hello world pbpaste | say 声は環境設定の Accessibility \u0026gt; Speech で変更可能。\nカーソル行あるいは選択中のテキストを読み上げるショートカットも設定できる。 optionesc\nkillall Finder や Dock など、GUIから終了させにくいアプリケーションを再起動する:\nkillall Finder killall Dock killall Kotoeri 動作が不安定になったとき、設定変更を反映させたいとき、メモリを開放したいときなどに。\n設定関連 defaults ~/Library/Preferences/ 以下にある各種設定ファイル.plistを編集する。 true に設定した項目を元に戻すには、 項目自体を delete するか、false に設定する。\ndefaults [write/delete] DOMAIN KEY -TYPE VALUE # Finderのタイトルバーにフルパスを表示 defaults write com.apple.finder _FXShowPosixPathInTitle -boolean true # Launchpadの並び順をリセット defaults write com.apple.dock ResetLaunchPad -bool true; killall Dock # 使わないXcodeのための重いindexingを切っておく defaults write com.apple.dt.Xcode IDEIndexDisable 1 # Quicklook上でコピペできるようにする defaults write com.apple.finder QLEnableTextSelection -bool true # 特定アプリをダークモードから除外する # osascript -e \u0026#39;id of app \u0026#34;Google Chrome\u0026#34;\u0026#39; defaults write com.google.Chrome NSRequiresAquaSystemAppearance -bool true # スクリーンショットの保存先を変更 defaults write com.apple.screencapture location \u0026#34;~/Pictures/Screenshot\u0026#34; # 影なしで撮る defaults write com.apple.screencapture disable-shadow -bool true # 撮影後のサムネイル表示(=遅延)を無効にする defaults write com.apple.screencapture show-thumbnail -bool false OnyX や TinkerTool などのGUIアプリを使うほうが簡単で安心かも\nlaunchctl サービスの起動、終了\nMacBook Pro の Touch Bar は完全なる失敗作であり欠陥品である。 目視を要求してtouch typingを阻害するばかりでなく、 意図せぬ接触による誤動作を誘発する。 例えば、再生ボタンをうっかり触るとMusic.appが起動してしまう。 次のコマンドでそれを防げる:\nlaunchctl unload -w /System/Library/LaunchAgents/com.apple.rcd.plist ただし副作用としてそれ以外の用途でも再生ボタンを使えなくなる。 Music.appの起動だけを防ぐオプションは存在しないのかな？\nlsregister Open with で表示されるアプリケーションが重複しまくったときなど、 ファイルとアプリケーションの関連付けに関する古い情報を消して再構築。 まっさらに戻るわけではない。:\n/System/Library/Frameworks/CoreServices.framework/Frameworks/LaunchServices.framework/Support/lsregister -kill -r -domain local -domain system -domain user 設定ファイルは ~/Library/Preferences/com.apple.LaunchServices.plist\nmds, mdworker Spotlightの設定やインデックス作成に関わる。 外付けのボリュームを除外したい場合、 GUIから環境設定してもejectするたびに戻ってしまうので、 下記のように mdutil を使う必要がある:\n# インデックス情報を表示 sudo mdutil -s /Volumes/Macintosh\\ HD # インデックスサービスを切る sudo mdutil -i off /Volumes/Macintosh\\ HD # インデックスを作る・更新する sudo mdiutil -p /Volumes/Macintosh\\ HD # インデックスを一旦削除して作り直し sudo mdutil -E /Volumes/Macintosh\\ HD インストールなど .dmg のマウント、.pkg からのインストール、 システムのソフトウェア・アップデートなどを ssh 越しにやらねばならぬときもある:\nhdiutil mount SomeDiskImage.dmg sudo installer -pkg SomePackage.pkg -target / softwareupdate -l softwareupdate -i -a ",
  "href": "/mac/command.html",
  "tags": [
   "mac"
  ],
  "title": "Mac固有コマンド",
  "type": "mac"
 },
 {
  "content": "https://www.gnu.org/software/make/manual/make.html\nあらかじめコンパイルの命令を Makefile に書いておくことで、 ターミナルに長いコマンドを何度も打ち込むのを避けられる。 クロスプラットフォームで依存関係を解決しつつビルドするような Makefile を自分で書くのは難しいので、 CMake や autotools を使って自動生成する。\nMakefile C++ソースコードと同じディレクトリに入れるだけでとりあえず使える例:\n## Options PROGRAM := a.out CXX := clang++ CC := clang CXXFLAGS := -Wall -Wextra -O3 -std=c++14 CPPFLAGS := -I/usr/local/include -I${HOME}/local/include LDFLAGS := -L/usr/local/lib -L${HOME}/local/lib #LDLIBS := -lboost_program_options TARGET_ARCH := -march=native ## Dependencies SRCS := $(wildcard *.cpp) OBJS := $(SRCS:.cpp=.o) -include Dependfile Dependfile: ${CXX} -MM ${CPPFLAGS} ${CXXFLAGS} ${TARGET_ARCH} ${SRCS} \u0026gt; Dependfile ## Targets .DEFAULT_GOAL := all .PHONY: all clean all: ${PROGRAM} @: ${PROGRAM}: ${OBJS} ${LINK.cpp} ${OUTPUT_OPTION} $^ ${LDLIBS} clean: ${RM} ${OBJS} ${PROGRAM} Rule https://www.gnu.org/software/make/manual/make.html#Rules\nコロンとタブを使って以下のような形式でルールを書くのが基本。 このMakefileがあるところでターミナルから make TARGET と打つと、 ターゲットよりもソースファイルが新しい場合にコマンドが実行される。\nTARGET : SOURCE1 SOURCE2 COMMAND a.out : main.cpp sub.cpp g++ -O2 main.cpp sub.cpp 下記のようなパターンルールが予め定義されている。 (see Pattern Rule)\n%.o : %.c $(CC) $(CPPFLAGS) $(CFLAGS) -c $\u0026lt; -o $@ %.o : %.cpp $(CXX) $(CPPFLAGS) $(CXXFLAGS) -c $\u0026lt; -o $@ 以下に紹介するように、ほかにも様々な変数や関数が用意されていて、 個別のファイル名などをいちいち入力しなくても済むようになっている。\nファイル名が明示的に書かれずルールのみで生成された中間ファイルは自動的に削除される。 .PRECIOUS ターゲットにその名を加えておくとそれを防げる。 逆に、名前は出すけど中間ファイルとして扱いたい場合は .SECONDARY ターゲットに加える。\nImplicit Variables https://www.gnu.org/software/make/manual/make.html#Implicit-Variables https://www.gnu.org/software/make/manual/make.html#Name-Index CC Cコンパイラ cc CXX C++コンパイラ g++ COMPILE.cpp $(CXX) $(CXXFLAGS) $(CPPFLAGS) $(TARGET_ARCH) -c LINK.cpp $(CXX) $(CXXFLAGS) $(CPPFLAGS) $(LDFLAGS) $(TARGET_ARCH) LINK.o $(CC) $(LDFLAGS) $(TARGET_ARCH) OUTPUT_OPTION -o $@ RM rm -f CPPFLAGS プリプロセッサ用オプション。 e.g., -DNDEBUG -I${HOME}/local/include CXXFLAGS C++コンパイラ用オプション。 e.g., -Wall -Wextra -O3 -std=c++14 LDFLAGS ライブラリパスを指定する。 e.g., -L/usr/local/lib -L{HOME}/local/lib LDLIBS リンクするライブラリを指定する。 昔はLOADLIBESも同じ機能だったが非推奨。 e.g., -lboost_program_options -lz TARGET_ARCH マシン依存なオプションを指定する。 e.g., -march=native -m64 -msse -msse2 -msse3 -mfpmath=sse Automatic Variables https://www.gnu.org/software/make/manual/make.html#Automatic-Variables\n$@ ターゲット $\u0026lt; 必須項目の先頭 $^ 必須項目のスペース区切り 重複してても削らずそのまま欲しい場合は $+ 新しく更新があったファイルだけ欲しい場合は $? Functions https://www.gnu.org/software/make/manual/make.html#Functions\n文字列関連 $(subst FROM,TO,TEXT) $(findstring FIND,IN) $(filter PATTERN...,TEXT) ファイル名 $(dir NAMES...) $(notdir NAMES...) $(basename NAMES...) $(addprefix PREFIX,NAMES...) $(wildcard PATTERN) $(abspath NAMES...) 条件分岐 $(if CONDITION,THEN,ELSE) 関数じゃない条件分岐 (ifeq, ifneq, ifdef, ifndef, else, endif) もある。\nその他 $(foreach VAR,LIST,TEXT): LIST の中身をそれぞれ VAR に入れて TEXT を実行 $(file op FILENAME,TEXT): text の結果をファイルに書き出す $(call VARIABLE,PARAMS...): $(1) $(2) などを使って定義しておいた VARIABLE を関数のように呼び出す $(origin VARIABLE): 変数がどう定義されたかを知れる $(error TEXT...), $(warning TEXT...), $(info TEXT...): エラーや警告をプリントする $(shell COMMAND...): シェルを呼び出す Targets https://www.gnu.org/software/make/manual/make.html#Standard-Targets\nall ディレクトリ内のcppソースをコンパイル clean コンパイル済みオブジェクトを一掃 _ v3.81以降であれば .DEFAULT_GOAL が効くので make all と同じ make clean make Options https://www.gnu.org/software/make/manual/make.html#Options-Summary\n-f file Makefile じゃない名前のファイルを指定したければ -j jobs 並列コンパイル。コア数+1くらいがちょうどいいらしい -C directory そのディレクトリに行って make -p 自動的に作られるものも含めてすべての変数を表示 ",
  "href": "/dev/make.html",
  "tags": [
   "package",
   "c++"
  ],
  "title": "make",
  "type": "dev"
 },
 {
  "content": "matplotlib はPythonにおけるデータ可視化のデファクトスタンダード。 基本的には何でもできるけど、基本的な機能しか提供していないので、 いくらかの便利機能を seaborn で補う。\n基本 https://matplotlib.org/stable/tutorials/introductory/usage.html\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns iris = sns.load_dataset(\u0026#39;iris\u0026#39;) # Create an empty Figure fig = plt.figure() # Add an Axes to this fig ax = fig.subplots() # Plot on this ax ax.scatter(\u0026#39;sepal_width\u0026#39;, \u0026#39;sepal_length\u0026#39;, data=iris) # Show figure (in Jupyter, Hydrogen, or other inline IPython environments) display(fig) # Show figure in a new window (with non-inline backends) fig.show() plt.close(fig) # Write to a file fig.savefig(\u0026#39;example.png\u0026#39;) 渡すデータは生のlistとかではなくtidyな pandas.DataFrame型にしておく。\nfrom matplotlib.pylab import * は単にMATLABっぽいインターフェイスにするための乱暴な手段で、 例としてよく見かけるけど公式に非推奨とされている。\nFigureやAxesを意識せず plt.plot() などを使うスタイルは分かりにくいので不採用。\npyplot https://matplotlib.org/stable/api/pyplot_summary.html\n最上位のモジュール。 Figure, Axesインスタンスを明示的に操作するスタイルでは、 最初にFigureを作るくらいしか出番が無いはず。\nplt.figure(num=None, figsize=None, dpi=None, facecolor=None, edgecolor=None, ...) Backendなどを設定してFigureインスタンスを作る plt.close(*args) Figureウィンドウを閉じる。 backend関係も切るっぽいので再び fig.show() しても開けない。 Figure https://matplotlib.org/stable/api/figure_api.html\nウィンドウを表示したり画像ファイルを保存したりする単位となるクラス。 コンストラクタを直接呼ぶのではなく plt や sns に作らせるのが普通。 複数の子Axesを保持できる。\nfig.subplots(nrows=1, ncols=1, sharex=False, sharey=False, ...) 子Axesをタイル状に並べて作る。 デフォルトでは単体のAxesを返す。 fig.add_subplot(*args, **kwargs) 指定した位置に子Axesを作る。 少し複雑な配置にしたいときはこれに GridSpec を渡すのが便利。 fig.clear() fig.axes を空っぽにする。 backend関係は切れないので子Axesを追加して再描画可能。 fig.savefig(fname, dpi=None, facecolor='w', edgecolor='w', **kwargs) 拡張子から画像形式を推定してくれるので format= は省略可能。 fig.show() Windowを開いてFigureを表示する。 JupyterやHydrogenなどのIPython環境では display(fig) を使う。 fig.axes 子Axesへの参照 Axes https://matplotlib.org/stable/api/axes_api.html\n軸やラベルを持ったひとつのプロットの単位となるクラス。\n描画のためのメソッドが ax.plot(), ax.scatter(), ax.hist() などたくさんある。 pandas.DataFrame のメソッドとして呼び出すのもあり:\n# Axes method with data ax.scatter(\u0026#39;sepal_width\u0026#39;, \u0026#39;sepal_length\u0026#39;, data=iris) # DataFrame method with ax iris.plot.scatter(\u0026#39;sepal_width\u0026#39;, \u0026#39;sepal_length\u0026#39;, ax=ax) 設定のためのメソッドも ax.set_title(), ax.set_xlim() などたくさん。 ax.set(**kwargs) でまとめて設定することもできる。\nax.clear() プロットしたものを消す。親Figureへの参照は残る。 ax.figure 親Figureへの参照 複数のAxesを配置する https://matplotlib.org/stable/api/gridspec_api.html https://matplotlib.org/stable/tutorials/intermediate/gridspec.html plt.subplots(nrows, ncols, sharex, sharey, ...) 等サイズに分割: fig = plt.figure() axes = fig.subplots(2, 2) sns.regplot(\u0026#39;x\u0026#39;, \u0026#39;y\u0026#39;, d, ax=axes[0, 0]) fig.tight_layout() mpl.gridspec.GridSpec(nrows, ncols, ...) e.g., 2x2分割して \u0026ldquo;品\u0026rdquo; みたいな配置にする: fig = plt.figure() gs = plt.GridSpec(2, 2) ax_top = fig.add_subplot(gs[0, :]) ax_bottom_l = fig.add_subplot(gs[1, 0]) ax_bottom_r = fig.add_subplot(gs[1, 1]) mpl.gridspec.GridSpecFromSubplotSpec(nrows, ncols, subplot_spec, ...) 入れ子で分割。 e.g., 左右に分け、それぞれをさらに3段に分ける: fig = plt.figure() gs = plt.GridSpec(1, 2) gsl = sns.mpl.gridspec.GridSpecFromSubplotSpec(3, 1, gs[0]) gsr = sns.mpl.gridspec.GridSpecFromSubplotSpec(3, 1, gs[1]) ax_ltop = fig.add_subplot(gsl[0]) Text, Annotation, Legend https://matplotlib.org/stable/tutorials/text/text_intro.html\nhttps://matplotlib.org/stable/tutorials/text/annotations.html\nhttps://matplotlib.org/stable/tutorials/intermediate/legend_guide.html\nSeaborn Axes-level plot 色分けや推定値の追加など、生のmatplotlibでやるにはちょっと大変なことがseabornの関数で簡単にできる。 Axesを受け取ってそこに描画するという単純な構造なので、何か自分で作ってもいい:\ndef my_scatter(x, y, data, ax): ax.scatter(x, y, data=data) return ax https://seaborn.pydata.org/examples\nCategorical plots https://seaborn.pydata.org/tutorial/categorical.html\nsns.stripplot(x, y, hue, data, order, ..., ax) 片軸がカテゴリカル変数の散布図 これよりやや規則的な sns.swarmplot() も良い。 sns.boxplot(x, y, hue, data, order, ..., ax) 箱ひげ図 sns.violinplot(x, y, hue, data, order, ..., ax) バイオリンプロット sns.boxenplot(x, y, hue, data, order, ..., ax) 箱ひげ図の変種。別名 letter-value plot 。 sns.pointplot(x, y, hue, data, order, ..., ax) 点推定値(平均値とか)の折れ線グラフ + エラーバー sns.barplot(x, y, hue, data, order, ..., ax) 平均値の棒グラフ + エラーバー sns.countplot(x, y, hue, data, order, ..., ax) カテゴリカル変数の頻度棒グラフ Distribution plots https://seaborn.pydata.org/tutorial/distributions.html\nsns.distplot(a, bins, hist=True, kde=True, rug=False, fit=None, ..., ax) ax.hist() + sns.kdeplot() + sns.rugplot() sns.kdeplot(data, data2=None, shade=False, ..., ax) カーネル密度推定。 sns.heatmap(data, vmin, vmax, cmap, center, ..., square, mask, ax) ヒートマップ。入力データはtidyじゃなくて行列の形。 Regression plots https://seaborn.pydata.org/tutorial/regression.html\nsns.regplot(x, y, data, ..., fit_reg=True, ci=95, ..., ax) 散布図 + 回帰線。 Axis Grid https://seaborn.pydata.org/tutorial/axis_grids.html\nFigureとAxisをいい感じに初期化して、関連するデータを縦・横・色の方向に並べる土台。 これにAxis-level plotを乗せるところまでショートカットする高級関数がFigure-level plot。 できあがったGridクラスの.set()系メソッドとか.figプロパティを通じていろいろ調整できる。 これをまた別のグリッドに埋め込むというRのgrobのような操作はたぶんできない。\nsns.FacetGrid https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\n(data, row, col, hue, col_wrap, sharex, sharey, ...)\nカテゴリカル変数でプロットを分けて並べる:\ngrid = sns.FacetGrid(iris, col=\u0026#39;species\u0026#39;, col_wrap=2) grid.map(sns.regplot, \u0026#39;sepal_width\u0026#39;, \u0026#39;sepal_length\u0026#39;) 変数によって色分けする:\ngrid = sns.FacetGrid(iris, hue=\u0026#39;species\u0026#39;) grid.map(plt.scatter, \u0026#39;sepal_width\u0026#39;, \u0026#39;sepal_length\u0026#39;) map() メソッドには plt.scatter など生のmatplotlib関数も渡せる。\nsns.lmplot(x, y, data, hue, col, row, ...) regplot() + FacetGrid() のショートカット。 sns.catplot(x, y, hue, data, row, col, ..., kind, ...) Categorical plot + FacetGrid() のショートカット。 kind: {strip, swarm, box, violin, boxen, point, bar, count} 昔は factorplot という名前だった。 sns.PairGrid https://seaborn.pydata.org/generated/seaborn.PairGrid.html\n(data, hue, ..., vars, x_vars, y_vars, ...)\nペアワイズ散布図 + 対角線ヒストグラム\ngrid = sns.PairGrid(iris) grid = grid.map_offdiag(sns.regplot) grid = grid.map_diag(sns.distplot) sns.pairplot(data, hue, hue_order, palette, vars, x_vars, y_vars, kind, diag_kind, ...) PairGrid() のショートカット。 kind: {scatter, reg} diag_kind: {hist, kde} sns.JointGrid https://seaborn.pydata.org/generated/seaborn.JointGrid.html\n(x, y, data, size, ratio, space, dropna, xlim, ylim)\n散布図 + 周辺分布:\ngrid = sns.JointGrid(\u0026#39;sepal_width\u0026#39;, \u0026#39;sepal_length\u0026#39;, iris) grid = grid.plot_joint(sns.regplot) grid = grid.plot_marginals(sns.distplot, kde=False) sns.jointplot(x, y, data, kind, stat_func, ...) JointGrid() のショートカット。 kind: {scatter, reg, resid, kde, hex} sns.ClusterGrid() sns.clustermap(data, ...) heatmap() + ClusterGrid() Style https://seaborn.pydata.org/tutorial/aesthetics.html\n背景色や補助線などの設定。\nsns.set_style(style, rc=None)\nstyle: {darkgrid, whitegrid, dark, white, ticks} rc: see below \u0026gt;\u0026gt;\u0026gt; sns.axes_style(style=\u0026#39;darkgrid\u0026#39;, rc=None) {\u0026#39;axes.axisbelow\u0026#39;: True, \u0026#39;axes.edgecolor\u0026#39;: \u0026#39;white\u0026#39;, \u0026#39;axes.facecolor\u0026#39;: \u0026#39;#EAEAF2\u0026#39;, \u0026#39;axes.grid\u0026#39;: True, \u0026#39;axes.labelcolor\u0026#39;: \u0026#39;.15\u0026#39;, \u0026#39;axes.linewidth\u0026#39;: 0, \u0026#39;figure.facecolor\u0026#39;: \u0026#39;white\u0026#39;, \u0026#39;font.family\u0026#39;: [\u0026#39;sans-serif\u0026#39;], \u0026#39;font.sans-serif\u0026#39;: [\u0026#39;Arial\u0026#39;, \u0026#39;Liberation Sans\u0026#39;, \u0026#39;Bitstream Vera Sans\u0026#39;, \u0026#39;sans-serif\u0026#39;], \u0026#39;grid.color\u0026#39;: \u0026#39;white\u0026#39;, \u0026#39;grid.linestyle\u0026#39;: \u0026#39;-\u0026#39;, \u0026#39;image.cmap\u0026#39;: \u0026#39;Greys\u0026#39;, \u0026#39;legend.frameon\u0026#39;: False, \u0026#39;legend.numpoints\u0026#39;: 1, \u0026#39;legend.scatterpoints\u0026#39;: 1, \u0026#39;lines.solid_capstyle\u0026#39;: \u0026#39;round\u0026#39;, \u0026#39;text.color\u0026#39;: \u0026#39;.15\u0026#39;, \u0026#39;xtick.color\u0026#39;: \u0026#39;.15\u0026#39;, \u0026#39;xtick.direction\u0026#39;: \u0026#39;out\u0026#39;, \u0026#39;xtick.major.size\u0026#39;: 0, \u0026#39;xtick.minor.size\u0026#39;: 0, \u0026#39;ytick.color\u0026#39;: \u0026#39;.15\u0026#39;, \u0026#39;ytick.direction\u0026#39;: \u0026#39;out\u0026#39;, \u0026#39;ytick.major.size\u0026#39;: 0, \u0026#39;ytick.minor.size\u0026#39;: 0} with 文でも使える:\nwith sns.axes_style('white'): # plot sns.despine(fig=None, ax=None, top=True, right=True, left=False, bottom=False, offset=None, trim=False) 指定した枠線を消す。 生の matplotlib だと ax.spines['top'].set_visible(False) Context https://seaborn.pydata.org/tutorial/aesthetics.html#scaling-plot-elements-with-plotting-context-and-set-context\nラベルや点・線などのスケール調整。\nsns.set_context(context, font_scale=1, rc=None)\ncontext {notebook: 1.0, paper: 0.8, talk, 1.3, poster, 1.6} rc: see below \u0026gt;\u0026gt;\u0026gt; sns.plotting_context(context=\u0026#39;notebook\u0026#39;, font_scale=1, rc=None) {\u0026#39;axes.labelsize\u0026#39;: 11, \u0026#39;axes.titlesize\u0026#39;: 12, \u0026#39;figure.figsize\u0026#39;: array([ 8. , 5.5]), \u0026#39;grid.linewidth\u0026#39;: 1, \u0026#39;legend.fontsize\u0026#39;: 10, \u0026#39;lines.linewidth\u0026#39;: 1.75, \u0026#39;lines.markeredgewidth\u0026#39;: 0, \u0026#39;lines.markersize\u0026#39;: 7, \u0026#39;patch.linewidth\u0026#39;: 0.3, \u0026#39;xtick.labelsize\u0026#39;: 10, \u0026#39;xtick.major.pad\u0026#39;: 7, \u0026#39;xtick.major.width\u0026#39;: 1, \u0026#39;xtick.minor.width\u0026#39;: 0.5, \u0026#39;ytick.labelsize\u0026#39;: 10, \u0026#39;ytick.major.pad\u0026#39;: 7, \u0026#39;ytick.major.width\u0026#39;: 1, \u0026#39;ytick.minor.width\u0026#39;: 0.5} with 文でも使える:\nwith sns.plotting_context('talk', font_scale=1.2): # plot Color https://matplotlib.org/stable/tutorials/colors/colormaps.html https://seaborn.pydata.org/tutorial/color_palettes.html いくつかの方法で指定できる:\n個々の描画関数の palette や cmap に指定 with sns.color_palette(): のブロック内で描画 sns.set_palette() でデフォルトを指定 パレットもいくつかある:\nColorBrewer の名前で指定。 _r をつけると逆に、_d をつけると暗めになる。 e.g., sns.color_palette('RdBu_r', n_colors=7) xkcd の名前リストを sns.xkcd_palette() に渡す cubehelix_palette() はgrayscaleでもいい感じで印刷できる 自分で作る: sns.hls_palette(), sns.husl_palette(), sns.light_palette(), sns.dark_palette(), sns.diverging_palette() その他 インストール pyenvか何かで最新のPython3系をインストールして、 pip install seaborn を実行。\n設定 https://matplotlib.org/stable/tutorials/introductory/customizing.html https://matplotlib.org/stable/faq/troubleshooting_faq.html https://matplotlib.org/stable/faq/environment_variables_faq.html ~/.matplotlib/matplotlibrc が読まれる。\nsite-packages/matplotlib/mpl-data/matplotlibrc にテンプレートがある。\nbackends https://matplotlib.org/stable/users/explain/backends.html\nMacで非Frameworkとしてビルドした自前Pythonをそのまま使うと怒られる:\nRuntimeError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends. ので ~/.matplotlib/matplotlibrc に backend: tkagg などと書いて対処する必要があったが、 3.1.0から大丈夫になった。\nキャッシュ問題 使用するPythonを変更すると以下のようなランタイムエラーが出ることがある:\nRuntimeError: Could not open facefile /Library/Python/2.6/site-packages/matplotlib/mpl-data/fonts/ttf/Vera.ttf; Cannot_Open_Resource そういうときはキャッシュを削除してみるとよい:\nrm -rf ~/.matplotlib/fontList.cache 書籍 ",
  "href": "/python/matplotlib.html",
  "tags": [
   "python",
   "graph"
  ],
  "title": "matplotlib + seaborn",
  "type": "python"
 },
 {
  "content": " http://meme-suite.org/ http://meme-suite.org/doc/overview.html Bailey et al. 2009 Bailey and Elkan 1994 インストール http://meme-suite.org/doc/install.html\nソースコードをダウンロードして展開:\nwget -O- http://meme-suite.org/meme-software/4.11.2/meme_4.11.2_1.tar.gz | tar xz configure してビルド:\ncd meme_4.11.2/ ./configure --prefix=${HOME}/bio/local/meme --with-url=http://meme-suite.org --enable-build-libxml2 --enable-build-libxslt --enable-opt CC=clang make パスを通す:\nexport PATH=${PATH}:/usr/local/meme/bin MEME http://meme-suite.org/doc/meme.html\n使い方 複数の配列が含まれるFASTAファイルを渡すだけ:\nmeme sequences.fasta [options] -h, -version ヘルプ、バージョン表示 -dna, -protein 配列がDNAかタンパク質か (-protein) -maxsize 入力ファイルの許容サイズ (100000) -nmotifs, -evt 探索するモチーフ数を制御するため、 個数そのものか E-value の上限を指定する。 -evt を使うときは -nmotifs 大きめにしておく。 (-nmotifs 1) -mod モチーフが配列上にどう分布しているか\noops: One Occurence Per Sequence\nzoops: Zero or OOPS\nanr: Any Number of Repetitions -nsites, -minsites, -maxsites それぞれのモチーフがいくつ登場すると仮定するか (デフォルト値は -mod により異なる) -w, -minw, -max 探索するモチーフの長さを指定 (-minw 8 -maxw 50) -revcomp 逆向きも考慮する -pal パリンドロームを探す -bfile \u0026lt;bfile\u0026gt; バックグラウンド配列を生成するマルコフ過程のパラメータを記述したファイルを指定。 これを指定しない場合はトレーニング配列の塩基頻度のみを利用した0階マルコフ。 FASTA配列からファイルを作ってくれるプログラム fasta-get-markov も用意されてる。 http://meme.nbcr.net/meme/doc/fasta-get-markov.html: # order 0 A 3.081e-01 C 1.919e-01 G 1.919e-01 T 3.081e-01 # order 1 AA 1.078e-01 AC 5.256e-02 AG 5.908e-02 AT 8.848e-02 CA 6.519e-02 CC 3.858e-02 CG 2.908e-02 CT 5.908e-02 GA 6.239e-02 GC 3.841e-02 GG 3.858e-02 GT 5.256e-02 TA 7.284e-02 TC 6.239e-02 TG 6.519e-02 TT 1.078e-01 一度適当に走らせてみて、出力結果 meme.txt の COMMAND LINE SUMMARY や meme.html の model parameters を見るとよい。デフォルト値もそこで分かる。\nスコア Bailey and Gribskov 1998\nE-value そのモチーフが同じサイズのランダムな配列の中にたまたま見つかる個数の期待値 Position p-value\nCombined p-value\nモチーフの出力形式 LOGO アルファベットの大きさで視覚的に表示 PSPM: position-specific probability matrix ポジションごとの塩基・アミノ酸の相対的な頻度を実数[0, 1]の行列で表示。 position weight matrix (PWM) と呼ぶことが多いような。 PSSM: position-specific scoring matrix このあと MAST で使える形式の行列 BLOCKS, FASTA そのモチーフを含む配列のID、開始位置、ヒットした領域の配列 Raw モチーフにヒットした領域を切り出して並べただけ regular expression [AT] のように正規表現の文字集合を使った配列 DREME Discriminative Regular Expression Motif Elicitation\n短いモチーフが得意で効率的。 background (negative) 配列を指定できる。 ChIP-seqデータではピーク周辺100bpくらいを使うべし。\nMEME-ChIP 長いモチーフが得意な MEME と 短いモチーフが得意な DREME を組み合わせて ensemble。\nMAST http://meme.nbcr.net/meme/doc/mast.html\n既知のモチーフ (MEME で発見されたとか) を配列データベースから検索する。\nReferences ",
  "href": "/bio/meme.html",
  "tags": [
   "genetics"
  ],
  "title": "MEME",
  "type": "bio"
 },
 {
  "content": "http://manpages.ubuntu.com/manpages/precise/man8/mount.8.html\n基本的な使い方 予め作っておいた空のディレクトリ dir に device をマウントする:\n[sudo] mount [-t type] [-o option[,option]...] device dir マウントを解除する:\n[sudo] umount dir 既にマウントされているものを列挙:\nmount -l 主な -o オプション defaults rw,suid,dev,exec,auto,nouser,async と同じ rw 読み書きモード。逆は ro suid SUIDとSGIDを有効に dev マウントしたファイルシステム上にあるデバイスを使えるように exec プログラム実行を許可 auto mount -a で一緒にマウントされるように nouser root 以外のユーザでマウントできないように async とりあえずメモリに置いたらいいことにして処理を進め、裏でディスクに書き込む。逆は sync nounix Unix拡張機能を無効に (cifs) iocharset=utf8 文字コードの設定。デフォルトは iso8859-1 uid, gid ユーザID、グループIDを指定 cifs/samba Ubuntu 12.04 から cifs マウント http://manpages.ubuntu.com/manpages/precise/man8/mount.cifs.8.html\ncifs-utils をインストール:\nsudo apt-get install cifs-utils パスワードをコマンド履歴や /etc/fstab に残さなくて済むように ~/.cifs のようなファイルを作っておく:\nusername=iwasaki password=****** mount コマンドでマウント:\nsudo mount -t cifs -o defaults,iocharset=utf8,nounix,uid=$(id -u),gid=$(id -g),credentials=$HOME/.cifs //ADDRESS/VOLUME ~/mnt 起動時に自動でマウントさせるには /etc/fstab に追記:\n//ADDRESS/VOLUME /home/iwasaki/mnt cifs credentials=/home/iwasaki/.cifs,uid=iwasaki,gid=iwasaki,nounix,iocharset=utf8,defaults 0 0 Ubuntu 12.04 のホームディレクトリを cifs/smb マウント出来るようにする samba をインストール:\nsudo apt-get install samba /etc/samba/smb.conf の一部を編集:\n[homes] comment = Home Directories browseable = no create mask = 0644 directory mask = 0755 valid users = %S サービスを再起動:\nsudo service smbd restart afp Ubuntu 12.04 のコマンドラインから afp でマウント afpfs-ng-utils をダウンロードしてインストール:\nwget http://launchpadlibrarian.net/90192653/afpfs-ng-utils_0.8.1-2_amd64.deb sudo dpkg -i afpfs-ng-utils_0.8.1-2_amd64.deb 以下のようなコマンドでマウント。できなかった:\nmount_afp 'afp://user:password@address/volume/' ~/mnt Ubuntu 12.04 の Nautilus から afp でマウント Nautilusをアクティブにして control + l (あるいはメニューバーから Go --\u0026gt; Location...） Location に afp://***.***.***.*** という形でIPアドレスを入力してConnect Mac の Finder からマウント Finderをアクティブにして command + k (あるいはメニューバーから Go --\u0026gt; Connect to Server...) Server Address に afp://***.***.***.*** という形でIPアドレスを入力してConnect sshfs https://github.com/libfuse/sshfs\nリモートのファイルシステムをssh経由でマウントし、 Finderとかで普通のディレクトリのように扱えるようにする。 マウントされる側はsshでログインできさえすればいいので、 手元のマシンに必要な準備をする。\nHomebrewでsshfsをインストール:\nbrew install sshfs brew install gromgit/fuse/sshfs-mac 非オープンソースのmacFUSE に依存するMacでは公式サポートから外れたので 非公式Tapを使ってインストールする。 libfuse などは自動的に入るはず。\nマウントポイントにする適当なディレクトリを作る。 e.g., mkdir ~/mount\nマウント/アンマウントする:\nsshfs watal@example.com:/home/watal ~/mount umount ~/mount ",
  "href": "/dev/mount.html",
  "tags": [
   "communication"
  ],
  "title": "mount",
  "type": "dev"
 },
 {
  "content": "https://www.scipy.org/\nInstallation https://www.scipy.org/install.html\npip install numpy scipy BLAS/LAPACK関連の確認:\nimport numpy as np np.show_config() NumPy 数値計算・配列演算ライブラリ\nhttps://numpy.org/doc/stable/reference/\nndarray: N次元配列クラス https://numpy.org/doc/stable/reference/arrays.ndarray.html\n2D行列に特殊化したものとして numpy.matrix もあったが2018年ごろから非推奨。\ndata.frame/tibble のようなものがほしいときは pandas.DataFrame の出番。\nhttps://numpy.org/doc/stable/reference/routines.matlib.html\nUniversal functions https://numpy.org/doc/stable/reference/ufuncs.html#available-ufuncs\nndarray に対して効率よく element-wise な演算をする関数\nRoutine https://numpy.org/doc/stable/reference/routines.html\nndarray を作ったり操作したりする関数全般\nSciPy: 高度な科学技術計算ライブラリ https://docs.scipy.org/doc/scipy/reference/\nscipy.stats https://docs.scipy.org/doc/scipy/reference/stats.html https://docs.scipy.org/doc/scipy/tutorial/stats.html 書籍 ",
  "href": "/python/scipy.html",
  "tags": [
   "python"
  ],
  "title": "NumPy, SciPy",
  "type": "python"
 },
 {
  "content": "https://pandas.pydata.org/\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns iris = sns.load_dataset(\u0026#39;iris\u0026#39;) 型 https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html\nSeries 1次元の名前付き np.array() みたいなもの。 DataFrame Seriesを列とする2次元の表。 axis=0 が行(index)で、axis=1 が列(columns)。 DataFrameを時間軸方向に重ねたような3次元構造として Panel ってのもあったけど MultiIndex があれば不要ってことでv0.20からdeprecated、v0.25で削除。 \u0026ldquo;pan(el)-da(ta)-s\u0026rdquo; が名前の由来だったのに。\n読み書き pd.read_csv(infile) sep=',' header='infer' names=None index_col=None usecols=None df.to_csv(outfile) sep=',' float_format=None index=True 基本操作 https://pandas.pydata.org/pandas-docs/stable/10min.html\ndf.columns df.index df.values df.dtypes df.head() df.tail() https://pandas.pydata.org/pandas-docs/stable/indexing.html\ndf[[\u0026#39;species\u0026#39;]] # DataFrame with 1 column df[\u0026#39;species\u0026#39;] # Series df.species # Series (not recommended) # label-based df.loc[0] df.loc[:,\u0026#39;sepal_width\u0026#39;] # integer-based df.iloc[0] df.iloc[:,1] # fast scalar (single value) lookup df.at[0,\u0026#39;sepal_width\u0026#39;] df.iat[0,1] df.species のような attribute access は既存のメソッド名(e.g., min)と被るとダメなので基本的には使わないほうが良さそう。\n番号でもラベルでもアクセスできる df.ix[] もあったが、 曖昧で危険なのでdeprecated.\nmethod chaining DataFrameを何回も再帰代入するより、メソッドを繋いだほうが書きやすいし読みやすい。 カッコかバックスラッシュを使えばドット前に改行やスペースを入れて整形可能。\n(iris.query(\u0026#39;species != \u0026#34;setosa\u0026#34;\u0026#39;) .filter(regex=\u0026#39;^(?!sepal)\u0026#39;) .assign(petal_area=lambda x: x[\u0026#39;petal_length\u0026#39;] * x[\u0026#39;petal_width\u0026#39;] * 0.5) .groupby(\u0026#39;species\u0026#39;) .aggregate(np.mean) .sort_values([\u0026#39;petal_length\u0026#39;], ascending=False) ) See also dplyr + magrittr on R\n変形 Reshaping Hierarchical indexing MultiIndex Index df.set_index(keys, drop=True, append=False, inplace=False, verify_integrity=False) 指定したcolumnをindexにする。 df.reset_index(level=None, drop=False, inplace=False, ...) 現在のindexをcolumn化して、新しいものを振り直す。 df.swaplevel(0, 1, axis=0)\ndf.reorder_levels([1, 0], axis=0)\ndf.rename_axis(mapper, axis=0, copy=True, inplace=False)\nmelt, pivot Rと同様、列を列として扱う。\ndf.melt(id_vars=None, value_vars=None, var_name=None, value_name='value', ...) 縦長に変形。Rでいう tidyr::pivot_longer()。 e.g., iris.melt('species', var_name='variable', value_name='value') df.pivot(index=None, columns=None, values=None) 横長に変形。Rでいう tidyr::pivot_wider()。 動かさない列を指定できないのかな？ reshape2::dcast() のように複数の値をaggregationできる亜種として df.pivot_table() があるけどまあ使わないのが無難か。 molten = iris.reset_index().melt([\u0026#39;index\u0026#39;, \u0026#39;species\u0026#39;]) molten.pivot(\u0026#39;index\u0026#39;, \u0026#39;variable\u0026#39;) # species columns are redundant molten.pivot(\u0026#39;index\u0026#39;, \u0026#39;variable\u0026#39;, \u0026#39;value\u0026#39;) # species column is removed stack, unstack MultiIndexを中心に考える。 nested tibble的なイメージ？\ndf.stack(level=-1, dropna=True) 縦長に変形。 melt() と違って変数名はcolumnにならず、 新しいindexとして最内側に追加される。 そのindexに名前をつけるオプションが欲しかった。 部分的に変形するオプションも無いので、 残したい列を予めindexにしておく必要がある。 df.unstack(level=-1, fill_value=None), s.unstack() 横長に変形。 展開するindexの階層を指定できる(デフォルトの-1は最内側)。 stacked = iris.set_index([\u0026#39;species\u0026#39;], append=True).stack() stacked.unstack() # more explicitly stacked.rename_axis([\u0026#39;id\u0026#39;, \u0026#39;species\u0026#39;, \u0026#39;variable\u0026#39;]).unstack(\u0026#39;variable\u0026#39;) 設定 https://pandas.pydata.org/pandas-docs/stable/options.html\npd.set_option(\u0026#39;display.max_rows\u0026#39;, 20) pd.set_option(\u0026#39;display.width\u0026#39;, None) ディスプレイ幅は os.get_terminal_size().columns で明示的に取得してもいい。\nmisc. 英語の発音としては [pǽndəz] のように濁るのが自然だけど 開発者は pan-duss [pǽndəs] と発音している とのこと。\n書籍 ",
  "href": "/python/pandas.html",
  "tags": [
   "python"
  ],
  "title": "Pandas",
  "type": "python"
 },
 {
  "content": "古いPythonではパッケージ管理のためにツールを別途インストールする必要があった。 Python 3.4 以降では venv と ensurepip が標準ライブラリに入って少しマシに。\n科学技術系の利用だけなら、Python本体のインストールからパッケージ管理までぜーんぶ Anaconda に任せるのが楽ちんらしい。 その場合 pip を混ぜて使ってはいけないので、本記事はほぼ無用。 ただし、環境を汚したりAnaconda特有の不具合が出たりするので私は使わないしオススメもしない。\npip https://pip.pypa.io/\nPyPI からの簡単にパッケージをインストールできるようにするツール。 アンインストール機能の無い easy_install に取って代わり、 現在では公式に推奨されている。 Python 2.7.9以降、3.4以降では標準ライブラリの ensurepip によって自動的にインストールされる。\n全体のヘルプ、コマンド毎の詳細ヘルプ:\npip3 help pip3 install --help よく使うコマンド:\npip3 list --outdated pip3 install -U setuptools pip wheel pip3 search jupyter 設定ファイルは ~/.config/pip/pip.conf と公式には書いてあるが ~/.config/python/pip.conf でも認識される:\n[list] format = columns 全パッケージをバージョンまでそっくり引き継ぐには:\npip3 freeze \u0026gt;requirements.txt pip3 install -r requirements.txt 手動インストール:\npython -m ensurepip --user # または curl -O https://bootstrap.pypa.io/get-pip.py python get-pip.py --user --user を付けた場合のインストール先は環境変数 PYTHONUSERBASE で指定できる。 その中の bin/ を PATH に追加するか、 絶対パスで pip コマンドを使う。\nvenv https://docs.python.org/3/library/venv.html\nPython実行環境を仮想化するパッケージ。 これで作った仮想環境内で pip を使ってパッケージ管理する。 Python 3.3 以降では venv が標準ライブラリ入りしたので virtualenv の個別インストールは不要になった。\n仮想環境を作る:\npython3 -m venv [OPTIONS] ~/.virtualenvs/myproject 仮想環境に入る、から出る:\nsource ~/.virtualenvs/myproject/bin/activate deactivate 仮想環境の置き場所はどこでもいいけど、 ~/.venvs とか ~/.virtualenvs にしておけばツールに見つけてもらいやすい。 例えば vscode-python, reticulate, etc.\nsetuptools https://github.com/pypa/setuptools\nパッケージ管理・作成の基本となるライブラリ。 コマンドラインツール easy_install はこれの一部として含まれているが、直接使うことはない。 (pip を使う)\nSee also \u0026ldquo;setuptools \u0026mdash; Pythonパッケージ作成\u0026rdquo;\nsetuptools の改良版としてしばらく distribute も利用されていたが、 その成果が setuptools にマージされたので忘れていい。\n関連書籍 ",
  "href": "/python/pip.html",
  "tags": [
   "python",
   "package"
  ],
  "title": "pip",
  "type": "python"
 },
 {
  "content": " Model Wright-Fisher model\nランダム交配 世代重複なし 集団サイズは有限 のNで一定 （ここがHardy-Weinbergと違う。そしてこのことによる遺伝的浮動に興味がある） 係数sの変異の固定確率 $\\frac {1 - e^{-2s}} {1 - e^{-4Ns}}$ Moran model\n世代重複あり(1個体が複製して、死ぬ1個体を置き換える)。 Wright-Fisherに合わせるならNステップを1世代と考える。 繁殖成功の標準偏差(ヘテロ接合頻度の減少速度＝遺伝的浮動の強さ)はWright-Fisherの倍。 有効集団サイズが半分。 増殖率rの変異の固定確率 $\\frac {1 - 1/r} {1 - 1/r^N}$ Statistics The unfolded site-frequency counts $\\xi_i$ 派生型のアリルが i 個、祖先型のアリルが n-i 個である変異サイトの数 The folded site-frequency counts $\\eta_i$ どっちが祖先型か不明な状態。 片方のアリルが i 個、もう片方のアリルが n-i 個である変異サイトの数 $\\eta_i = \\frac{\\xi_i + \\xi _{n - i}}{1 + \\delta _{i, n - i}}$\nThe number of segregating (polymorphic) sites $S$ 配列セットの中で、多型のあるサイトの数 Nucleotide diversity / 塩基多様度 $\\pi$ 整列済み配列セットについてペアワイズで塩基の異なるサイト数を数え、 ペアあたりで平均したもの。 多型サイト数が同じでも、アリル頻度が均等なほど大きくなり、 少数のアリルが優占してたりすると小さくなる。 Population mutation rate / 集団突然変異率 $\\theta$ 二倍体常染色体なら $4N_e\\mu$、 二倍体X染色体なら $3N_e\\mu$、 一倍体なら $2N_e\\mu$。 直接測定することができないためほかの値から推定する。 Watterson (1975): $S$ から推定\n\\[\\begin{aligned} a_1 \u0026= 1 + \\frac 1 2 + \\frac 1 3 + ... + \\frac 1{n-1}\\\\ \\mathrm{E}[S] \u0026= \\theta L a_1\\\\ \\theta_w \u0026= \\frac{S}{L a_1} \\end{aligned}\\] Tajima (1983): $\\pi$ から推定\n\\[\\begin{aligned} \\mathrm{E}[\\pi] \u0026= \\theta L\\\\ \\theta_\\pi \u0026= \\frac \\pi L \\end{aligned}\\] Selection 表現型の頻度分布に着目 directional selection / 方向性選択 形質値の頻度分布が一方向的に動くように働く選択。 その方向に表現型変化をもたらす変異に対しては positive selectionがかかり、 逆方向の表現型変化をもたらす変異に対しては purifying selectionがかかる。 stabilizing selection / 安定化選択 有利で頻度の高い形質値を中心として、 頻度分布が広がらないように働く選択。 disrupbtiveの逆。 結果的に配列に対して purifying selection がかかることは多いと思われるが、 より安定してその形質値を実現できるようなアリルに対して positive selectionがかかることもあるだろう。 disruptive selection / 分断化選択 中間的な表現型が不利で、形質値の頻度分布に谷ができるように働く選択。 stabilizingの逆。 表現型可塑性や表現型多型で対処される場合もあり、 必ずしもbalancing selectionや種分化をもたらさない。 遺伝子型頻度に着目 positive selection 有益アリルの頻度を上げるように働く選択 negative selection = purifying selection / 純化選択 有害アリルを集団から取り除くように働く選択 background selection (Charlesworth et al. 1993): 有害変異に対する purifying selection によって 近傍配列まで遺伝的多様度が減少する どっちでも balancing selection / 平衡選択 多型を維持するように働く選択\nheterozygote advantage\ntemporally varying selection\nspatially varying selection\nfrequency-dependent selection / 頻度依存選択\nantagonistic pleiotropy, disassortative mating, self-incompatibility ",
  "href": "/bio/popgen.html",
  "tags": [
   "genetics"
  ],
  "title": "Population Genetics",
  "type": "bio"
 },
 {
  "content": " Author Christopher M. Bishop Book Pattern Recognition and Machine Learning Publisher Springer Materials http://research.microsoft.com/en-us/um/people/cmbishop/prml/ 輪読担当 岩嵜航 日程 2014-12-08 11. Sampling Methods 10章では決定論的な近似を見てきた。 この章ではサンプリングを伴う Monte Carlo 法を取り扱う。\nモンテカルロ法の由来\nスタニスワフ・ウラムがソリテアの成功率を考えてた時に思いついて、 同僚のジョン・フォン・ノイマンが計算機上での実用まで持ってったらしい。 モナコ公国のモンテカルロ地区に国営カジノがあって、 ウラムの叔父がそこで負けて親戚から借金したことにちなんで 同僚のニコラス・メトロポリスが命名したらしい。\n目標: 変数 $\\mathbf z$ の分布 $\\color{red}{p(\\mathbf z)}$ を考えた上で、 ある関数 $\\color{blue}{f(\\mathbf z)}$ の値がどうなるか予測したい。\nFigure 11.1\n$\\color{blue}{f(\\textbf{z})}$ の期待値は (式11.1)\n\\[ \\mathbb E[f] = \\int f(\\mathbf z) p(\\mathbf z) \\mathrm d \\mathbf z \\] みたいな感じで表せるが、だいたいは複雑過ぎて解析的に解けないので、そういうときどうしようかという話。\n$\\color{red}{p(\\mathbf z)}$ の分布から $L$ 個サンプリングしてきた $\\mathbf{z}_l$ をそれぞれ $f$ に放り込んで平均を取ってみよう (式11.2)。\n\\[ \\hat f = \\frac 1 L \\sum^L_{l=1} f(\\mathbf z_l) \\] その推定値の期待値は (Exercise 11.1)\n\\[\\begin{aligned} \\mathbb E[\\hat f] \u0026= \\mathbb E \\left[\\frac 1 L \\sum^L_{l=1} f(\\mathbf z_l) \\right] \\\\ \u0026= \\frac 1 L \\sum^L_{l=1} \\int f(\\mathbf z_l) p(\\mathbf z_l) \\mathrm d \\mathbf z_l \\\\ \u0026= \\frac 1 L \\sum^L_{l=1} \\mathbb E[f] \\\\ \u0026= \\mathbb E[f] \\end{aligned}\\] で真の期待値と同じになる。 この推定値の分散は (Exercise 11.1, 式 11.3)\n\\[\\begin{aligned} \\text{var}[\\hat f] \u0026= \\text{var} \\left[\\frac 1 L \\sum^L_{l=1} f(\\mathbf z_l) \\right] \\\\ \u0026= \\frac 1 {L^2} \\sum^L_{l=1} \\text{var}[f(\\mathbf z)] \\\\ \u0026= \\frac 1 L \\text{var}[f] \\\\ \u0026= \\frac 1 L \\mathbb E[(f - \\mathbb E[f])^2] \\end{aligned}\\] となる。注意すべき点としては:\n推定精度が次元数によらない 基本的には $L$ をそんなに大きく取らなくても(10とか20くらいで)よさそう ただし、サンプルが独立じゃない場合にはその辺を加味した有効サンプル数が十分になるように多めに取るべし $\\color{red}{p(\\mathbf z)}$ が大きいところで $\\color{blue}{f(\\mathbf z)}$ がゼロに近くなるような場合、少確率で出てくる大きな値に推定値が引っ張られることがあるので比較的多めに取るべし $\\color{red}{p(\\mathbf z)}$ が実は $p(z_1, z_2, \u0026hellip;, z_M)$ という同時確率だということを思い出そう。 $z_i$ がそれぞれ独立な分布から出てくる場合はいいとして、そうじゃない場合はどうしたらいいか？\nFigure 8.2\n変数の因果関係がこのような閉路なし有向グラフで表せる場合、同時確率は **式 8.4** $p(x_1)p(x_2)p(x_3)p(x_4 \\mid x_1,x_2,x_3)p(x_5 \\mid x_1,x_3)p(x_6 \\mid x_4)p(x_7 \\mid x_4,x_5)$ のように条件付き確率の積で表せる。 依存関係の親となるほうから順に条件付き確率で生成 (ancestral sampling 伝承サンプリング) していくことにすると、同時確率は一般的に (式 11.4)\n\\[ p(\\mathbf z) = \\prod_{i=1}^M p(\\mathbf z_i \\mid \\mathrm{pa}_i) \\] というふうに書ける。 変数の一部が観測可能な場合は logic sampling (セクション11.1.4で登場する 重点サンプリング importance sampling の特殊ケース) が使える。\n因果が分からなくて無向グラフで表されるような場合には $z_1$ から $z_M$ まで一周するだけでは求まらず、 ギブズサンプリング (Gibbs sampling) のような計算量のかかる手法が必要になる。\n11.1. Basic Sampling Algorithms コンピュータ上でサンプリングを行うときに真の乱数を使うことは稀で、だいたいは適当なシードから決定論的な過程で擬似乱数を生成することになる。 擬似乱数の質も問題になったりするけどこの本では詳しく扱わない。 いい感じで $(0,1)$ の一様乱数が生成できるものとして進める。\nUnix/Linux系OSが提供する乱数\nハードウェア的なノイズから生成した真の乱数は /dev/random から読み出せるが、 いくつも生成しようとするとノイズが溜まるまで待たされることになるのであまり使わない。 待ち時間無しにそれなりの擬似乱数を作ってくれるデバイスとして /dev/urandom があるが、ここから毎回読み出すのもコストが高いので、シード生成にのみ使う。\n/cxx/random\n11.1.1 Standard distributions 変数 $z$ が $(0,1)$ の一様乱数だとして、 適当な関数をかけて $y = f(z)$ とするとその分布は (式 11.5)\n\\[ p(y) = p(z) \\left| \\frac {\\mathrm dz} {\\mathrm dy} \\right| \\] となる。 変換後の乱数 $y$ が任意の形の分布 $\\color{red}{p(y)}$ に従うようにするにはどうしたらいいか。 $\\color{red}{p(y)}$ の不定積分を (式 11.6)\n\\[ z = h(y) \\equiv \\int _{-\\infty}^y p(\\hat y) \\mathrm d\\hat y \\] のように $\\color{blue}{h(y)}$ として定義してみると 図 11.2 のような関係になる。\nFigure 11.2\n縦軸を $z$ として青い線を逆関数の目線で見てみると、$y$ が中央付近に来るような $z$ の区間はすごく短いが、$y$ が左から3分の1くらいのところに来るような $z$ の区間はかなり長い。 その不定積分の逆関数を一様乱数にかけて $y = h^{-1}(z)$ とすれば欲しかった分布の乱数が出てくる！\n例えば指数分布だと (式 11.7)\n\\[\\begin{aligned} p(y) \u0026= \\lambda \\exp(-\\lambda y) \\\\ z = h(y) \u0026= \\int_0^y \\lambda \\exp(-\\lambda \\hat y) \\mathrm d \\hat y \\\\ \u0026= \\left[-\\exp(-\\lambda \\hat y) \\right]_0^y \\\\ \u0026= 1 - \\exp(-\\lambda y) \\\\ \\exp(-\\lambda y) \u0026= 1 - z \\\\ -\\lambda y \u0026= \\ln(1 - z) \\\\ y \u0026= -\\frac {\\ln(1 - z)} \\lambda \\end{aligned}\\] となるので、$y = -\\lambda^{-1} \\ln(1 - z)$ とすれば $y$ は指数分布に従う乱数となる。\n別の例としてコーシー分布も同じように変換できる (式 11.8, Exercise 11.3)\n\\[\\begin{aligned} p(y) \u0026= \\frac 1 \\pi \\frac 1 {1 + y^2} \\\\ z = h(y) \u0026= \\int_{-\\infty}^y \\frac 1 \\pi \\frac 1 {1 + \\hat y^2} \\mathrm d \\hat y \\\\ \u0026= \\frac 1 \\pi \\left[\\arctan(\\hat y) \\right]_{-\\infty}^y \\\\ \u0026= \\frac 1 \\pi \\left(\\arctan(y) + \\frac \\pi 2 \\right) \\\\ \u0026= \\frac {\\arctan(y)} \\pi + \\frac 1 2 \\\\ \\arctan(y) \u0026= \\pi(z - \\frac 1 2) \\\\ y \u0026= \\tan\\left[\\pi(z - \\frac 1 2)\\right] \\end{aligned}\\] 多変量の場合はヤコビアンを使えばよい\n\\[ p(y_1, ..., y_M) = p(z_1, ..., z_M) \\left| \\frac {\\partial (z_1, ..., z_M)} {\\partial (y_1, ..., y_M)} \\right| \\] 例として2系統の独立な正規乱数を生成する Box-Muller 法を見てみる。 まず $(-1,1)$ の一様乱数をふたつ $z_1, z_2$ として取ってきて、 $z_1^2 + z_2^2 \\leq 1$ を満たさなければ捨てる。 これは下図の円の中に収まる一様乱数だけ取ってくることに相当する。\nFigure 11.3\n$r^2 = z_1^2 + z_2^2$ として (式 11.10, 式 11.11)\n\\[\\begin{aligned} y_1 \u0026= z_1 \\left(\\frac {-2\\ln r^2} {r^2}\\right)^{1/2}\\\\ y_2 \u0026= z_2 \\left(\\frac {-2\\ln r^2} {r^2}\\right)^{1/2} \\end{aligned}\\] のように変換すると $y_1$ と $y_2$ の同時分布は\n\\[\\begin{aligned} p(y_1, y_2) \u0026= p(z_1, z_2) \\left| \\frac{\\partial(z_1, z_2)} {\\partial(y_1, y_2)} \\right|\\\\ \u0026= \\left[\\frac 1 {\\sqrt{2\\pi}} \\exp(-\\frac {y_1^2} 2) \\right] \\left[\\frac 1 {\\sqrt{2\\pi}} \\exp(-\\frac {y_2^2} 2) \\right] \\end{aligned}\\] のように表され、それぞれ独立な標準正規乱数になっていることがわかる。 平均と分散を変えたければ、$y = \\mu + \\sigma z$ のように標準偏差をかけて平均を足せばよい。\nC++11 の std::normal_distribution や GSL の gsl_ran_gaussian でも使われている。 円に収まらないものを棄却する方法ではなく、三角関数を使ってそのまま用いる方法が取られる。\n多変量の場合も同様に $\\mathbf y = \\mathbf \\mu + \\mathbf{Lz}$ として動かせる。 ただし共分散は $\\mathbf \\Sigma = \\mathbf{LL}^\\mathrm T$ として コレスキー分解 (Cholesky decomposition)する。 これは対称行列に特化したLU分解で、$\\mathbf L$ は下三角行列になる。 変換後の平均と分散を確かめてみる (Excersize 11.5)\n\\[\\begin{aligned} \\mathbb E[\\mathbf y] \u0026= \\mathbb E[\\mathbf \\mu + \\mathbf{Lz}] = \\mathbf \\mu + \\mathbf 0 \\\\ \\text{cov}[\\mathbf y] \u0026= \\mathbb E\\left[(\\mathbf y - \\mathbb E[\\mathbf y])(\\mathbf y - \\mathbb E[\\mathbf y])^\\mathrm T \\right] \\\\ \u0026= \\mathbb E[(\\mathbf \\mu + \\mathbf{Lz} - \\mathbf \\mu)(\\mathbf \\mu + \\mathbf{Lz} - \\mathbf \\mu)^\\mathrm T] \\\\ \u0026= \\mathbb E[\\mathbf{Lz}(\\mathbf{Lz})^\\mathrm T] \\\\ \u0026= \\mathbf{LL}^\\mathrm T = \\mathbf \\Sigma\\\\ \\end{aligned}\\] ただし一様乱数 $\\mathbf z$ については $\\mathbb E[\\mathbf z] = \\mathbf 0$ かつ $\\mathbb E[\\mathbf{zz}^\\mathrm T] = \\mathbf I$ 。\nここで説明したような手法が使えるのは、不定積分の逆関数が簡単に得られるような場合だけ。 より一般的に使える rejection sampling と importance sampling について、この先で見ていく。\n",
  "href": "/lectures/prml-11-1.html",
  "tags": [
   "math",
   "book"
  ],
  "title": "PRML輪読会 11章1節",
  "type": "lectures"
 },
 {
  "content": " Author Christopher M. Bishop Book Pattern Recognition and Machine Learning Publisher Springer Materials http://research.microsoft.com/en-us/um/people/cmbishop/prml/ 輪読担当 岩嵜航 日程 2014-03-10 2. Probability Distributions density estimation: 密度推定 与えられた確率変数xのセットから確率分布p(x)をモデル化すること。 parametric method: パラメトリック法 平均や分散といった少数の変数によって規定される分布を仮定して当てはめる。 頻度主義的には、尤度関数など特定のクライテリアに従って最適化して値を決める。 ベイズ主義的には、事前分布と観察データから事後分布を得る。 i.i.d. 独立同分布 あるいは 独立同一分布 independent and identically distributed の略。 確率変数のセットが同じ分布に従っていて独立なとき。 2.1–2.2 離散型確率変数\n2.3 Gaussian distribution: 正規分布\n2.4 exponential family: 指数型分布族\n2.5 nonparametric method: ノンパラメトリック法\n2.1 Binary Variables 二値変数 $x \\in {0,1}$\nコインの表裏(head/tail)のように、2つの値を取りうる確率変数。 e.g. 表なら x = 1、裏なら x = 0。\nBernoulli distribution ベルヌーイ分布\n表の出る確率が μ であるコインを1回投げて、 表が出る確率と裏が出る確率はそれぞれ\n\\[\\begin{aligned} p(x = 1 \\mid \\mu) \u0026= \\mu\\\\ p(x = 0 \\mid \\mu) \u0026= 1 - \\mu \\end{aligned}\\] x の確率分布として書いて平均と分散 (Exercise 2.1) を求めると\n\\[\\begin{aligned} \\text{Bern}(x \\mid \\mu) \u0026= \\mu ^ x (1 - \\mu) ^ {1 - x} \\\\ \\mathrm{E}[x] \u0026= 0(1 - \\mu) + 1\\mu = \\mu\\\\ \\text{var}[x] \u0026= \\mathrm{E}[(x - \\mu)^2] = \\mu^2 (1 - \\mu) + (1 - \\mu)^2 \\mu = \\mu (1 - \\mu) \\end{aligned}\\] パラメータ μ の下で N 回投げたデータセット $D = {x_1, \u0026hellip;, x_N}$ が得られる確率、すなわち尤度は\n\\[ p(D \\mid \\mu) = \\prod_{n = 1}^N {p(x_n \\mid \\mu)} = \\prod_{n = 1}^N {\\mu^{x_n} (1 - \\mu)^{1 - x_n}} \\] これを最大化する μ を求めるため、まず対数を取って\n\\[\\begin{aligned} \\ln p(D \\mid \\mu) \u0026= \\sum_{n = 1}^N {\\ln p(x_n \\mid \\mu)}\\\\ \u0026= \\sum_{n = 1}^N {\\{x_n \\ln \\mu + (1 - x_n) \\ln (1 - \\mu)\\}}\\\\ \u0026= \\ln \\mu \\sum_{n = 1}^N x_n + \\ln (1 - \\mu) \\sum_{n = 1}^N (1 - x_n)\\\\ \u0026= \\{\\ln \\mu - \\ln(1 - \\mu)\\} \\sum_{n = 1}^N x_n + N \\ln (1 - \\mu)\\\\ \\end{aligned}\\] sufficient statistic: 十分統計量\nここで対数尤度は個々の $x_n$ によらず、 総和 $\\sum_n {x_n}$ だけに依存している。 そんな感じのやつを十分統計量と呼ぶが、ここでは詳しく触れない。\nμ で微分したものが0になるように\n\\[\\begin{aligned} \\frac{d}{d\\mu} \\ln p(D \\mid \\mu) = \\{\\frac{1}{\\mu} + \\frac{1}{1 - \\mu}\\} \\sum_{n = 1}^N x_n - \\frac{N}{1 - \\mu} \u0026= 0\\\\ - \\frac{N}{1 - \\mu} \u0026= - \\frac{1}{\\mu (1 - \\mu)} \\sum_{n = 1}^N x_n\\\\ \\mu_\\text{ML} \u0026= \\frac{1}{N} \\sum_{n = 1}^N {x_n}\\\\ \u0026= \\frac{m}{N} \\end{aligned}\\] 結局のところ標本平均と同じ。 コイン3回投げて3回表だと最尤は μ = 1 だが、これはどう考えてもover-fittingである。 だいたい半々で表裏が出るっしょ、という事前情報で補正していきたい。\nBinomial distribution 二項分布\n確率 μ で表の出るコインを N 回投げて表が出る回数 m の確率分布\n\\[ \\text{Bin}(m \\mid N, \\mu) = \\binom{N}{m} \\mu^m (1 - \\mu)^{N - m} \\] Figure 2.1\n例えば N = 10, μ = 0.25 のときの m の頻度分布\n1回1回の観察は独立なベルヌーイ試行であり、 そういうときは $\\mathrm{E}[x + z] = \\mathrm{E}[x] + \\mathrm{E}[z]$ かつ $\\text{var}[x + z] = \\text{var}[x] + \\text{var}[z]$ が成り立つので (Exercise 1.10)、平均と分散は (Exercise 2.4)\n\\[\\begin{aligned} \\mathrm{E}[m] \u0026= \\mathrm{E}[\\sum_{n=1}^N x_n] = \\sum_{n=1}^N \\mathrm{E}[x_n] = \\sum_{n=1}^N \\mu = N \\mu \\\\ \\text{var}[m] \u0026= \\text{var}[\\sum_{n=1}^N x_n] = \\sum_{n=1}^N \\text{var}[x_n] = \\sum_{n=1}^N \\mu (1 - \\mu) = N \\mu (1 - \\mu) \\end{aligned}\\] 2.1.1 beta distribution 3回続けて表が出たからといって μ=1 なわけがない、 という事前情報をうまく取り入れて過学習を避けたい。 事前分布 p(μ) を導入しよう。\n\\[ \\text{posterior} \\propto \\text{prior} \\times \\text{likelihood} \\] 尤度が $\\mu^x (1 - \\mu)^{1 - x}$ という形なので、 事前分布も μ と 1 - μ の累乗にしておくと、 事後分布と事前分布のの関数形が同じになって (共役性, conjugacy) いろいろ便利 (後述)。\n\\[ \\text{Beta}(\\mu \\mid a, b) = \\frac{\\Gamma(a + b)}{\\Gamma(a) \\Gamma(b)} \\mu^{a-1} (1 - \\mu)^{b-1} \\] ガンマ関数 のところは(積分して1になるように)正規化するための係数で、 形を決めるのは後ろの部分。 a と b はパラメータ μ の分布を決めるための 超パラメータ (hyperparameter)。\n平均と分散は (Exercise 2.6)\n\\[\\begin{aligned} \\mathrm{E}[\\mu] \u0026= \\int_0^1 \\mu \\text{Beta}(\\mu \\mid a,b)d\\mu\\\\ \u0026= \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} \\int_0^1 \\mu^{a} (1-\\mu)^{b-1} d\\mu\\\\ \u0026= \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} \\frac{\\Gamma(a+1)\\Gamma(b)}{\\Gamma(a+b+1)}\\\\ \u0026= \\frac{\\Gamma(a+b)}{\\Gamma(a)} \\frac{a\\Gamma(a)}{(a+b)\\Gamma(a+b)}\\\\ \u0026= \\frac{a}{a + b}\\\\ \\text{var}[\\mu] \u0026= \\int_0^1 \\mu^2 \\text{Beta}(\\mu \\mid a,b)d\\mu - \\mathrm{E}[\\mu]^2\\\\ \u0026= \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} \\frac{\\Gamma(a+2)\\Gamma(b)}{\\Gamma(a+b+2)} - \\mathrm{E}[\\mu]^2\\\\ \u0026= \\frac{\\Gamma(a+b)}{\\Gamma(a)} \\frac{a(a+1)\\Gamma(a)}{(a+b)(a+b+1)\\Gamma(a+b)} - \\mathrm{E}[\\mu]^2\\\\ \u0026= \\frac{a(a+1)}{(a+b)(a+b+1)} - (\\frac{a}{a+b})^2\\\\ \u0026= \\frac{ab}{(a + b)^2 (a + b + 1)} \\end{aligned}\\] Figure 2.2a, Figure 2.2b, Figure 2.2c, Figure 2.2d\nベータ分布がどんな形になるか、いろんな *a*, *b* でプロットしてみた これを二項分布の尤度関数(2.9)と掛け算して得られる事後分布の比例関係は\n\\[ p(\\mu \\mid m, N, a, b) \\propto \\mu^{m + a - 1} (1 - \\mu)^{N - m + b - 1} \\] ベータ分布と同じ形をしているので、同じように正規化できる:\n\\[\\begin{aligned} p(\\mu \\mid m, N, a, b) \u0026= \\frac{\\Gamma(N + a + b)}{\\Gamma(m + a) \\Gamma(N - m + b)} \\mu^{m + a - 1} (1 - \\mu)^{N - m + b - 1}\\\\ \u0026= \\text{Beta}(\\mu \\mid m + a, N - m + b) \\end{aligned}\\] この事後分布は、新しいデータを加えて再評価するときに事前分布として使える。\nFigure 2.3 a = 2, b = 2 の事前分布に(Figure 2.3a)、 1回投げて表が出たというデータが追加されると(Figure 2.3b)、 事後分布は a = 3, b = 2 のベータ分布になる(Figure 2.3c)。\n表の観測数 m 回、裏の観測数 N - m 回が追加されたら、 事前分布の a と b をその分それぞれ増やすだけで事後分布が得られる。 したがって超パラメータ a と b はその時点での表と裏の 有効観測数 (effective number of observations) と解釈することができる。\nsequential approach 逐次学習 観測毎に事後分布を更新していけるのでリアルタイムで利用できる。 処理し終わったデータは捨ててもよい。 コインの真の特性 μ を知るのが目的ではなく、 手元のデータを元に次のコイントスの結果を予測したいとしたら、 興味があるのは $p(\\mu)$ ではなくて $p(x = 1 \\mid D)$\n\\[\\begin{aligned} p(x = 1 \\mid D) = \\int_0^1 p(x = 1 \\mid \\mu) p(\\mu \\mid D) d\\mu = \\int_0^1 \\mu p(\\mu \\mid D) d\\mu = \\mathrm{E}[\\mu \\mid D] = \\frac{m + a}{N + a + b} \\end{aligned}\\] ベータ分布の平均値の式の a と b に観察数を足しただけ。 事後分布の平均値は常に最尤推定値と事前平均の間にあり、観察数を上げていくと最尤推定の $\\frac{m}{N}$ に近づく。 分散は小さく(事後分布はシャープに)なっていく。\nデータを生成する分布の上で事後平均を平均すると事前平均になる (Exercise 2.8):\n\\[\\begin{aligned} \\mathrm{E}_D[\\mathrm{E}_\\theta[\\theta \\mid D]] \u0026\\equiv \\int\\left\\{\\int \\theta p(\\theta \\mid D) d\\theta\\right\\}p(D)dD\\\\ \u0026= \\int\\int \\theta p(\\theta,D) d\\theta dD\\\\ \u0026= \\int\\left\\{\\int p(\\theta,D) dD \\right\\} \\theta d\\theta\\\\ \u0026= \\int p(\\theta)\\theta d\\theta \\equiv \\mathrm{E}_\\theta[\\theta] \\end{aligned}\\] 事後分散の平均と事後平均の分散を足すと事前分散になる:\n\\[\\begin{aligned} \\mathrm{E}_D[\\text{var}_\\theta[\\theta \\mid D]] + \\text{var}_D[\\mathrm{E}_\\theta[\\theta \\mid D]] \u0026= \\mathrm{E}_D[\\mathrm{E}_\\theta[\\theta^2 \\mid D] - \\mathrm{E}_\\theta[\\theta \\mid D]^2] + \\mathrm{E}_D[\\mathrm{E}_\\theta[\\theta \\mid D]^2] - \\mathrm{E}_D[\\mathrm{E}_\\theta[\\theta \\mid D]]^2\\\\ \u0026= \\mathrm{E}_D[\\mathrm{E}_\\theta[\\theta^2 \\mid D]] - \\mathrm{E}_D[\\mathrm{E}_\\theta[\\theta \\mid D]]^2\\\\ \u0026= \\mathrm{E}_\\theta[\\theta^2] - \\mathrm{E}_\\theta[\\theta]^2\\\\ \u0026= \\text{var}_\\theta[\\theta] \\end{aligned}\\] データによって例外はあるが、平均的には「事前分散 \u0026gt; 事後分散」となる。\n2.2 Multinomial Variables 多値変数 $x \\in {1, 2, \u0026hellip;, K}$ サイコロの目のように、3つ以上の値を取りうる確率変数。 e.g. $x \\in {1,2,3,4,5,6}$ 1-of-K 符号化法 長さ K のベクトルのうち $x_k$ だけが1で、そのほかが0。 例えばサイコロで3が出たらその観察値の表記は \\[ \\vec{x} = (0, 0, 1, 0, 0, 0) \\] 確率 $\\mu_k$ で $x_k = 1$ になるとすると、 サイコロを1回振るときの x の分布は以下のように表せる。\n\\[\\begin{aligned} p(\\vec{x} \\mid \\vec{\\mu}) \u0026= \\prod _{k = 1}^K {\\mu _k^{x _k}}\\\\ \\sum _{\\vec{x}} p(\\vec{x} \\mid \\vec{\\mu}) \u0026= \\sum _{k=1}^K {\\mu _k} = 1\\\\ \\mathrm{E}[\\vec{x} \\mid \\vec{\\mu}] \u0026= \\sum _{\\vec{x}} p(\\vec{x} \\mid \\vec{\\mu}) \\vec{x} = \\vec{\\mu} \\end{aligned}\\] サイコロを N 回振った観察データ D に対応する尤度関数は\n\\[\\begin{aligned} p(D \\mid \\vec{\\mu}) = \\prod_{n=1}^N \\prod_{k=1}^K \\mu_k^{x_{nk}} = \\prod_{k=1}^K \\mu_k^{\\sum_n x_{nk}} = \\prod_{k=1}^K \\mu_k^{m_k} \\end{aligned}\\] $m_k$ は N 回のうち k が出た回数。 出る順番や他の出目にはよらず、総和だけでよい、つまりこれも 十分統計量 の例。\n$\\mu_k$ の和が1になるという拘束条件の下で対数尤度を最大化する $\\mu_k$ を求めるには下記のようにラグランジュ未定乗数法を用いる。\n\\[ \\mu_k^{ML} = - \\frac{m_k}{\\lambda} = \\frac{m_k}{N} \\] 結局、観察総数 N のうちその目が出た数の割合が最尤推定値。\nラグランジュ未定係数法 (Appendix E)\n極値を求めたい関数と拘束条件をそれぞれ f, g で表すと\n\\[\\begin{aligned} df \u0026= \\frac{\\partial f}{\\partial \\mu_1} + ... + \\frac{\\partial f}{\\partial \\mu_K} = 0\\\\ dg \u0026= \\frac{\\partial g}{\\partial \\mu_1} + ... + \\frac{\\partial g}{\\partial \\mu_K} = 0 \\end{aligned}\\] \\[ \\frac{\\partial f}{\\partial \\mu_k} + \\lambda \\frac{\\partial g}{\\partial \\mu_k} = 0 \\] て感じで拘束条件のない連立方程式に置き換えられる。 今回の例では\n\\[\\begin{aligned} f(\\vec{\\mu}) = \\sum_{k=1}^K m_k \\ln \\mu_k;\\; g(\\vec{\\mu}) = \\sum_{k=1}^K \\mu_k - 1 \\end{aligned}\\] \\[\\begin{aligned} \\frac{\\partial f}{\\partial \\mu_k} + \\lambda \\frac{\\partial g}{\\partial \\mu_k} = \\frac{m_k}{\\mu_k} + \\lambda \u0026= 0\\\\ m_k + \\lambda \\mu_k \u0026= 0\\;\\therefore \\mu_k^{ML} = - \\frac{m_k}{\\lambda}\\\\ \\sum_k(m_k + \\lambda \\mu_k) \u0026= 0\\\\ N + \\lambda \u0026= 0\\\\ \\lambda \u0026= -N \\end{aligned}\\] 多項分布 (Multinomial distribution)\n観察総数とパラメータを条件とした、それぞれの出目の同時分布\n\\[\\begin{aligned} \\text{Mult}(m_1, ..., m_K \\mid \\vec{\\mu}, N) = \\binom{N}{m_1, ..., m_K} \\prod_{k=1}^{K} \\mu_k^{m_k} = \\frac{N!}{m_1! ... m_K!} \\prod_{k=1}^{K} \\mu_k^{m_k} \\end{aligned}\\] 2.2.1 Dirichlet distribution 多項分布の共役事前分布を考えよう。\n式の形からするとおそらく $\\mu_k$ の累乗にすればいいはず。 ということで、積分して1になるように正規化してみる。\n\\[\\begin{aligned} \\text{Dir}(\\vec{\\mu} \\mid \\vec{\\alpha}) = \\frac{\\Gamma(\\sum_k{\\alpha_k})}{\\Gamma(\\alpha_1)...\\Gamma(\\alpha_K)} \\prod_{k=1}^K \\mu_k^{\\alpha_k - 1} \\end{aligned}\\] Figure 2.4\n$0 \\le \\mu_k \\le 1$ かつ $\\sum_k \\mu_k = 1$ という制約下での $K$ 変数のディリクレ分布は $K – 1$ 次元の 単体 (simplex) になる。\nFigure 2.5a, Figure 2.5b, Figure 2.5c\nいろんな $α$ でのディリクレ分布。 simplexの面が水平軸方向に、密度が垂直軸になっている。\n事後分布はこれと尤度の掛け算に比例する (2.40)。 それを積分して1になるよう正規化する (2.41)。\n\\[\\begin{aligned} \\text{posterior} \u0026\\propto \\text{prior} \\times \\text{likelihood}\\\\ p(\\vec{\\mu} \\mid D, \\vec{\\alpha}) \u0026\\propto \\text{Dir}(\\vec{\\mu} \\mid \\vec{\\alpha}) \\text{Mult}(D \\mid \\vec{\\mu})\\\\ \u0026\\propto \\prod_{k=1}^K {\\mu_k^{\\alpha_k + m_k - 1}}\\\\ p(\\vec{\\mu} \\mid D, \\vec{\\alpha}) \u0026= \\frac{\\Gamma(\\sum_k{\\alpha_k} + N)}{\\Gamma(\\alpha_1 + m_1) ... \\Gamma(\\alpha_K + m_K)} \\prod_{k=1}^K {\\mu_k^{\\alpha_k + m_k - 1}}\\\\ \u0026= \\text{Dir}(\\vec{\\mu} \\mid \\vec{\\alpha} + \\vec{m}) \\end{aligned}\\] 確かに事後分布もディリクレ分布の形をしている。 K = 2 にすると二項分布・ベータ分布の話と一致。 逆に言うと、ディリクレ分布はベータ分布を一般化した多変量ベータ分布と見なせる。 超パラメータ $\\alpha_k$ はサイコロで k が出た有効観察数のように解釈できる。\nJohann Peter Gustav Lejeune Dirichlet (1805–1859)\n名前は \u0026rsquo;le jeune de Richelet (リシュレから来た若者)\u0026rsquo; に由来。 最初の論文でフェルマーの最終定理の部分的な証明をして一躍有名に。 作曲家メンデルスゾーンの妹と結婚した。\n",
  "href": "/lectures/prml-2-1.html",
  "tags": [
   "math",
   "book"
  ],
  "title": "PRML輪読会 2章前半",
  "type": "lectures"
 },
 {
  "content": " Author Christopher M. Bishop Book Pattern Recognition and Machine Learning Publisher Springer Materials http://research.microsoft.com/en-us/um/people/cmbishop/prml/ 輪読担当 岩嵜航 日程 2014-06-30 3. Linnear Models For Regression 3.1 Linear Basis Function Models: 八島さん、関口さん\n3.2 The Bias-Variance Decomposition: チャッキーさん\n3.3 Bayesian Linear Regression: 佐伯さん、永田さん\n3.4 Bayesian Model Comparison 最尤推定における過学習の問題 → 点推定じゃなくて周辺化することで回避しよう\n訓練データだけでモデルを比較できる (確認データ不要) すべてのデータを訓練に使うことができる (cross-validation不要) 複雑性のパラメータも含めて同時に決められる e.g. relevance vector machine (Chapter 7) モデルの不確実さを確率で表し、加法定理・乗法定理を駆使して評価しよう\n\\[\\begin{aligned} p(X) \u0026= \\sum^Y p(X,Y) \\\\ p(X,Y) \u0026= p(Y \\mid X) P(X) \\end{aligned}\\] 変数\n新しい入力: $\\mathbf x$\nそれに対する出力(予測したい): $t$\nモデルの中のパラメータ: $\\mathbf w$\n観察(トレーニング)データ: $\\mathcal D$\nL 個のモデル: $\\mathcal M_1, \u0026hellip;, \\mathcal M_L$\nモデルの事後分布 $p(\\mathcal M _i \\mid \\mathcal D)$ は、\n$p(\\mathcal M _i)$: どのモデルがアリかなという好み(事前分布)と、 $p(\\mathcal D \\mid \\mathcal M _i)$: そのモデルの下での観察データの出やすさ (model evidence; marginal likelihood 周辺尤度) の積に比例する (式 3.66)。\n\\[ p(\\mathcal M _i \\mid \\mathcal D) \\propto p(\\mathcal M _i) p(\\mathcal D \\mid \\mathcal M _i) \\] これを評価したいんだけど、 モデルの事前分布なんてだいたい分からないので、重要なのは後者のevidence。\nBayes factor ベイズ因子\nモデル $\\mathcal M _j$ に対する $\\mathcal M _i$ のevidence比 $\\frac {p(\\mathcal D \\mid \\mathcal M _i)} {p(\\mathcal D \\mid \\mathcal M _j)}$\nMixture distribution\nモデルの事後分布が分かれば予測分布 predictive distribution (新しい $\\mathbf x$ に対して $t$ がどんな値となるか) も加法定理と乗法定理より導かれる (式 3.67)\n\\[\\begin{aligned} p(t \\mid \\mathbf x, \\mathcal D) \u0026= \\sum _{i=1} ^L p(t, \\mathcal M _i \\mid \\mathbf x, \\mathcal D) \\\\ \u0026= \\sum _{i=1} ^L {p(t \\mid \\mathbf x, \\mathcal M _i, \\mathcal D) p(\\mathcal M _i \\mid \\mathcal D)} \\end{aligned}\\] これは、それぞれのモデルでの予測分布(入力に対してどういう出力になりそうか)を 事後分布(どのモデルっぽいか)で重み付けした平均した、混合分布。\n例えば L=2 でモデルの片方の予測が $t = a$ らへんの鋭いピーク、 もう片方のモデルの予測が $t = b$ らへんの鋭いピークだった場合、 混合分布の予測はその中点 $t = (a + b) / 2$ にピークを持つのではなく、二山になってしまう。\nModel selection\nパラメータセット $w$ を持つモデル $\\mathcal M_i$ のevidenceをまた加法定理と乗法定理でばらしてみると (式 3.68)\n\\[\\begin{aligned} p(\\mathcal D \\mid \\mathcal M _i) \u0026= \\int p(\\mathcal D, \\mathbf w \\mid \\mathcal M _i) \\mathrm d \\mathbf w \\\\ \u0026= \\int p(\\mathcal D \\mid \\mathbf w, \\mathcal M _i) p(\\mathbf w \\mid \\mathcal M _i) \\mathrm d \\mathbf w \\end{aligned}\\] パラメータセットの尤度をその確率分布で重み付けして積分したもの、 ってことで周辺尤度と呼ばれるのが納得できる。 また、そのモデルからデータセットが生成される確率 (ただしパラメータは事前分布からランダムに取ったもの) とも理解できる。 この $p(\\mathbf w \\mid \\mathcal M_i)$ はモデルで想定してる何らかの事前分布ってことでいいのかな？\n積分の中身からすると、パラメータの事後分布を求める式の正規化項になる (式 3.69)\n\\[ p(\\mathbf w \\mid \\mathcal D, \\mathcal M _i) = \\frac {p(\\mathcal D \\mid \\mathbf w, \\mathcal M _i) p(\\mathbf w \\mid \\mathcal M _i)} {p(\\mathcal D \\mid \\mathcal M _i)} \\] あるひとつのパラメータ $w$ を持つモデルを考える。\nFigure 3.12 近似\nパラメータ $w$ の事前分布(青)と、それよりシャープな事後分布(赤)。 MAP推定値らへんで長方形に分布してるものとして近似。 Figure 3.12 のように近似すると式3.68の積分をただの掛け算で書き変えられる (モデル依存の表記を省略, 式 3.70, 3.71)。\n\\[\\begin{aligned} p(\\mathcal D) \u0026= \\int p(\\mathcal D \\mid w) p (w) \\mathrm dw \\\\ \u0026\\simeq \\frac 1 {\\Delta w _\\text{prior}} \\int p(\\mathcal D \\mid w) \\mathrm dw \\\\ \u0026\\simeq \\frac 1 {\\Delta w _\\text{prior}} p(\\mathcal D \\mid w _\\text{MAP}) \\Delta w _\\text{posterior} \\\\ \u0026= p(\\mathcal D \\mid w _\\text{MAP}) \\frac {\\Delta w _\\text{posterior}} {\\Delta w _\\text{prior}} \\\\ \\ln p(\\mathcal D) \u0026\\simeq \\ln p(\\mathcal D \\mid w _\\text{MAP}) + \\ln \\left( \\frac {\\Delta w _\\text{posterior}} {\\Delta w _\\text{prior}} \\right) \\end{aligned}\\] 第一項は一番いいパラメータの当てはまりの良さ、 第二項はモデルの複雑性によるペナルティ (事後分布の幅が狭くなるほど大きな負になる)。\n$M$ 個のパラメータを持つモデルを考える。 事前分布と事後分布の幅の比が全てのパラメータで等しいとすると (式 3.72)\n\\[\\begin{aligned} p(\\mathcal D) \u0026= p(\\mathcal D \\mid w _\\text{MAP}) \\left(\\frac {\\Delta w _\\text{posterior}} {\\Delta w _\\text{prior}} \\right)^M \\\\ \\ln p(\\mathcal D) \u0026\\simeq \\ln p(\\mathcal D \\mid w _\\text{MAP}) + M \\ln \\left( \\frac {\\Delta w _\\text{posterior}} {\\Delta w _\\text{prior}} \\right) \\end{aligned}\\] パラメータが増える(モデルの複雑性が増す)ごとに第一項は大きくなっていくかもしれないが、 第二項のペナルティも大きな負になっていく。 中程度が良さそう → 過学習しない！\n長方形じゃなくてもっとちゃんとしたGaussian近似をSection 4.4.1で\nFigure 3.13 どうして中程度の複雑性のモデルが好まれるか\n横軸はデータセットが取りうる値を1次元で表現。 モデルの複雑性を $\\mathcal M _1 \u003c \\mathcal M _2 \u003c \\mathcal M _3$ とする。 シンプルなモデル $\\mathcal M _1$ は生成(説明)できるデータの範囲が狭く (いろいろパラメータを変えても似通ったデータセットしか出てこない)、 複雑なモデル $\\mathcal M _3$ はいろんなデータを生成できるがそれぞれの重みは低い。 特定のデータセット $\\mathcal D _0$ に対しては中程度の複雑さを持つモデル $\\mathcal M _2$ が一番大きいevidenceを持つことになる。\nExpected Bayes factor\n$\\mathcal M_1$ が真のモデルだとする。 ベイズ因子は個々のデータで見ると 正しくない $\\mathcal M_2$ とかで大きくなる場合もあるが、 真の分布の上でを平均すると (式 3.73)\n\\[ \\int p(\\mathcal D \\mid \\mathcal M _1) \\ln \\frac {p(\\mathcal D \\mid \\mathcal M _1)} {p(\\mathcal D \\mid \\mathcal M _2)} \\mathrm d \\mathcal D \\] で Kullback-Leibler divergence (Section 1.6.1 式 1.113) と同じ形になり（対数の中身と符号を入れ替え）、 常に正（ただし2つの分布が等しい場合は0）の値をとることが分かっているので、 平均的には正しいモデルのベイズ因子が大きくなり、好まれる。 ただし、データを生成する真の分布が L 個のモデルの中に含まれてれば、の話。\nまとめ\nBayesian frameworkでは過学習を避け、訓練データだけでモデル比較できる でもモデルの形に関する仮定は必要で、それが正しくないと誤った結論を導きうる 結論は事前分布の特性にかなり依存 非正則事前分布では正規化定数が定義できないためevidenceを定義できない じゃあ正則事前分布の極限(e.g. 分散∞の正規分布)をとればいいかというと、 それではevidenceが0に収束してしまう 先に2つのモデルのevidence比を取ってから極限をとるといいかも 実際の応用では独立なテストデータを評価用に取っとくのが賢明 (←え、結局？) ",
  "href": "/lectures/prml-3-4.html",
  "tags": [
   "math",
   "book"
  ],
  "title": "PRML輪読会 3章4節",
  "type": "lectures"
 },
 {
  "content": "標準・公式 MacやLinuxならシステムの一部として /usr/bin/python が既にインストールされているので、 何もしなくても使えっちゃ使える。 でも大概そこに入ってるのは古い2.7とかなので、 ちゃんと使える3.xを使いたければ python.jp/環境構築ガイド に従って最新版を入れるのが簡単。\npyenv 管理者権限なしでホーム以下にインストールするには pyenv が便利。\nHomebrew か Git を使ってpyenvをインストール:\nbrew install pyenv # or git clone https://github.com/pyenv/pyenv.git ~/.pyenv mkdir -p ~/.pyenv/cache Pythonのインストール先を決める環境変数 PYENV_ROOT を公式推奨の ~/.pyenv に設定し、 ついでにPATHも追加しておく。 シェルの設定ファイル (e.g., ~/.bashrc) に次のように追記:\nexport PYENV_ROOT=${HOME}/.pyenv if [ -d $PYENV_ROOT ]; then pyenv_versions=($(ls ${PYENV_ROOT}/versions | sort -V)) export PYENV_LATEST=${PYENV_ROOT}/versions/${pyenv_versions[@]: -1} PATH=${PYENV_LATEST}/bin:$PATH unset pyenv_versions fi pyenv shell や pyenv local を使ってPythonのバージョンを頻繁に切り替える場合は、 公式の説明どおりに eval \u0026quot;$(pyenv init --path)\u0026quot; や eval \u0026quot;$(pyenv init -)\u0026quot; を設定してshimsを使う方法のほうがいいかもしれないけど、 そうでなければこのようにPATHだけ設定するほうが単純で、 起動時間も短くなる。\nシェルを再起動して設定を反映し、 目当てのバージョンを探してインストール:\nexec $SHELL -l pyenv install -l | less pyenv install 3.10.4 exec $SHELL -l R から reticulate 越しに呼ぶ場合は共有ライブラリを有効にしてビルドする:\nenv PYTHON_CONFIGURE_OPTS=\u0026#34;--enable-shared\u0026#34; pyenv install 3.7.13 # or Rscript -e \u0026#39;reticulate::install_python(\u0026#34;3.7.13\u0026#34;)\u0026#39; pip のパスを確認し、パッケージを入れる:\nwhich pip3 pip3 install -U setuptools pip wheel pip3 install -r /path/to/requirements.txt よく使うパッケージは requirements.txt の形でまとめておくと楽。\n既知の問題 https://github.com/pyenv/pyenv/wiki/Common-build-problems\n3.1.0より古いmatplotlib で macosx backend を使いたい場合などは環境変数 PYTHON_CONFIGURE_OPTS=\u0026quot;--enable-framework\u0026quot; をセットしてFramework型でビルドする。\nMojaveで \u0026ldquo;zlib not available\u0026rdquo; と怒られる問題は CFLAGS=\u0026quot;-I$(xcrun --show-sdk-path)/usr/include\u0026quot; を定義して回避。\nPEP 394 になるべく沿うように、 python では /usr/bin/python が呼び出される状態を維持しつつ、 python3, pip3 を明示的に使うようにしたい。 が、いまのところできない？\nAnaconda Scientificな用途で使いたい場合は Numpy/Scipy などの主要パッケージもまとめて面倒みてくれる Anaconda を使うという選択肢もある。 私は使わない。 GUIのインストーラでもいいし、Homebrewでも入れられる:\nbrew install anaconda export PATH=/usr/local/anaconda3/bin:\u0026#34;$PATH\u0026#34; ただしPATH上でシステムコマンドを上書きしちゃうヤンチャな面もあるので、 それが気になる人はpyenv越しに入れることで汚染をある程度防げる。 全部入りに抵抗がある場合は pyenv install miniconda3-latest から小さくスタートすることも可能。 パッケージ管理では pip の代わりに非公式の conda を使うことになる。\n環境変数 https://docs.python.org/3/using/cmdline.html#environment-variables\nシェルの設定ファイル(~/.zshrc とか)で export しておく。 一時的に無効したいときは python -E で起動。\nPYTHONPATH import の探索パス (sys.path) の先頭付近に場所を追加できる。 例えば自分で書いたモジュールやパッケージの置き場所を指定しておけば、 いつでも優先的に import できるようになる。\nPYTHONUSERBASE pip install や setup.py install における --user オプションの目的地を指定できる。 デフォルトでは ${HOME}/.local 。 MacのFramework buildでは ${HOME}/Library/Python/2.7 とかになる。\n現在の設定は site の USER_BASE で確認できる (python -m site)。 USER_SITE はその下の {BASE}/lib/python*.*/site-packages に配置され、 sys.path に含まれる。 除外したいときは PYTHONNOUSERSITE をセットするか python -s で起動。\nPYTHONSTARTUP インタラクティブモードで起動するときに読み込むファイルを指定できる。\n関連書籍 ",
  "href": "/python/install.html",
  "tags": [
   "python"
  ],
  "title": "Pythonインストール",
  "type": "python"
 },
 {
  "content": " https://docs.python.org/3/tutorial/modules.html https://docs.python.org/3/reference/import.html https://packaging.python.org/ ファイル構成 GitHubやローカルの開発環境から pip で直接インストールできる形。\npywtl/ ├── LICENSE ├── README.md ├── pyproject.toml ├── src/wtl/ │ ├── __init__.py │ └── hello.py └── tests/ リポジトリ名(pywtl)とパッケージ名(wtl)は必ずしも一致してなくてもよい。\nソースコードは src の中に入れる流派と、ルート直下に置く流派がある。 落とし穴が少なくて推奨されているのは前者。\n開発向けの -e,--editable オプションをつけたローカルインストールではコピーが起こらず、 編集後に再インストールしなくてもそのまま反映される。\npip3 install -v --user -e ~/git/pywtl/ python3 -m wtl.hello python3 -m site pyproject.toml パッケージ作成に関わる全てのメタ情報を書いておくファイル。 setuptools に依存しない形式として PEP 517, PEP 621 で決められた。 過去によく使われていた setup.py, setup.cfg, MANIFEST.in などは非推奨になった。\nPyPA/Flit (setuptools後継？), PDM, Poetry, など後発のツールは早くから対応していて、 setuptools もようやく61.0から使えるようになった。 [project] テーブルは PEP 621 で項目が決められているためツールによらず共通。 それ以外の [build-system] などは使うツールによって異なる。\n[build-system] requires = [\u0026#34;flit_core \u0026gt;=3.4,\u0026lt;4\u0026#34;] build-backend = \u0026#34;flit_core.buildapi\u0026#34; [project] name = \u0026#34;wtl\u0026#34; authors = [ {name = \u0026#34;Watal M. Iwasaki\u0026#34;, email = \u0026#34;heavywatal@gmail.com\u0026#34;} ] license = {file = \u0026#34;LICENSE\u0026#34;} readme = \u0026#34;README.md\u0026#34; dynamic = [\u0026#34;description\u0026#34;, \u0026#34;version\u0026#34;] classifiers = [ \u0026#34;Development Status :: 2 - Pre-Alpha\u0026#34;, \u0026#34;Environment :: Console\u0026#34;, \u0026#34;Intended Audience :: Science/Research\u0026#34;, \u0026#34;License :: OSI Approved :: MIT License\u0026#34;, \u0026#34;Topic :: Scientific/Engineering :: Bio-Informatics\u0026#34;, ] requires-python = \u0026#34;\u0026gt;=3.11\u0026#34; dependencies = [ \u0026#34;tomli-w\u0026#34;, ] [project.optional-dependencies] dev = [ \u0026#34;black\u0026#34;, \u0026#34;pytest\u0026#34;, \u0026#34;pytest-cov\u0026#34;, \u0026#34;ruff\u0026#34;, ] [project.urls] Source = \u0026#34;https://github.com/heavywatal/pywtl\u0026#34; [project.scripts] \u0026#34;hello.py\u0026#34; = \u0026#34;wtl.hello:main\u0026#34; [tool.pyright] typeCheckingMode = \u0026#34;strict\u0026#34; [tool.ruff] select = [\u0026#34;E\u0026#34;, \u0026#34;F\u0026#34;, \u0026#34;W\u0026#34;, \u0026#34;I\u0026#34;] [tool.pytest.ini_options] pythonpath = [\u0026#34;src\u0026#34;] testpaths = [\u0026#34;tests\u0026#34;] [tool.coverage.run] source = [\u0026#34;src\u0026#34;] [tool.coverage.report] exclude_also = [ \u0026#34;if __name__ == .__main__.:\u0026#34;, ] dynamic に指定したものは __init__.py に __version__ = \u0026quot;0.1.0\u0026quot; などと書いてあるものを参照できる。 比較したいときは packaging.version.parse() を利用する。\ndependencies に列挙された依存パッケージは pip3 install で自動的にインストールされる。 一方 requirements.txt はインストール過程には関与せず、 能動的に pip3 install -r requirements.txt を打たなきゃインストールされない。\noptional dependencies もインストールしたい場合は pip3 install -v -e .[dev] のように [key] を使ってパッケージを指定する。\nproject.scripts で設定したものは ${prefix}/bin/ に実行可能ファイルが配置される。 以前は console_scripts で設定していた。\nコード整形やテストのような各種開発ツールの設定も [tool.***] に記述できる。 linterとしては pyproject.toml 対応拒否のflake8 を捨てて超高速Rust製ruffを使う。\nソースコード wtl/__init__.py このディレクトリがひとつのパッケージであることを示すファイル。 空でもいいし、初期化処理やオブジェクトを記述してもよい。 文字列変数 __version__ = \u0026quot;0.1.2\u0026quot; を定義して wtl.__verison__ のように参照できるようにしておくのが慣例。 wtl/hello.py\n\u0026#34;\u0026#34;\u0026#34;Simple module to say hello \u0026#34;\u0026#34;\u0026#34; import getpass def main(): print(\u0026#34;Hello, \u0026#34; + getpass.getuser() + \u0026#34;!\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() ソースツリーの中にあるファイルを参照するには importlib.resources が使える。 Pythonスクリプトではない設定ファイルなどを同梱して読み込むのに便利。\n",
  "href": "/python/packaging.html",
  "tags": [
   "python",
   "package"
  ],
  "title": "Pythonパッケージ作成",
  "type": "python"
 },
 {
  "content": "Finder上で Space を押すだけで、 いろんなファイルを開かずに覗き見ることができる。 /Library/QuickLook/ または ~/Library/QuickLook/ 以下にプラグインを配置することで、機能を拡張できる。 Homebrew で入れるのが楽ちん。\nPlugins BetterZipQL.qlgenerator: 圧縮ファイルを展開せずに一覧表示 Suspicious Package.qlgenerator: pkgからインストールする前にの中身をチェックできるように QuickLookCSV.qlgenerator: CSVファイルをセルで表示 QLStephen.qlgenerator: READMEやMakefileのような拡張子無しのファイルを表示 Syntax Highlight or QLColorCode.qlgenerator: さまざまなソースコードを色分け表示。 FigTree: NEXUSあるいはNEWICK形式のtreeファイルから系統樹を描く Commands brew install qlcommonmark syntax-highlight qlstephen quicklook-csv webpquicklook betterzip suspicious-package # cannot be opened because the developer cannot be verified. xattr ~/Library/QuickLook/QuickLookCSV.qlgenerator # コマンドラインから利用 qlmanage -p some.csv # キャッシュ削除してプラグイン再読込: qlmanage -r cache \u0026amp;\u0026amp; qlmanage -r killall Finder Dock # 認識されているプラグインを一覧表示 qlmanage -m ",
  "href": "/mac/quicklook.html",
  "tags": [
   "mac"
  ],
  "title": "QuickLook",
  "type": "mac"
 },
 {
  "content": "プログラムの書き方によって速度やメモリ効率は大きく変わる。 Rでは大抵、生のforループを避けて、R標準のベクトル演算やちゃんとしたパッケージの関数を使っていれば大丈夫。 でも、どうしても、さらに速度を追い求めたい場合にはRcppが有用となる。\n長さnの調和級数を求める例:\nr_for = function(n) { s = 0; for (i in seq_len(n)) {s = s + 1 / i}; s } r_vec = function(n) sum(1 / seq_len(n)) Rcpp::cppFunction(\u0026#34;double rcpp(int n) { double s = 0; for (int i = 1; i \u0026lt;= n; ++i) {s += 1.0 / i;} return s; }\u0026#34;) # Compilation takes a few seconds here n = 1000000L rbenchmark::benchmark(r_for(n), r_vec(n), rcpp(n))[,1:4] # test replications elapsed relative # 1 r_for(n) 100 3.968 29.835 # 2 r_vec(n) 100 0.473 3.556 # 3 rcpp(n) 100 0.133 1.000 Documentation Project Home: http://www.rcpp.org/ CRAN: https://cran.r-project.org/package=Rcpp Rcpp-JSS-2011.pdf: 原典。 Rcpp-introduction.pdf: なぜRcppを使うのか。 Rcpp-attributes.pdf 現在主流となっているRcppの使い方全般。 \u0026ldquo;Rcpp Attributes\u0026rdquo; という名前がついていて、 \u0026ldquo;inline\u0026rdquo; という古いパッケージのやり方を置き換えたらしい。 Rcpp-package.pdf: 自作RパッケージでRcppを使う。 Rcpp-extending.pdf: C++クラスから既存のR型へ、またその逆の変換。 Rcpp-modules.pdf: C++クラスをRC/S4としてRにexposeする。 Rcpp-sugar.pdf: ベクトル化とlazy評価が効くRの記法をC++側で使う。 Rcpp-quickref.pdf Rcpp-FAQ.pdf GitHub: https://github.com/RcppCore/Rcpp 上のPDFは分量が多いわりに意外と網羅的ではない。 ざっくり読んでなんとなく分かってきたら、 さらなるドキュメントを求めてネットの海を彷徨うよりソースコードに当たったほうが早い。特に inst/unitTests はかなり参考になる。 API: http://dirk.eddelbuettel.com/code/rcpp/html/ Advanced R: Rewriting R code in C++ みんなのRcpp and Rcpp for everyone by 津駄@teuderさん Rスクリプトの途中で使う ファイルあるいは文字列をコンパイルして使う:\nRcpp::sourceCpp(\u0026#34;fibonacci.cpp\u0026#34;) Rcpp::sourceCpp(code=\u0026#39; #include \u0026lt;Rcpp.h\u0026gt; // [[Rcpp::plugins(cpp14)]] // [[Rcpp::export]] int fibonacci(const int x) { if (x \u0026lt; 1) return 0; if (x == 1) return 1; return fibonacci(x - 1) + fibonacci(x - 2); } \u0026#39;) fibonacci(9L) # [1] 34 いろいろな準備を任せて、関数をひとつだけ定義するショートカット:\nRcpp::cppFunction(plugins = c(\u0026#34;cpp14\u0026#34;), \u0026#39; int fibonacci(const int x) { if (x \u0026lt; 1) return 0; if (x == 1) return 1; return fibonacci(x - 1) + fibonacci(x - 2); } \u0026#39;) fibonacci(9L) # [1] 34 Rパッケージで使う Rcpp-package.pdf by Dirk Eddelbuettel and Romain François tidyverseではcpp11を使うようになったので R Packages (Wickham and Bryan) におけるRcppの扱いは小さい。 準備手順 まずRcppコード以外の部分を作っておく。 devtoolsページを参照。\nusethis::use_rcpp() を実行して設定を整える。 DESCRIPTION や src/.gitignore などが書き換えられる。\nR/***-package.R など適当なとこに @useDynLib の設定を追加:\n#\u0026#39; @useDynLib hello, .registration = TRUE #\u0026#39; @importFrom Rcpp sourceCpp #\u0026#39; @keywords internal \u0026#34;_PACKAGE\u0026#34; @importFrom Rcpp sourceCpp を省くと、パッケージ利用時に 'enterRNGScope' not provided by package 'Rcpp' のようなエラーが出る場合がある (明示的に library(Rcpp) するなどして既にRcppロード済みの環境では動く)。\n同じところに .onUnload も定義しておく:\n.onUnload = function(libpath) { library.dynam.unload(\u0026#34;hello\u0026#34;, libpath) } すると unloadNamespace(\u0026quot;hello\u0026quot;) したときに共有ライブラリもちゃんと外れるようになる。 ちなみに devtools::unload() はこれを省略してもちゃんとリロードしてくれる。\n外部ライブラリのリンクに関する設定など、 開発者側で指定すべきビルドオプションは src/Makevars に指定:\nCXX_STD=CXX14 PKG_CPPFLAGS=-DSTRICT_R_HEADERS -I/usr/local/include PKG_LIBS=-L/usr/local/lib -Wl,-rpath,/usr/local/lib -lthankyou STRICT_R_HEADERS を定義しておくことで余計なマクロ定義を防げる。 configure や CMake を使って src/Makevars.in から生成する手もある。 configure や cleanup といったスクリプトはbash拡張を含まない /bin/sh で実行可能じゃなきゃいけないらしいので、 checkbashisms をインストールしてチェックすることが求められる (brew install checkbashisms)。\nCXX_STD=CXX14 が存在しない場合は DESCRIPTION の SystemRequirements: C++14 が参照されるので、 ほかに src/Makevars を使う用が無い場合はそっちで指定するのが楽。\n参考: Japan.R 2018 LT \u0026ldquo;Rcppパッケージで外部C++ライブラリを使う\u0026rdquo;\nどうしてもユーザ側で指定すべきオプションがある場合は ~/.R/Makevars に書いてもらう。 例えばMPI依存パッケージをmacOSでビルドでしようとすると clang: error: unsupported option '-fopenmp' と怒られるので brew install llvm で別のコンパイラを入れて下記のように指定する:\nLLVM_LOC=/usr/local/opt/llvm CC=$(LLVM_LOC)/bin/clang CXX=$(LLVM_LOC)/bin/clang++ src/ 以下にソースコードを書く。\nソースコード src/*.cpp #include \u0026lt;Rcpp.h\u0026gt; //\u0026#39; First example //\u0026#39; @param args string vector //\u0026#39; @export // [[Rcpp::export]] int len(const std::vector\u0026lt;std::string\u0026gt;\u0026amp; args) { return args.size(); } std::abort() や std::exit() は呼び出したRセッションまで殺してしまう。 例外は投げっぱなしで拾わなくても大丈夫で、 std::exceptionの派生クラスならwhat()まで表示してもらえる。 グローバル変数やクラスのstaticメンバは dyn.unload() されるまで生き続ける。 parallel::mclapply() とかでフォークした先での変更は子同士にも親にも影響しない。 標準出力をRのコンソールに正しく流すには std::cout じゃなくて Rcpp::Rcout を使うべし。 とのことなんだけど、その仕事を担ってるのは中のストリームバッファのほうなので、 rdbuf() を使ってバッファを差し替えれば Rcout のガワは実は必要ない。 時間があればそのへんの提案と実装をちゃんと送りたいけど\u0026hellip; https://github.com/RcppCore/Rcpp/pull/918 詳細 アタリがついてる場合は namespace Rcpp とかからブラウザのページ内検索で探すのが早い。\nRcppで楽ができるとはいえ、R本体の内部情報も知っておいたほうがいい。 C++から直接 R API に触れるべきではないという意見もあって、 R-develで議論になっている。 R APIの例外処理で longjmp が多用されているため、 RAIIを期待したC++コードはデストラクタが呼ばれなくてバグる危険性が高い、 というのがひとつ大きな問題らしい。 ちゃんと理解しないうちは Rinternals.h の中身を直接呼ぶのは避けて、 Rcpp:: 名前空間内のC++関数だけを使うようにするのがとりあえず安全。\nhttps://github.com/hadley/r-internals https://github.com/wch/r-source/blob/trunk/src/include/Rinternals.h https://cran.r-project.org/doc/manuals/r-release/R-ints.html https://cran.r-project.org/doc/manuals/r-release/R-exts.html 型 SEXP: S Expression Rのあらゆるオブジェクトを表すC言語上の型。 メモリアロケーションやgcへの指示 (PROTECT/UNPROTECT など) が必要。 そういうことは Rcpp が肩代わりしてくれるので基本的には直接触らない。 Rcpp::RObject SEXP の thin wrapper であり Rcpp から R の変数を扱う際の基本クラス。 メモリ開放のタイミングは依然としてgc次第なものの、 コード上ではRAIIのような感覚で気楽に使える。 Rcpp::Vector\u0026lt;T\u0026gt; vector/instantiation.h 抜粋: typedef Vector\u0026lt;LGLSXP\u0026gt; LogicalVector; typedef Vector\u0026lt;INTSXP\u0026gt; IntegerVector; typedef Vector\u0026lt;REALSXP\u0026gt; NumericVector; // DoubleVector typedef Vector\u0026lt;STRSXP\u0026gt; StringVector; // CharacterVector typedef Vector\u0026lt;VECSXP\u0026gt; List; // GenericVector クラスのメンバとして生の配列ではなくそこへの参照を保持する。 しかし std::vector とは異なり、 このオブジェクトをコピーしてもメモリ上の中身はコピーされず、 ふたつとも同じ生配列を参照する。\nC++関数がRから呼ばれるとき Rcpp::Vector\u0026lt;\u0026gt; 受け取りの場合はうまく参照渡しになるが、 const std::vector\u0026lt;\u0026gt;\u0026amp; 受け取りの場合はコピーが発生する。\nRcpp::DataFrame Rの上では強力だけどC++内では扱いにくい。 出力として使うだけに留めるのが無難。 関数オーバーロードもテンプレートもそのままRにexportすることはできない。 実行時の型情報で振り分ける関数で包んでexportする必要がある。 http://gallery.rcpp.org/articles/rcpp-return-macros/\nタグ [[Rcpp::export]] これがついてるグローバル関数は RcppExport.cpp を介してライブラリに登録され、 .Call(`_{PACKAGE}_{FUNCTION}`) のような形でRから呼び出せる様になる。 それを元の名前で行えるような関数も RcppExport.R に自動で定義してもらえる。 [[Rcpp::export(\u0026quot;.new_name_here\u0026quot;)]] のように名前を変更することもできる。 ドットで始まる名前にしておけば load_all(export_all=TRUE) の状態での名前空間汚染を多少調整できる。 Rパッケージの NAMESPACE における export() とは別物。 [[Rcpp::plugins(cpp14)]] たぶん sourceCpp() とか cppFunction() で使うための機能で、 パッケージ作りでは効かない。 ほかに利用可能なものはソースコード R/Attributes.R に書いてある。 [[Rcpp::depends(RcppArmadillo)]] ほかのパッケージへの依存性を宣言。 たぶんビルド時のオプションをうまくやってくれる。 #include は自分で。 [[Rcpp::interfaces(r,cpp)]] Rcpp::export するとき、どの言語向けにいろいろ生成するか。 何も指定しなければ r のみ。 cpp を指定すると、ほかのパッケージから Rcpp::depends できるようにヘッダーを用意してくれたりするらしい。 [[Rcpp::init]] これがついてる関数はパッケージロード時に実行される。 [[Rcpp::internal]]\n[[Rcpp::register]]\n自作C/C++クラスをRで使えるようにする Rcpp::XPtr\u0026lt;T\u0026gt; に持たせてlistか何かに入れるか、 \u0026ldquo;Rcpp Modules\u0026rdquo; の機能でRC/S4の定義を自動生成してもらう。 ここで説明するのは後者。 Moduleの記述を自分でやらず Rcpp::exposeClass() に生成してもらう手もある。\nRcppExports.cpp に自動的に読み込んでもらえるヘッダー (e.g., src/{packagename}_types.h) で自作クラスの宣言と Rcpp::as\u0026lt;MyClass\u0026gt;() / Rcpp::wrap\u0026lt;MyClass\u0026gt;() の特殊化を行う。\n#include \u0026lt;RcppCommon.h\u0026gt; RCPP_EXPOSED_CLASS(MyClass); // これで as\u0026lt;MyClass\u0026gt; / wrap\u0026lt;MyClass\u0026gt; の特殊化が定義される // 必ず #include \u0026lt;Rcpp.h\u0026gt; より前に来るように #include \u0026#34;myclass.hpp\u0026#34; // 自作クラスの宣言 どこかのソースファイルでモジュールを定義\n#include \u0026lt;Rcpp.h\u0026gt;` RCPP_MODULE(mymodule) { Rcpp::class_\u0026lt;MyClass\u0026gt;(\u0026#34;MyClass\u0026#34;) .constructor\u0026lt;int\u0026gt;() .const_method(\u0026#34;get_x\u0026#34;, \u0026amp;MyClass::get_x) ; } zzz.R でモジュールを読み込む。 関数やクラスを全てそのまま公開するか、 Module オブジェクト越しにアクセスさせるようにするか。\nRcpp::loadModule(\u0026#34;mymodule\u0026#34;, TRUE)` # obj = MyClass$new(42L) modulename = Rcpp::Module(\u0026#34;mymodule\u0026#34;) # obj = mymodule$MyClass$new(42L) 場所は {packagename}-package.R とかでもいいけど読まれる順序が重要。 setClass(\u0026quot;Rcpp_MyClass\u0026quot;) を書く場合にはそれより後で読まれるようにしないと devtools::load_all() や devtools::test() などリロード後のオブジェクト生成でエラーになる: trying to generate an object from a virtual class\nパッケージを読み込むといくつかのRC/S4クラスが定義される。\nRcpp_MyClass C++Object を継承した Reference Class (RC)。 S4メソッドをカスタマイズするには明示的に setClass(\u0026quot;Rcpp_MyClass\u0026quot;) したうえで setMethod(\u0026quot;show\u0026quot;, \u0026quot;Rcpp_MyClass\u0026quot;, \\(obj) {}) などとしていく。 C++Object R上でC++オブジェクトを扱うための親S4クラス。 Rコンソール上での表示はこれの show() メソッドがデフォルトで利用される。 C++ object \u0026lt;0x7fd58cfd2f20\u0026gt; of class 'MyClass' \u0026lt;0x7fd59409d1d0\u0026gt; C++Class コンストラクタをR側にexposeするためのクラスで、 MyClass$new(...) のようにして新規オブジェクトを生成する。 ただしデフォルト引数を扱えないのでファクトリ関数を普通に [[Rcpp::Export]] したほうが簡単かも。 staticメソッドも同様に扱えれば一貫性があったんだけど今のところ無理そう。 C++Function としてならexposeできる。 C++Function わざわざModule機能でexposeした関数を扱うS4。 普通に [[Rcpp::Export]] する場合と比べたメリットは？ Module environment を継承したS4。 RC/S4関連文献\n?setRefClass or https://stat.ethz.ch/R-manual/R-devel/library/methods/html/refClass.html https://adv-r.hadley.nz/s4.html http://adv-r.had.co.nz/OO-essentials.html#rc 問題点 Rcpp ModulesはRCのメソッドにデフォルト引数を持たせることができない。 元のC++クラスのメソッドにデフォルト引数があっても無視。 パッケージロード後、例えば .onAttach() の中で MyClass::methods(fun = ...) などとしてR関数としてメソッドを定義することは可能ではある。 でもそれだと print(MyClass) の表示にも追加されず obj$ からの補完候補にも挙がらない。\nReference Class はドキュメントを書きにくい。 個々のメソッドの冒頭で書くdocstringは MyClass$help(\u0026quot;some_method\u0026quot;) のようにして確認できるが man/*.Rd を生成しない。 Roxygenもほとんど助けてくれない。 この状況はR6でもほぼ同じ。 あんまり需要ないのかな。。。\n結局、オブジェクトを第一引数にとるラッパーR関数をすべてのメソッドに用意して、 そいつにRoxygenコメントを書くのが現状の最適解か。 オブジェクトを第一引数に取るグローバルC++関数を Rcpp::Export する手もあって、 そっちのほうがソースコードの冗長性も低く抑えられるけど、 なぜか呼び出しコストが10µs, 2KBくらい余分にかかる。 この時間は namespace:: 有り無しの差と同じくらい。\nマクロ http://dirk.eddelbuettel.com/code/rcpp/html/module_8h.html\nRCPP_EXPOSED_AS(MyClass) as\u0026lt;MyClass\u0026gt; を定義してくれるマクロ。参照型やポインタ型もやってくれる。 RCPP_EXPOSED_WRAP(MyClass) wrap\u0026lt;MyClass\u0026gt; を定義してくれるマクロ。 RCPP_EXPOSED_CLASS_NODECL(MyClass) 上の2つを同時にやってくれるショートカット。 RCPP_EXPOSED_CLASS(MyClass) それらの前にさらに class MyClass; の前方宣言もする。 ",
  "href": "/rstats/rcpp.html",
  "tags": [
   "r",
   "c++",
   "package"
  ],
  "title": "Rcpp",
  "type": "rstats"
 },
 {
  "content": " タブ区切りテキストやCSVファイルを読み込んでdata.frameにするツール。 .gz や .xz などの圧縮ファイルも透過的に読み書き可能。 標準でも read.table() や read.csv() があるけど、それらと比べて\n場合により数倍高速・省メモリ 列の名前や型を指定しやすい 指定した列だけ読み込むこともできる 生data.frameより安全な tibble として返してくれる 空白行を勝手にスキップする (1.2から skip_empty_rows = TRUE) 勝手に列名を変更しない する (2.0から name_repair = \u0026quot;unique\u0026quot;) stringsAsFactors = FALSE とイチイチ書かなくて文字列を読める R 4.0 から標準関数もこの挙動。 tidyverse に含まれているので、 install.packages(\u0026quot;tidyverse\u0026quot;) で一括インストール、 library(tidyverse) で一括ロード。 例えば:\nlibrary(conflicted) library(tidyverse) write_tsv(diamonds, \u0026#34;diamonds.tsv.gz\u0026#34;) read_tsv(\u0026#34;diamonds.tsv.gz\u0026#34;) https://r4ds.hadley.nz/data-import.html https://cran.r-project.org/package=readr 主な関数 ファイル読み込み read_delim(file, delim, quote = \u0026#39;\u0026#34;\u0026#39;, escape_backslash = FALSE, escape_double = TRUE, col_names = TRUE, col_types = NULL, col_select = NULL, id = NULL, locale = default_locale(), na = c(\u0026#34;\u0026#34;, \u0026#34;NA\u0026#34;), quoted_na = TRUE, comment = \u0026#34;\u0026#34;, trim_ws = FALSE, skip = 0, n_max = Inf, guess_max = min(1000, n_max), name_repair = \u0026#34;unique\u0026#34;, num_threads = readr_threads(), progress = show_progress(), show_col_types = should_show_types(), skip_empty_rows = TRUE, lazy = TRUE) read_csv(...) や read_tsv(...) は区切り文字 delim = 指定済みのショートカット。\nread_table(...) 連続する空白文字をひとつの区切りと見なして処理 read_fwf(file, col_positions, ...) fixed width file. 第二引数の指定方法は fwf_empty(infile, skip = 0, col_names = NULL) で自動推定 fwf_widths(widths, col_names = NULL) で幅指定 fwf_positions(start, end, col_names = NULL) で開始・終了位置指定 read_lines(file, skip = 0, n_max = -1L, ...), read_lines_raw(...) 1行を1要素とした文字列ベクタとして読み込む read_file(file) ファイルの内容まるごと文字列で返す 同じ形式の複数ファイルをvectorで渡せば、順に読み込んで rbind() してくれる。 purrr::map(file, readr::read_csv) |\u0026gt; purrr::list_rbind(names_to = \u0026quot;file\u0026quot;) と同等だが、それよりも少し高速で、列の型推定もうまくいきやすい。 ただし names_to = のように元のファイルとの関連を残すオプションは無い。\nファイルの中身を文字列として渡すことも可能。 自動で判別してもらえるけど I() で包むのが確実。\ncontent = \u0026#34;x,y\\n1,a\\n\u0026#34; readr::read_csv(I(content)) x y 1 1 a ファイル書き出し 標準の write.*() 関数をオプション無しで使うと、 左端に余計な列を追加したり、不要な\u0026quot;クオート\u0026quot;を追加したりする。\niris |\u0026gt; head(2) |\u0026gt; write.csv(stdout()) \u0026#34;\u0026#34;,\u0026#34;Sepal.Length\u0026#34;,\u0026#34;Sepal.Width\u0026#34;,\u0026#34;Petal.Length\u0026#34;,\u0026#34;Petal.Width\u0026#34;,\u0026#34;Species\u0026#34; \u0026#34;1\u0026#34;,5.1,3.5,1.4,0.2,\u0026#34;setosa\u0026#34; \u0026#34;2\u0026#34;,4.9,3,1.4,0.2,\u0026#34;setosa\u0026#34; iris |\u0026gt; head(2) |\u0026gt; readr::write_csv(stdout()) Sepal.Length,Sepal.Width,Petal.Length,Petal.Width,Species 5.1,3.5,1.4,0.2,setosa 4.9,3,1.4,0.2,setosa readr::write_*() のデフォルトはそれよりだいぶマシだが、 欠損値を空欄ではなく NA にしてしまうことだけ要注意。 空欄にするには毎回 na = \u0026quot;\u0026quot; が必要。\nwrite_delim(x, file, delim = \u0026#34; \u0026#34;, na = \u0026#34;NA\u0026#34;, append = FALSE, col_names = !append, quote = c(\u0026#34;needed\u0026#34;, \u0026#34;all\u0026#34;, \u0026#34;none\u0026#34;), escape = c(\u0026#34;double\u0026#34;, \u0026#34;backslash\u0026#34;, \u0026#34;none\u0026#34;), eol = \u0026#34;\\n\u0026#34;, num_threads = readr_threads(), progress = show_progress()) write_csv(...) や write_tsv(...) は区切り文字 delim = 指定済みのショートカット。\nwrite_lines(x, file, sep = \u0026quot;\\n\u0026quot;, na = \u0026quot;NA\u0026quot;, append = FALSE) vectorやlistを1行ずつ書き出す。 write_file(x, file, append = FALSE) 文字列をそのまま書き出す。 文字列から別の型へ parse_number(x, na = c(\u0026quot;\u0026quot;, \u0026quot;NA\u0026quot;), locale = default_locale(), trim_ws = TRUE) 文字列で最初に登場する数値を抜き出す。 邪魔な文字が前後に入っていても大丈夫。 x = c(\u0026#34;42\u0026#34;, \u0026#34;1.293e2\u0026#34;, \u0026#34;i18n\u0026#34;, \u0026#34;24/7\u0026#34;) readr::parse_number(x) [1] 42.0 129.3 18.0 24.0 parse_double(x, ...) 文字列を実数型として解釈して返す。 \u0026quot;6e23\u0026quot; のような指数形式も大丈夫。 異物が混じっていた場合は警告して欠損値扱い。 readr::parse_double(x) [1] 42.0 129.3 NA NA attr(,\u0026#34;problems\u0026#34;) row col expected actual 1 3 NA a double i18n 2 4 NA no trailing characters 24/7 as.double(x) [1] 42.0 129.3 NA NA parse_double(x, ...), parse_integer(x, ...) 文字列を整数型として解釈して返す。 小数点などを含む文字列は警告して欠損値扱い。 readr::parse_integer(x) [1] 42 NA NA NA attr(,\u0026#34;problems\u0026#34;) row col expected actual 1 2 NA no trailing characters 1.293e2 2 3 NA no trailing characters i18n 3 4 NA no trailing characters 24/7 as.integer(x) [1] 42 129 NA NA 標準の as.integer() が1ではなく129を返すということは、 一旦実数型で読み取ったあと小数点以下を切り捨ててるっぽい。\nparse_logical(x, ...) 特定の文字列を論理値型として解釈。 大文字小文字は問わない。 \u0026ldquo;0以外の数字はtrue\u0026rdquo; のような数値型からの変換規則とは異なる。 T, F を TRUE, FALSE 扱いしてしまうことに注意。 readr::parse_logical(c(\u0026#34;1\u0026#34;, \u0026#34;0\u0026#34;, \u0026#34;t\u0026#34;, \u0026#34;f\u0026#34;, \u0026#34;TRUE\u0026#34;, \u0026#34;FALSE\u0026#34;, \u0026#34;2\u0026#34;)) [1] TRUE FALSE TRUE FALSE TRUE FALSE NA attr(,\u0026#34;problems\u0026#34;) row col expected actual 1 7 NA 1/0/T/F/TRUE/FALSE 2 as.logical(c(\u0026#34;1\u0026#34;, \u0026#34;0\u0026#34;, \u0026#34;t\u0026#34;, \u0026#34;f\u0026#34;, \u0026#34;TRUE\u0026#34;, \u0026#34;FALSE\u0026#34;, \u0026#34;2\u0026#34;)) [1] NA NA NA NA TRUE FALSE NA as.logical(c(-1, 0, 1, 2)) [1] TRUE FALSE TRUE TRUE parse_factor(x, levels, ordered = FALSE, ...)\nparse_date(x, format = \u0026quot;\u0026quot;, ...), parse_datetime(x, format = \u0026quot;\u0026quot;, ...), parse_time(x, format = \u0026quot;\u0026quot;, ...)\n列の型を指定する https://cran.r-project.org/web/packages/readr/vignettes/column-types.html\n基本的には何も指定しなくても数値などを認識していい感じに設定してくれる。 文字列を勝手にfactorに変換したりはしない。 整数と実数は区別せずnumeric型で読む(1.2から)。\n明示的に型を指定したい場合は col_types 引数に cols() 関数の値を渡す。 文字列で \u0026quot;ccdi_\u0026quot; のように省略することも可能。\nread_csv(\u0026#34;mydata.csv\u0026#34;, col_types=\u0026#34;ccdi_\u0026#34;) colsp = cols(length=col_double(), count=\u0026#34;i\u0026#34;, .default=\u0026#34;c\u0026#34;) read_csv(\u0026#34;mydata.csv\u0026#34;, col_types=colsp) [c] col_character(): 文字列 [i] col_integer(): 整数 [d] col_double(): 実数 [l] col_logical(): TRUE or FALSE [D] col_date(format = \u0026quot;\u0026quot;): 日付 [t] col_time(format = \u0026quot;\u0026quot;): 時間 [T] col_datetime(format = \u0026quot;\u0026quot;): 日付 [n] col_number(): 数字以外の文字が含まれていても無視して数字として返す [?] col_guess(): 推測 [_] col_skip(): 列を読まない col_factor(levels, ordered): factor 指定した列だけ読むには cols(..., .default = col_skip()) とするか cols_only(...) を使う。\n設定 最近ちょっと表示がおせっかい過ぎるので ~/.Rprofile で設定を直す:\noptions( readr.num_columns = 0L, readr.show_col_types = FALSE, readr.show_progress = FALSE ) read::read_*() で読み込んだばかりのtibbleは各列の型情報を含んだ spec_tbl_df という特殊なサブクラスになっている。 この機能を切るオプションは用意されていない。 すぐに普通のtibbleが欲しい場合は [] で空subsettingするのが手軽。\ndata = readr::readr_example(\u0026#34;mtcars.csv\u0026#34;) |\u0026gt; readr::read_csv() class(data) [1] \u0026#34;spec_tbl_df\u0026#34; \u0026#34;tbl_df\u0026#34; \u0026#34;tbl\u0026#34; \u0026#34;data.frame\u0026#34; class(data[]) [1] \u0026#34;tbl_df\u0026#34; \u0026#34;tbl\u0026#34; \u0026#34;data.frame\u0026#34; Excelファイルを読み込む https://github.com/tidyverse/readxl\n自分のデータは絶対にExcel形式ではなくCSVやTSV形式で保存すべきだが、 人から受け取ったファイルや論文のサプリデータがExcelだったら仕方がない。 readxl というパッケージを利用すれば、 一旦Officeで開いてCSVに変換するという手間なしで直接Rに読み込める。\nRの中から install.packages(\u0026quot;readxl\u0026quot;) でインストールし、 使う前に library(readxl) でパッケージを読み込む。\nexcel_sheets(path) ファイルに含まれるシートの名前を取得 read_excel(path, sheet = 1, col_names = TRUE, col_types = NULL, na = \u0026quot;\u0026quot;, skip = 0) .xls と xlsx のどちらの形式でも読める。 sheet は番号でも名前でもいい。 それ以降の引数については readr の関数と同じ。 tibble https://r4ds.had.co.nz/tibbles.html https://github.com/tidyverse/tibble tbl_df クラスが付与された改良版data.frameのことをtibbleと呼ぶ。 readr で読み込んだデータもこの形式になる。\ntbl_mtcars = as_tibble(mtcars) class(tbl_mtcars) [1] \u0026#34;tbl_df\u0026#34; \u0026#34;tbl\u0026#34; \u0026#34;data.frame\u0026#34; class(mtcars) [1] \u0026#34;data.frame\u0026#34; 生のdata.frameとの違いは:\n巨大なデータをうっかりprint()しても画面を埋め尽くさない。 (逆に全体を見たい場合は工夫が必要。後述)\nprint(tbl_mtcars) mpg cyl disp hp drat wt qsec vs am gear carb 1 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 2 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 -- 31 15.0 8 301 335 3.54 3.570 14.60 0 1 5 8 32 21.4 4 121 109 4.11 2.780 18.60 1 1 4 2 列名の部分一致で良しとしない。 例えば mtcars$m は黙ってmpg列vectorを返してしまうが、 tbl_mtcars$m は警告つき NULL 。\nmtcars$m ## [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7 15.0 21.4 tbl_mtcars$m ## Warning: Unknown or uninitialised column: `m`. ## NULL 型に一貫性があり、勝手にdrop = TRUEしない。 例えば mtcars[,\u0026quot;mpg\u0026quot;] はvectorになってしまうが、 tbl_mtcars[,\u0026quot;mpg\u0026quot;] はtibbleのまま。 vectorが欲しい場合は二重四角括弧 tbl_mtcars[[\u0026quot;mpg\u0026quot;]]。\nmtcars[,\u0026#34;mpg\u0026#34;] # implicit drop = TRUE [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7 15.0 21.4 tbl_mtcars[,\u0026#34;mpg\u0026#34;] # remains tibble mpg 1 21.0 2 21.0 -- 31 15.0 32 21.4 行の名前は使わない。 行の名前として保持されている情報を使いたい場合は rownames_to_column() とか rowid_to_column() で独立した列にしておく必要がある。\n新しいtibble 1.4以降では pillar というパッケージが有効数字や欠損値などの表示形式を勝手にイジるようになってしまった。 見やすくない上に遅いので私は registerS3method() で上書きしている。\n関数 tibble::tibble(...) tibbleを新規作成。ちょっと昔までは dplyr::data_frame() だった。 base::data.frame() と違ってバグが混入しにくくて便利: 勝手に型変換しない (stringsAsFactors = FALSEが基本) 勝手に列名を変えない 長さ1の変数以外はリサイクルしない 引数の評価がlazyに行われるので前の列を利用して後の列を作ったりできる tbl_df クラスを付加 ただし1.4以降のバージョンでは表示が遅くて見にくい tibble::as_tibble(x) 既存のdata.frameやmatrixをtibbleに変換。 ちょっと昔までは dplyr::tbl_df() とか dplyr::as_data_frame() だった。 v2.0からは列名がちゃんとついてないとエラーになる。 tibble::new_tibble(x, ..., nrow, class = NULL) tibbleのサブクラスを扱う開発者向け as_tibble() 。 検証なし、nrow 必須の代わりに高速。 クラスを先頭に追加できるのも便利。 tibble::enframe(x, name = \u0026quot;name\u0026quot;, value = \u0026quot;value\u0026quot;) 名前付きvectorとかlistを2列のtibbleに変換する。 tibble::deframe(x) はその逆。 c(a = 1, b = 2) |\u0026gt; enframe() |\u0026gt; deframe() tibble::add_row(.data, ..., .before = NULL, .after = NULL) 既存のtibbleに新しいデータを1行追加する。 tibble::rownames_to_column(df, var = \u0026quot;rowname\u0026quot;) 行の名前をcharacter型で1列目の変数にする。dplyr::add_rownames()の後継。 tibble::rowid_to_column(df, var = \u0026quot;rowid\u0026quot;) はそれを整数で。 tibble::column_to_rownames(df, var = \u0026quot;rowname\u0026quot;) はその逆。 tibble::remove_rownames(df) は消すだけ。 tibble::glimpse(.data, width = NULL) データの中身をざっと見る。 print() とか str() のようなもの。 pillar::type_sum(x) オブジェクトの型 pillar::obj_sum(x) type_sumとサイズ e.g., \u0026quot;data.frame [150 x 5]\u0026quot; 設定 表示される行数や幅を調節する項目には以下のようなものがある。 ~/.Rprofile に書いておけば起動時に勝手に設定される。\nheight = 30L # for example width = 160L options( pillar.neg = FALSE, pillar.subtle = FALSE, pillar.print_max = height, pillar.print_min = height, pillar.width = width, width = min(width, 10000L) ) https://github.com/r-lib/pillar/blob/main/R/options.R https://github.com/tidyverse/tibble/blob/main/R/options.R 大きいtibbleの全体を表示する 普通に print() すると大きいtibbleの全体が見えない。 print() 関数にオプションを指定したり utils::page() を利用したりする必要がある。 RStudioやVSCodeを使っている場合は View() でスプレッドシートのように閲覧可能。\n# pillar:::print.tbl()にオプションを渡す方式 diamonds |\u0026gt; print(n = Inf, width = Inf) diamonds |\u0026gt; page(\u0026#34;print\u0026#34;, n = Inf, width = Inf) # 標準data.frame用のprint()を呼ぶ方式 diamonds |\u0026gt; base::print.data.frame(max = .Machine$integer.max) オプションをいちいち設定しなくて済むように max_print(), less() のような関数を定義しておくのもよい。\ngetOption(\u0026quot;max.print\u0026quot;) の初期値は99999だがRStudioでは勝手に1000まで下げられる。\n関連書籍 ",
  "href": "/rstats/readr.html",
  "tags": [
   "r",
   "tidyverse"
  ],
  "title": "readr",
  "type": "rstats"
 },
 {
  "content": "https://www.repeatmasker.org/\nインストール プログラム Homebrew で一撃:\nbrew install brewsci/bio/repeatmasker 依存プログラムも自動的に入る: RMBlast, HMMER, Tandem Repeat Finder, h5py.\nデータベース 単純な反復や一般的なアーティファクトはライブラリに組み込まれているが、 もっとしっかり使いたい場合はライブラリを更新して利用する。\nディレクトリ移動: cd $(brew --prefix)/opt/repeatmasker/libexec/ ファイルを追加・更新する。 RepBase: アカデミックな用途なら無料で使えていたが、いつの間にか有料になっていた。 Dfam: 使いたいものをダウンロードして ./Library/Dfam.* を差し替える。\nRepeatMasker 4.1.0 以前は Dfam.hmm や Dfam.embl を使っていたが、 4.1.1 以降では Dfam.h5 を使う。 4.1.2 には始めから Dfam 3.3 curatedonly が付いてくる。 変更を反映させる: /usr/bin/perl ./configure \u0026lt;configure.input 実行時に毎回 -lib オプションでファイルを指定する手もある。\n使用方法 https://www.repeatmasker.org/webrepeatmaskerhelp.html\nコマンド https://github.com/rmhubley/RepeatMasker/blob/master/repeatmasker.help :\nRepeatMasker -help RepeatMasker -pa 4 -qq -species oryza -dir . -xsmall -gff seq.fa 入力ファイルは圧縮 .fa.gz でもいいけど勝手に展開してしまうので注意。\n-engine [crossmatch|wublast|abblast|ncbi|hmmer|decypher]\n-parallel 1 並列化の恩恵は大きい -s (slow), -q (quick), -qq (rush) sensitivityとのトレードオフ -nolow / -low low complexity DNAをマスクしない -noint / -int interspersed repeatsをマスクしない -norna small RNAをマスクしない -div [number] コンセンサスからの分化度が指定したパーセント未満のやつだけマスク -lib [filename] 自分で用意したライブラリを使う。 そのときのスコア閾値: -cutoff 225 -species CLADE_NAME http://www.ncbi.nlm.nih.gov/Taxonomy/taxonomyhome.html/ に出てくるクレード名で指定可能。 大文字小文字は無視。 chimpanzee, mammals みたいな英語名も一部可能。 デフォルトは primate らしい。 -frag 60000 最大マスク長 -nopost 最後に自動で PostProcess を走らせない。 -dir OUTDIR 出力先。デフォルトはカレントではなくクエリと同じとこ。 -xsmall 反復配列を小文字にするsoft mask。 デフォルトでは N に置き換えるhard mask。 -gff GFFファイルも出力する。 -small \u0026ldquo;returns complete .masked sequence in lower case\u0026rdquo; 意味不明。ソースコードを眺めた感じでは、何もしてない。 生成物 計算途中の一時ファイルが ./RM_{pid}.{datetime} に書き出される。\n${INFILE}.cat.gz RepeatMasker が出力する大元の結果。 以下のファイルはこれを元に ProcessRepeats が作る。 ${INFILE}.masked 見つかった箇所を N や小文字に置き換えた配列ファイル。 入力ファイルの折り返し幅を保持してくれない。 soft mask済み入力ファイルの小文字を保持してくれない。 判定から外れた部分は大文字に戻される。 追加マスクしたいならGFFとかを使って自分でやる必要がある。 ${INFILE}.tbl 見つかった反復配列の要約 ${INFILE}.out アノテーション情報。 固定幅っぽい変なレイアウトの表で扱いにくい。ちょっと眺めるだけ。 ${INFILE}.out.gff -gff オプションを付ければ作ってくれる。 実質的に使える出力ファイルはこれだけかも。 RepeatScout http://bix.ucsd.edu/repeatscout/\n反復配列を de novo で拾い、RepeatMaskerで利用可能なライブラリを生成する。\nRepeatScout本体をインストール: wget -O- http://bix.ucsd.edu/repeatscout/RepeatScout-1.0.5.tar.gz | tar xz cd RepeatScout-1/ make L-mer の頻度テーブルをつくる: build_lmer_table -l 14 -sequence myseq.fa -freq lmer_table そのテーブルと配列から反復配列のFASTAを作る: RepeatScout -sequence myseq.fa -output rs_output.fa -freq lmer_table -l 14 TRFとNSEGを呼び出して \u0026gt;50% low-complexity なものを除外: cat rs_output.fa | filter-stage-1.prl \u0026gt;rs_filtered1.fa NSEG はビルド不可能なので filter-stage-1.prl を適当に書き換える必要がある。 RepeatMaskerで位置と登場回数を調べる: RepeatMasker -parallel 4 -dir . -lib rs_filtered1.fa myseq.fa 一定回数に満たないものを除外: cat rs_filtered1.fa | filter-stage-2.prl --thresh=10 --cat=myseq.fa.out \u0026gt;rs_filtered2.fa 遺伝子領域のGFFなどを与え、mobile elementっぽくないものを除去: compare-out-to-gff.prl --gff=known_genes.gff --cat=myseq.fa.out --f=rs_filtered2.fa \u0026gt;lib.ref RepeatModeler https://www.repeatmasker.org/RepeatModeler/\n上記RepeatScout手順を簡単に実行するラッパー？ 私はうまく使えた試しがない。\n前準備 RepeatMasker, TRF, RepeatScout: 上記 CD-Hit: brew install brewsci/bio/cd-hit RECON: wget -O- http://www.repeatmasker.org/RepeatModeler/RECON-1.08.tar.gz | tar xz cd RECON-1.08/src/ make make install #include \u0026lt;ctypes.h\u0026gt; を明示的に書けと怒られるので書く。 bin/ にPATHを通す。 RepeatModeler 本体: wget -O- https://www.repeatmasker.org/RepeatModeler/RepeatModeler-2.0.3.tar.gz | tar xz cd RepeatModeler-2.0.3/ ./configure Perlモジュールをインストールせよと言われたら cpan JSON とか適当に。 使い方 カレントディレクトリにBLASTデータベースを構築: BuilDatabase -name Colletotrichum_orbiculare -engine ncbi path/to/Colletotrichum_orbiculare.fa 本体を実行(かなり時間がかかる): RepeatModeler -engine ncbi -pa 4 -database Colletotrichum_orbiculare \u0026gt;run.out RM_[PID].[DATE]/ に結果が書き出される。 できあがった consensi.fa.classified をライブラリとして RepeatMasker を実行: RepeatMasker -lib consensi.fa.classified some_sequence.fa TEclass https://www.compgen.uni-muenster.de/tools/teclass\nトランスポゾンを分類する。\n与えられたFASTAに含まれているのはTEだ、という仮定で分類するだけので、 単純反復配列や重複遺伝子などを予めしっかり除去しておく必要がある。\n本体をダウンロード:\nwget -O- http://www.compgen.uni-muenster.de/tools/teclass/download/TEclass-2.1.3.tar.gz | tar xz cd TEclass-2.1.3/ less README 周辺ライブラリを整備。 でもとりあえず分類したいだけならblastclustなどは不要らしい:\n./Download_dependencies.sh ./Compile_dependencies.sh ./Configure.pl pre-built classifiers (\u0026gt;400MB) をダウンロード:\ncd path/to/TEclass/classifiers/ wget -O- http://www.compgen.uni-muenster.de/tools/teclass/download/classifiers.tar.gz | tar xz 実行: ./TEclassTest.pl file.fa\n結果はひとつのディレクトリにまとめて書き出される\nfile.fa: 元ファイルからTEだけ抜き出したもの？ file.fa.html: 一覧 file.fa.lib: RepeatMasker用？ file.fa.stat: LTRなどがそれぞれいくつあったか集計 ",
  "href": "/bio/repeatmasker.html",
  "tags": [
   "genetics"
  ],
  "title": "RepeatMasker",
  "type": "bio"
 },
 {
  "content": "生物多様性と複雑性の源泉 生物の多様な機能・形態はどのようにして獲得されてきたか？ 地球上の生物たちの驚くべき多様さと精巧さは全て、単純な共通祖先に端を発する進化の歴史の中で発展してきたものであり、その背景には生命の多様化・複雑化をもたらす力学的な法則が存在しているはずである。 この確率論的な過程を定式化して示すことが、私の理論進化学者としての目標である。 この試みは“Tree of Life”全体の在り方を統一的に理解するための重要なステップとなる。 私はこれまで 遺伝子制御ネットワーク(Gene Regulatory Network, GRN) が併せ持つ 頑健性 と 新奇性 がいかにして進化を駆動してきたか、 隠蔽変異 に着目してこの問題に取り組んできた。\n問題点: 表現型進化の中間段階を記述する理論の不在 進化の原動力は遺伝的変異であり、集団内に存在する遺伝的変異の量は表現型進化の方向や速度を決定する(Lande 1979; Hansen and Houle 2008)。この仮説に関する集団遺伝学的な理論研究の多くは、遺伝的変異はみな表現型の違いとして表れるという仮定の下に「見える変異」のみを取り扱うものであった。しかし生物が新たな機能を獲得するためには複数の突然変異を必要とする場合が多いため、ほとんど機能的でないかむしろ有害であるような中間段階（いわゆる適応度の谷）を生物がどのように超えるのかが、これまで複雑性の進化を考える上での大きな問題であった。\nまた、生物は突然変異を重ねるだけでどんな新しい表現型でも自由に作り出せるというわけではない。突然変異がゲノム中でランダムに起こるとき、それが表現型に及ぼす影響はランダムでも相加的でもなく、個体発生の過程に由来する制約を受ける。このことは、遺伝子と表現型の間を結ぶ分子発生学的システムを考慮することが表現型進化を理解する上で不可欠であることを示している。\n解決方策: 変異を隠蔽・放出する進化発生学的過程の理論構築 生物集団が新しい環境に遭遇すると、元々見えていたよりも大きな表現型分散が現れることがある(Clausen et al. 1940)。 このことは、通常生息している環境では「見えない変異」すなわち 隠蔽変異(Cryptic Genetic Variation, CGV) が集団内に蓄積しており、環境変動に応じてそれらが一気に顕在化することを示唆している。 このような隠蔽変異の蓄積と放出を可能にするメカニズムは進化キャパシタと呼ばれ、環境激変に対する適応進化や、適応度の谷を越えるような表現型進化に寄与すると考えられる。\n新奇な表現型の多くは全く新しい遺伝子の獲得を伴わず、むしろ既に持っている遺伝子セットの時空間的な発現パターンが変化することによって生じる(Jacob 1977; Carroll et al. 2008)。 したがって、どの遺伝子がいつ、どこで、どのくらい発現するかを司るGRNは、進化における新奇性の源であるとも言える。 さらに、GRNは代替経路やフィードバックループなどにより安定化されており、ネットワークの一部に突然変異が起きても簡単には表現型を変化させない。 本研究では、このようなGRNの頑健性が進化キャパシタとして隠蔽変異の蓄積・放出に寄与するとき、選択圧の強さなどの環境的要因や、GRN構造などの遺伝的要因にどう影響されるか、挙動を把握することを目的とする。\n研究方法: GRNの個体ベースモデル 環境シグナルが遺伝子発現を誘導し、GRNに規定された遺伝子間相互作用を経て個体の表現型が決まる。 表現型値と環境が定める目標値との距離に応じて個体の適応度が計算され、近いほど多くの複製を次世代に残す。 複製の際にGRNの一部で突然変異が生じる。このような過程で遺伝的に均一な状態から何万世代も進化させた個体群について、遺伝的多様性と表現型多様性を計測した。 続いて元の生息環境とは異なる環境シグナルを集団内の各個体に与え、そこで現れる表現型多様性を計測した。\n結果１: 強い安定化選択圧の下でも隠蔽変異としての進化ポテンシャルは枯渇しない 選択圧は遺伝的多様性や隠蔽変異にどのような影響を与えるだろうか？ これまでの理論研究では、強い選択圧の下では遺伝的変異が枯渇し、進化可能性が低くなると考えられてきた。 しかしGRNを組み込んだ個体の集団を選択圧の強い条件下で進化させた場合、遺伝的多様性や表現型多様性は減少した一方で、新規環境で顕在化する隠蔽変異は減少しなかった。 このことは、強い選択圧の下にあり表現型のばらつきが一見小さく見えるような形質・集団にも隠蔽変異が蓄積しており、環境変動時の適応進化に寄与しうる可能性を示唆している。\n結果２: 環境不均一性が高く可塑性が要求される環境ではGRNが複雑化する 生息環境の不均一性は集団の遺伝的構造の進化にどのような影響を及ぼすだろうか？ 個体が生涯で多様な環境を経験するような条件下で集団を進化させた場合、各時間断面で見られる表現型多様性は変化せず、集団中に蓄積する遺伝的多様性や隠蔽変異は減少した。 このことから、環境の異質性は選択圧の強さと同様に遺伝的変異を量的に制限する一方で、質的には選択圧の強さと異なる影響を持つことが示唆される。その証左として、環境異質性の高い進化条件下ほど、より大きく密なGRNが選択される傾向があった。 適応しなければならない環境の多様性によってGRNの複雑性への要求が高まるという関係性は、生命システムの複雑化を理解する上で普遍的な重要性を持つ。\n結果３: 大きなGRNほど隠蔽変異による進化可能性が高い ではGRNの構造は隠蔽変異や表現型多様性にどのような影響を与えるだろうか？ 環境条件を揃えつつ遺伝的なパラメータを変化させてシミュレーションを実施したところ、大きく密なGRNほど隠蔽変異を蓄積し、新規環境における表現型多様性が高いことが分かった。 これは、GRNが大きく複雑になるほど、新しい環境に対する適応の可能性や新奇形質を創出するポテンシャルが高くなることを示唆している。 近年、重複遺伝子を多く保持する種ほど侵略的に生息域を広げやすいことが明らかになったが、このような傾向もGRNの複雑性と何らかの関係があるのかもしれない。\n\\ 強い選択圧 可塑性要求 遺伝的多様性 減少 減少 表現型多様性 減少 変化なし 隠蔽変異 減少しない やや減少 GRN複雑性 変化なし 増大 まとめ: 生命システムの複雑化と多様な環境への進化的適応が相互促進的に作用する 分子発生学的過程と進化生態学的過程の両方を組み込んだモデルを構築することで、これまで扱えなかった「見えない変異」の量的な議論が可能となった。 3つの結果を環状に結ぶと、生命システムの複雑性の増大と多様な環境への適応は隠蔽変異を通して相互に促進しながら持続的に進化を駆動し、生物の多様性と複雑性の源泉となっている可能性が示唆される。 近い将来、ゲノム解読技術がさらに進歩し、GRN構造を個体・集団レベルで比較・解析できる時代が訪れるだろう。 本研究は来るべきその日に向け、進化学的なタイムスケールにおいてGRN構造と隠蔽変異がどう変化していくか、またそれが進化の道筋にどのような影響を与えるかを理解するための理論的な基盤を提供し、その重要性を訴えるものである。\nIwasaki, W. M. and Tsuda, M. E. and Kawata, M. (2013) BMC Evol Biol 13, 1 pp.91 [pmid:23622056] [doi:10.1186/1471-2148-13-91] Genetic and environmental factors affecting cryptic variations in gene regulatory networks\nGlossary 遺伝子制御ネットワーク (Genetic Regulatory Network: GRN) ある遺伝子が転写因子として別の遺伝子の発現を促進(あるいは抑制)し、 それがまた別の遺伝子を、というような相互作用関係をネットワークとして捉えたもの。 細胞分化・形態形成・細胞周期・環境応答などはこのような複雑な相互作用によって生み出されている。 突然変異によるネットワーク構造の変化は、制御下流の遺伝子群が発現する場所・時期・組み合わせを変化させることで不連続な表現型変異をもたらすことがある。\n隠蔽変異 (Cryptic Genetic Variation: CGV) 生物というシステムが持つ頑健性により、突然変異が起きても表現型に影響を与えない場合がある。 このような中立変異はある割合で集団中・集団間に保持される。 この隠れた遺伝的変異は、通常の環境では表現型の違いを生み出さないが、遺伝的・環境的な撹乱を受けたときに表現型多型として顕在化する。 その一部に、今までにない適応的な新奇形質が含まれている可能性があるのではないか、というお話。 そのように出現した新奇形質は始めは環境依存の可塑的なものだが、遺伝的同化によってのちのち普通の表現型として定着しうる。\n遺伝的同化 (Genetic assimilation) 始めに可塑的な応答として適応的な新奇形質が出現し、後からそれを安定化するような遺伝的基盤が進化すること。 これまでの理論研究の多くは、新奇形質を出現させる引き金が突然変異であると仮定してきた。 しかし、ショウジョウバエ(Waddington 1953)やスズメガ(Suzuki \u0026amp; Nijhout 2006)の実験などにおいて、 集団中に隠れていた遺伝的変異が外部から与えられる刺激によって顕在化して形態や体色の多型を生じること、 また、刺激なしでも新しい表現型になる系統がその後の人為選択によって生じることが観察されている。 あたかも獲得形質が遺伝しているようにも見えるが、可塑的な応答(reaction norm)の遺伝的基盤が進化していると見るのが正しい。\n学習して手に入れた形質は遺伝しないが、その学習をよりよくできる遺伝子型が世代を経て選択されていく結果、最終的に学習なしで生得的にその形質を示す遺伝子型まで行くかも、というボールドウィン効果も遺伝的同化の一種と見なせる。\nWest-Eberhard 2003, Kirschner \u0026amp; Gerhart 2005\n",
  "href": "/research.html",
  "tags": null,
  "title": "Research Interests",
  "type": "page"
 },
 {
  "content": " reshape2はもう古い。 data.frameを処理をするなら、同じ作者が新しく設計しなおした tidyr + dplyr のほうがより高速で洗練されているのでそちらを使おう。 ただし3次元以上のarrayを扱うにはまだ便利。\nhttp://had.co.nz/reshape/ https://cran.r-project.org/web/packages/reshape2/ https://www.rdocumentation.org/packages/reshape2 http://seananderson.ca/2013/10/19/reshape.html melt() data.frameの複数列の値を、カテゴリ変数1列と値1列の組に変換する。 これにより、変換する列数の分だけdata.frameが縦長(long-format)になる。 やや冗長性は増すが、ggplot2 での作図などさまざまな操作がしやすくなる。\nこの用途ならこれじゃなくて tidyr::gather() を使おう。\narray対象ならまだ使い道はある。 例えば3次元arrayを melt(arr, c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;, \u0026quot;z\u0026quot;)) として3列のmatrixに戻せるのは便利。 ただしこれも dplyr::as.tbl_cube() のほうが高速。\nreshape2::melt(data, id.vars, measure.vars, variable.name=\u0026#34;variable\u0026#34;, value.name=\u0026#34;value\u0026#34;, na.rm=FALSE, factorsAsStrings=TRUE, ...) data data.frame id.vars そのまま列として維持したい列名を文字列で指定。 何も指定しなければ measure.vars 以外のすべて。 measure.vars 列名を variable に、値を value に分解したい列名を文字列で指定。 何も指定しなければ id.vars 以外のすべて。 variable.name=\u0026quot;variable\u0026quot; meltされた列名を格納する新しい列の名前。 value.name=\u0026quot;value\u0026quot; meltされた値を格納する新しい列の名前。 na.rm=FALSE NA が含まれる行を取り除くかどうか。 Example ライブラリを読み込んでサンプルデータを見てみる(wide-format)\n\u0026gt; library(reshape2) \u0026gt; head(reshape2::french_fries) time treatment subject rep potato buttery grassy rancid painty 61 1 1 3 1 2.9 0.0 0.0 0.0 5.5 25 1 1 3 2 14.0 0.0 0.0 1.1 0.0 62 1 1 10 1 11.0 6.4 0.0 0.0 0.0 26 1 1 10 2 9.9 5.9 2.9 2.2 0.0 63 1 1 15 1 1.2 0.1 0.0 1.1 5.1 27 1 1 15 2 8.8 3.0 3.6 1.5 2.3 データをlong-formatに整形\n\u0026gt; molten = reshape2::melt(reshape2::french_fries, id.vars=c(\u0026#34;time\u0026#34;, \u0026#34;treatment\u0026#34;, \u0026#34;subject\u0026#34;, \u0026#34;rep\u0026#34;), variable.name=\u0026#34;flavor\u0026#34;, na.rm=TRUE) \u0026gt; head(molten) time treatment subject rep flavor value 1 1 1 3 1 potato 2.9 2 1 1 3 2 potato 14.0 3 1 1 10 1 potato 11.0 4 1 1 10 2 potato 9.9 5 1 1 15 1 potato 1.2 6 1 1 15 2 potato 8.8 ggplot2 で作図\n\u0026gt; library(ggplot2) \u0026gt; gp = ggplot(molten, aes(x=time, y=value, colour=treatment, shape=as.factor(rep))) \u0026gt; gp = gp + geom_point(alpha=0.3) \u0026gt; gp = gp + geom_smooth(aes(group=treatment), method=loess, se=FALSE) \u0026gt; gp = gp + facet_grid(flavor ~ subject) \u0026gt; gp dcast() カテゴリ変数を含むdata.frameを melt() と逆方向に (long-formatからwide-formatへ)整形する。\nこの用途ならこれじゃなくて tidyrのspread()を使おう。 fun.aggregateのように関数をグループごとに適用したい場合は dplyrのgroup_by()とsummarise()を使う。\n3次元以上のarrayを作りたいときはreshape2::acast()が便利。 e.g., acast(data, x ~ y ~ z, dplyr::first, value.var=\u0026quot;v\u0026quot;, fill=0)\nreshape2::dcast(data, formula, fun.aggregate=NULL, ..., margins=NULL, subset=NULL, fill=NULL, drop=TRUE, value.var=guess_value(data)) data melt() されたような形でカテゴリ変数を含むdata.frame formula x_var ~ y_var ~ z_var ~ ... のような形で出力形式を指定 fun.aggregate=NULL mean や sum など、整形後に同じマスに来る複数の値に適用する関数。 デフォルトでは length が働いて要素数が得られる。 ... aggregate関数への引数を渡せる margins=NULL 列全体の平均や行全体の和などを追加するかどうか subset=NULL 適用範囲を限定する e.g., subset=.(variable==\u0026quot;length\u0026quot;) fill=NULL\ndrop=TRUE\nvalue.var=guess_value(data)\nExample データは上の melt() の例で作った molten。\nfun.aggregate を省略すると length が適用されて要素数が分かる\n\u0026gt; reshape2::acast(molten, treatment ~ flavor) Aggregation function missing: defaulting to length potato buttery grassy rancid painty 1 232 231 232 232 232 2 232 230 232 232 231 3 231 231 231 231 231 グループごとの平均値をdata.frameで\n\u0026gt; reshape2::dcast(molten, treatment ~ flavor, mean) treatment potato buttery grassy rancid painty 1 1 6.887931 1.780087 0.6491379 4.065517 2.583621 2 2 7.001724 1.973913 0.6629310 3.624569 2.455844 3 3 6.967965 1.717749 0.6805195 3.866667 2.525541 足し算すると辞書式に並ぶ\n\u0026gt; reshape2::acast(molten, treatment ~ flavor + rep, mean) potato_1 potato_2 buttery_1 buttery_2 grassy_1 grassy_2 rancid_1 rancid_2 painty_1 painty_2 1 6.772414 7.003448 1.797391 1.762931 0.4456897 0.8525862 4.283621 3.847414 2.727586 2.439655 2 7.158621 6.844828 1.989474 1.958621 0.6905172 0.6353448 3.712069 3.537069 2.315517 2.597391 3 6.937391 6.998276 1.805217 1.631034 0.5895652 0.7706897 3.752174 3.980172 2.038261 3.008621 チルダで繋ぐと1次元増える\n\u0026gt; reshape2::acast(molten, treatment ~ flavor ~ rep, mean) , , 1 potato buttery grassy rancid painty 1 6.772414 1.797391 0.4456897 4.283621 2.727586 2 7.158621 1.989474 0.6905172 3.712069 2.315517 3 6.937391 1.805217 0.5895652 3.752174 2.038261 , , 2 potato buttery grassy rancid painty 1 7.003448 1.762931 0.8525862 3.847414 2.439655 2 6.844828 1.958621 0.6353448 3.537069 2.597391 3 6.998276 1.631034 0.7706897 3.980172 3.008621 ",
  "href": "/rstats/reshape2.html",
  "tags": [
   "r"
  ],
  "title": "reshape2",
  "type": "rstats"
 },
 {
  "content": " Project Homepage https://cran.r-project.org/web/packages/rgl/ 長らく2種類の書き方が混在していたが、 バージョン 1.0 からrgl.*() 関数の使用が非推奨となり一本化された。 ドキュメントも一新されて使いやすくなってきた。\nプロット Primitive shapes https://dmurdoch.github.io/rgl/reference/primitives.html\nrgl::points3d(x, y = NULL, z = NULL, ...) 散布図。 rgl::lines3d(x, y = NULL, z = NULL, ...) 折れ線 rgl::segments3d(x, y = NULL, z = NULL, ...) 線分 rgl::triangles3d(x, y = NULL, z = NULL, ...) 3点を結ぶ面 rgl::quads3d(x, y = NULL, z = NULL, ...) 4点を結ぶ三角形2つ Other shapes rgl::spheres3d(x, y = NULL, z = NULL, radius = 1, fastTransparency = TRUE, ...) 球体。 rgl::surface3d(x, y = NULL, z = NULL, ..., normal_x = NULL, normal_y = NULL, normal_z = NULL, texture_s = NULL, texture_t = NULL, flip = FALSE) 地形図のような局面 rgl::plot3d(x, ...) type =引数で上記の様々な形を描ける高次関数。 ホントはあんまり使いたくないけど、 xlim, ylim, zlim オプションを受け付ける関数がこれしかないようなので、 境界を指定しつつ球体を描きたい場合は spheres3d() ではなく plot3d(type = \u0026quot;s\u0026quot;) を使うしかないっぽい。 背景や軸などの調整 rgl::title3d(main, sub, xlab, ylab, zlab, line, level, floating, ...) これを使うとmainとsubも視点によって動いてしまう。 bgplot3d({plot.new(); title(\u0026quot;main\u0026quot;)}) なら固定背景に書ける。 rgl::mtext3d(text, edge, at = NULL, line = 0, level = 0, floating = FALSE, pos = NA, ...)\nrgl::bg3d(color, sphere = FALSE, back = \u0026quot;lines\u0026quot;, fogtype = \u0026quot;none\u0026quot;, fogScale = 1, col, ...)\nrgl::light3d(theta = 0, phi = 15, x = NULL, y = NULL, z = NULL, viewpoint.rel = TRUE, ambient = \u0026quot;#FFFFFF\u0026quot;, diffuse = \u0026quot;#FFFFFF\u0026quot;, specular = \u0026quot;#FFFFFF\u0026quot;)\nrgl::par3d(..., no.readonly = FALSE, dev = cur3d(), subscene = currentSubscene3d(dev))\nrgl::material3d(..., id = NULL) プロットに渡せるオプション(colorなど)はここで確認 軸 rgl::axis3d(edge, at = NULL, labels = TRUE, tick = TRUE, line = 0, pos = NULL, nticks = 5, ...) xyz と +- の組み合わせで軸1本を指定して描く。 x は　x-- と等価。 rgl::box3d(...) 12辺の箱を描く。 rgl::bbox3d(xat = NULL, yat = NULL, zat = NULL, xunit = \u0026quot;pretty\u0026quot;, yunit = \u0026quot;pretty\u0026quot;, zunit = \u0026quot;pretty\u0026quot;, expand = 1.03, draw_front = FALSE, xlab = NULL, ylab = NULL, zlab = NULL, xlen = 5, ylen = 5, zlen = 5, marklen = 15, marklen.rel = TRUE, ...) 手前の辺が自動で消えるような箱を描く。 rgl::axes3d(edges = \u0026quot;bbox\u0026quot;, labels = TRUE, tick = TRUE, nticks = 5, box = FALSE, expand = 1.03, ...) 上記の3つをまとめる関数。分かりにくいので使わないほうがいい。 edges = \u0026quot;bbox\u0026quot; の場合 tick = FALSE は効かないので xlen = 0, ylen = 0, zlen = 0 とする必要がある。 rgl::view3d(theta = 0, phi = 15, fov = 60, zoom = 1, scale = par3d(\u0026quot;scale\u0026quot;), interactive = TRUE, userMatrix) theta: 0のとき正面がxy平面。観察者が地球の公転と同じ方向に動くのが正。\nphi [-90, 90]: 0のとき視点が水平面(xz平面)上。観察者が上に動くのが正。\nfov [0, 179]: 0のとき無限遠から見たような平行投影。 複数の図をまとめる # レイアウトを指定 mfrow3d(nr, nc, byrow = TRUE, parent = NA, sharedMouse = FALSE, ...) layout3d(mat, widths, heights, parent = NA, sharedMouse = FALSE, ...) # 次のsubsceneに移動 next3d(current = NA, clear = TRUE, reuse = TRUE) これらはなぜかグローバルスコープでしか動作しない。 つまり、関数やループ内に入れるとサイズなどがうまく反映されない。\n出力 デバイスの起動と終了 rgl::open3d(..., params = get3dDefaults(), useNULL = rgl.useNULL(), silent = FALSE) 明示的に新しいデバイスを開く。 何も無い状態でplot3d()などが呼ばれたら勝手に開かれる。 サイズ指定はwindowRect = c(0, 0, 600, 600)のような引数で。 rgl::close3d(dev = cur3d(), silent = TRUE) デバイスを閉じる。 rgl::clear3d(type = c(\u0026quot;shapes\u0026quot;, \u0026quot;bboxdeco\u0026quot;, \u0026quot;material\u0026quot;), defaults = getr3dDefaults(), subscene = 0)\nDisplay https://dmurdoch.github.io/rgl/dev/articles/rgl.html#default-display\nデフォルトでは独立のウィンドウ(XQuartz, X11など)が立ち上がる。\noptions(rgl.useNULL = TRUE) 独立ウィンドウが開くのを抑制。 rglwidget() WebGLに変換してRStudio, VSCode, ウェブブラウザなど出力。 options(rgl.printRglwidget = TRUE) 自動的に rglwidget() を呼ぶ。使わないほうが無難。 ファイルに書き出す rgl::scene3d(minimal = TRUE) rglネイティブな形での全構成要素リスト。 rgl::snapshot3d(filename, fmt = \u0026quot;png\u0026quot;, top = TRUE, ..., scene, width, height, webshot) PNGのみ。 top = FALSEにしてはダメ。謎。 rbl.postscript(filename, fmt = \u0026quot;eps\u0026quot;, drawText = TRUE) ps, eps, tex, pdf, svg をサポート。 透過や bgplot3d は反映されないらしいので注意。 rgl::writeWebGL() deprecatedだから代わりに rglwidget() を使えとのことだがそちらにファイル書き出し機能は無い。 rmarkdown/knitrでHTMLに埋め込む パッケージを読み込み、hookを設定しておく(rgl::setupKnitr() を使う手もある):\n```{r library} options(rgl.useNULL = TRUE) library(rgl) knitr::knit_hooks$set(webgl = rgl::hook_webgl) knitr::knit_hooks$set(rgl = rgl::hook_rgl) ``` WebGLを出力したい場合は webgl = TRUE、 PNG静止画を出力したい場合は rgl = TRUE:\n```{r plot, webgl = TRUE} rgl::box3d() rgl::title3d(\u0026#34;main\u0026#34;, \u0026#34;sub\u0026#34;, \u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;) ``` rgl::rglwidget() を明示的に呼ぶならhookは不要。 複数描画したいときは htmltools::tagList() に詰める手もある:\n```{r widget} purrr::map(seq_len(3), ~{ rgl::box3d() rgl::rglwidget(width = 200, height = 200) }) |\u0026gt; htmltools::tagList() ``` WebGLへの変換は出力先がHTMLであることを条件にしているらしく、 rmarkdown::render() なら上記コードで問題ないが knitr::knit() はダメ。 強制的に変換する手段はあるのかな\u0026hellip;?\nアニメーション rgl::spin3d(axis = c(0, 0, 1), rpm = 5, dev = cur3d(), subscene)\nrgl::par3dinterp(times = NULL, userMatrix, scale, zoom, FOV, method, extrapolate)\nrgl::play3d(f, duration = Inf, dev = cur3d(), ..., startTime = 0)\nrgl::movie3d(f, duration, dev = cur3d(), ..., fps = 10, movie = \u0026quot;movie\u0026quot;, frames = movie, dir = tempdir(), covert, clean, verbose, top, type, startTime, webshot)\nrgl::view3d(-25, 15, 40) rgl.bringtotop() .anime = rgl::spin3d(axis = c(0, 1, 0), rpm = 15) rgl::play3d(.anime) rgl::movie3d(.anime, duration = 4, fps = 16, movie = \u0026#34;basename\u0026#34;, dir = \u0026#34;~/tmp\u0026#34;) ほかに3Dグラフを描けそうな手段: plotly threejs VisPy ",
  "href": "/rstats/rgl.html",
  "tags": [
   "r",
   "graph"
  ],
  "title": "rgl",
  "type": "rstats"
 },
 {
  "content": "ファイルをコピーし、2つのディレクトリを同期する。 更新があったものだけをコピーする、 ひとつのsshセッションで複数のファイルを送受信する、 という使い方が可能なので cp や scp よりも便利な場合が多い。\nhttps://rsync.samba.org/ https://github.com/WayneD/rsync 基本 オプションについては後述するとして、基本は:\nrsync -auv SRC/ DST/ SRC側の末尾のスラッシュの有無によって結果が大きく異なることに注意。 DST側は付けても付けなくても同じ。\n## SRC/ 以下のファイルが DST/ 以下に入る rsync -auv SRC/ DST ## ディレクトリ DST/SRC が作られる rsync -auv SRC DST ## 結果は同じだが、下のほうがより明示的 rsync -auv SRC/DIR DST rsync -auv SRC/DIR/ DST/DIR/ sshの設定をしておけばリモートホストへの転送も可能。 その場合は宛先を remote_machine:~/DST のようにコロンで指定する。\nOptions -a, --archive バックアップ用途のオプション一式 -rlptgoD -u, --update 受け手の方が新しいファイルをスキップ -v, --verbose 冗長なメッセージ表示 -n, --dry-run 実際に送受信を行わず試してみる -z, --compress 圧縮・展開のCPUコストはかかるけど通信量は減る --delete SRC側に存在しないものがDST側にあったとき削除 --delete-excluded 除外設定されているファイルが受け手側にあったら削除（危険！） --ignore-existing 受信側に存在していたら無視 --iconv 文字コードを変換する。 異なるOS/FS間でウムラウトや日本語を含むファイルを送受信するときに使う。 リモート側だけ指定すれば十分だけどローカルも明示的に指定できる。 その場合、pushかpullかによらず ={LOCAL},{REMOTE} の順番で。 例えばローカルのLinuxマシンとリモートの古いMacでやり取りする場合は --iconv=utf8-mac or --iconv=utf8,utf8-mac 。 新しいMacのAPFSはLinuxと同じ utf8 と見なしておけば良さそう\u0026hellip;? Exclude and include https://download.samba.org/pub/rsync/rsync.1#PATTERN_MATCHING_RULES\n先に記述したものほど優先。 正規表現ではなくglob寄りの独自文法。 基本的にはpathの最終コンポーネント(basename)の部分が評価対象。 上位ディレクトリから順に評価し、除外されたらそれより下は読みに行かない。 末尾が / ならディレクトリにのみマッチ。 --include=\u0026lt;PATTERN\u0026gt; マッチするファイル・ディレクトリを除外しない --exclude=\u0026lt;PATTERN\u0026gt; マッチするファイル・ディレクトリを除外 --exclude-from=\u0026lt;FILE\u0026gt; ファイルに記述した除外パターンを読む -C, --cvs-exclude ほぼいつでも無視したいであろうものを除外する。 基本的にはこれを使う方針で大丈夫そうだが tags, core とかはディレクトリ名として普通に使いそうなので注意。 RCS SCCS CVS CVS.adm RCSLOG cvslog.* tags TAGS .make.state .nse_depinfo *~ #* .#* ,* _$* *$ *.old *.bak *.BAK *.orig *.rej .del-* *.a *.olb *.o *.obj *.so *.exe *.Z *.elc *.ln core .svn/ .git/ .hg/ .bzr/ これに加えて ${HOME}/.cvsignore も自動で読まれるというのが便利。 SSH越しの送受信 rsync -auv user@example.com:~/dir/ ~/dir/ SSH公開鍵を設定してパスワード無しでログインできるようにしておく。 .ssh/config でユーザー名とかも登録しておくとさらに楽。 RequestTTY yes を付けてると当然ながら怒られる。 リモート側の .bashrc とかで標準出力に何かを表示するようにしてあるとコケる ",
  "href": "/dev/rsync.html",
  "tags": [
   "communication"
  ],
  "title": "rsync",
  "type": "dev"
 },
 {
  "content": "https://www.bioconductor.org/packages/devel/bioc/html/rtracklayer.html\n読み書き https://genome.ucsc.edu/FAQ/FAQformat.html\nGFF, BED, bedGraph, BED15, WIG, BigWig, 2bit \u0026lt;\u0026mdash;\u0026gt; GRanges, GRangesList\nimport(filename, format, text, ...) ファイルあるいは文字列を読み込んで GRanges に変換する。 format は拡張子から判断してくれる。 import.gff(), import.bed() など形式ごとの関数も定義されていて、 引数などの詳しいヘルプもそっちから見られる。 GFFは feature.type=\u0026quot;exon\u0026quot; などとして興味のある行だけ読んだほうが良さそう。 結果として、余分な列も減る。\nexport(gr, filename, format, ...) GRanges をGFFなどの形式でファイルに書き出す。 UCSC Table Browserにアクセス https://genome.ucsc.edu/cgi-bin/hgTables\nゲノム、トラック、テーブルの指定方法が独特\nsession = browserSession() genome(session) = \u0026#34;hg19\u0026#34; query = ucscTableQuery(session, \u0026#34;knownGene\u0026#34;) tableName(query) = \u0026#34;kgXref\u0026#34; kgXref = getTable(query) # data.frame query = ucscTableQuery(session, \u0026#34;knownGene\u0026#34;, \u0026#34;hg19\u0026#34;, \u0026#34;kgXref\u0026#34;) テーブルを指定しなければ先頭が使われる\nquery = ucscTableQuery(session, \u0026#34;cytoBand\u0026#34;, \u0026#34;hg19\u0026#34;) cytoBand = track(query) # GRanges トラックやテーブルの名前を閲覧\ntrackNames(session) tableNames(query) ",
  "href": "/rstats/rtracklayer.html",
  "tags": [
   "r",
   "bioconductor"
  ],
  "title": "rtracklayer",
  "type": "rstats"
 },
 {
  "content": "tidyverse 解析も作図も 整然データ (tidy data) を用意するところから始まる。\n初学者向け講義資料2023 わかりやすいスライド: 整然データってなに？ by @f_nisihara 詳しい解説: 整然データとは何か by @f_nisihara 原著: Tidy Data by @hadley tidy datasets are all alike but every messy dataset is messy in its own way\n\u0026mdash; Hadley Wickham\ntidyverse はそういう思想に基いて互いに連携するようデザインされたパッケージ群で、 R標準の関数よりも遥かに分かりやすく安全で高機能なものを提供してくれている。\nグラフ描画には ggplot2 data.frame内の計算・要約・抽出には dplyr data.frameの変形・ネストには tidyr リストなどに対するループ処理には purrr 文字列処理には stringr data.frame \u0026lt;=\u0026gt; CSV/TSV の読み書きには readr list \u0026lt;=\u0026gt; json の読み書きには jsonlite 日本語版でも英語版でも公開オンライン版でもいいのでとにかく R for Data Science (r4ds) を読むのが一番。 即戦力が欲しい場合は、新しく日本語で書かれた「RユーザのためのRStudio[実践]入門−tidyverseによるモダンな分析フローの世界」が取っつきやすい。\n確率分布 https://cran.r-project.org/web/views/Distributions.html\nhttps://en.wikibooks.org/wiki/R_Programming/Probability_Distributions\n関数の種類 d___(x, ...) 確率密度関数 (PDF)。 $P[X = x]$ p___(q, ..., lower.tail = TRUE, log.p = FALSE) 累積分布関数 (CDF)。 デフォルトでは左からqまでの積分 $P[X \\leq q]$ 。 lower.tail = FALSE とするとqより右の積分(相補CDF) $P[X \u0026gt; q]$ 。 第一引数やパラメータ...はvector処理されるが、 後半の論理値引数はvectorを与えても先頭のみが使われる。 離散分布の場合は境界の値を含むか含まないかでバー1本分の差が出るので注意。 ホントは lower.tail = FALSE でも境界を含むべきだと思うんだけど。\n1 - pnorm(...)やlog(pnorm(...))のほうが直感的に分かりやすいので、 lower.tail = FALSEやlog.p = TRUEは不要なようにも思われるが、 これらの引数で内部処理させたほうが浮動小数点型の限界付近での計算が正確。\n# complementary 1 - pnorm(10, 0, 1) # 0 pnorm(10, 0, 1, lower.tail = FALSE) # 7.619853e-24 # log log(pnorm(10, 0, 1)) # 0 pnorm(10, 0, 1, log.p = TRUE) # -7.619853e-24 q___(p, ..., lower.tail = TRUE, log.p = FALSE) 累積分布関数の逆関数。 左からの累積確率が p となる確率変数の値。 p___() の逆関数。 lower.tail = FALSE とすると右から。 r___(n, ...) 乱数を n 個生成する dnorm(c(0, 1.96)) ## [1] 0.39894228 0.05844094 pnorm(c(0, 1.96)) ## [1] 0.5000000 0.9750021 qnorm(c(0.5, 0.975)) ## [1] 0.000000 1.959964 rnorm(4) ## [1] -1.77327259 0.95713346 0.27941121 0.08387267 分布の種類 離散\n_binom(size, prob) _geom(prob) _hyper(m, n, k) _nbinom(size, prob, mu) _pois(lambda) _signrank(n) _wilcox(m, n) 連続\n_beta(shape1, shape2) _cauchy(location = 0, scale = 1) _chisq(df) _exp(rate = 1) _f(df1, df2) _gamma(shape, rate = 1, scale = 1 / rate) _lnorm(meanlog = 0, sdlog = 1) _logis(location = 0, scale = 1) _norm(mean = 0, sd = 1) _t(df) _unif(min = 0, max = 1) _weibull(shape, scale = 1) 関数を作る 基本\nmy_add = function(x, y = 1) { x + y } my_add(13, 29) # 42 my_add(3) # 4 (2つめの引数を省略すると1) 引数の選択肢を指定・チェック match.arg() 省略すると先頭の要素が採用される\ngreat_scott = function(name = c(\u0026#34;Marty\u0026#34;, \u0026#34;Emmett\u0026#34;, \u0026#34;Biff\u0026#34;)) { name = match.arg(name) print(sprintf(\u0026#34;I am %s.\u0026#34;, name)) } great_scott(\u0026#34;Marty\u0026#34;) # OK great_scott(\u0026#34;DeLorean\u0026#34;) # Error great_scott() # OK, Marty 引数の有無をチェック 困ったことに、引数不足で呼び出した瞬間にはエラーを出してくれず、 関数の中でその引数の中身を参照しようとしたところで初めてエラーが出て止まる\nmy_func = function(x) { print(\u0026#34;WTF! This line is printed anyway.\u0026#34;) print(x) } my_func() 関数の中で missing() を使えばエラーを出さずに有無をチェックできる。 これを利用して引数省略可能な関数を作ることもできるが、 それなら素直にデフォルト引数を使ったほうがシンプルだし、 利用者は引数を見るだけで意図を読み取れる\ngood_func = function(x, y = x) { x + y } bad_func = function(x, y) { if (missing(y)) {y = x} x + y } good_func(3) # 6 bad_func(3) # 6 可変長引数 (...) 最後の引数をピリオド３つにし、list(...) か c(...) で受け取る\nfunc = function(x, y, ...){ for (i in c(...)) { cat(i, \u0026#34;\\n\u0026#34;) } } 引数を中身ではなく名前として評価 value = 42 fun1 = function(x) x fun2 = function(x) substitute(x) fun3 = function(x) deparse(substitute(x)) fun1(value) ## [1] 42 fun1(quote(value)) ## value fun2(value) ## value fun3(value) ## [1] \u0026#34;value\u0026#34; 詳しくは non-standard expression (NSE) で調べる:\nhttps://adv-r.hadley.nz/meta https://cran.r-project.org/web/packages/dplyr/vignettes/programming.html https://rlang.tidyverse.org/articles/tidy-evaluation.html 最適化・高速化 最新の R を使う 2.11.0 64bit化 2.14.0 すべての標準パッケージがバイトコンパイルされている 2.15.? data.frameの操作が劇的に速くなった ベクトル化されてる関数・演算子を使う Rの数値や文字列は1個の値でもベクトル扱い。 同じ長さ(または長さ1)の相手との計算はベクトルのまま行える。 自分で for 文を書くよりこれを利用するほうが楽チンで高速。\nx = c(1, 2, 3) # 長さ3の数値ベクトル x + x # 同じ長さ同士の計算 # [1] 2 4 6 y = 42 # 長さ1の数値ベクトル x + y # 長さ3 + 長さ1 = 長さ3 (それぞれ足し算) # [1] 43 44 45 # 自分でforを書く悪い例。 # コードが無駄に長くなる上に実行速度も遅い。 z = c(0, 0, 0) for (i in seq_len(3)) { z[i] = x[i] + y } 四則演算のみならず様々な処理がベクトルのまま行えるようになっている。 e.g., log(), sqrt(), ifelse(), paste()\n並列化 See foreach #parallel\nイテレータでメモリ節約 See foreach #iterators\nボトルネックを知る Rprof() # start profiling some_hard_work() Rprof(NULL) # end profiling summaryRprof() 実行時間の計測・比較 r_for = function(n) { s = 0; for (i in seq_len(n)) {s = s + 1 / i}; s } r_vec = function(n) sum(1 / seq_len(n)) Rcpp::cppFunction(\u0026#34;double rcpp(int n) { double s = 0; for (int i = 1; i \u0026lt;= n; ++i) {s += 1.0 / i;} return s; }\u0026#34;) # Compilation takes a few seconds here n = 1000000L system.time(r_for(n)) system.time(r_vec(n)) system.time(rcpp(n)) 計測にはr-libチームによるbenchが便利。 時間だけでなくメモリや実行結果までチェックした上、可視化までお世話してくれる:\ndf = bench::mark(r_for(n), r_vec(n), rcpp(n)) df # expression min mean median max itr/sec mem_alloc n_gc n_itr total_time result memory time gc # \u0026lt;char\u0026gt; \u0026lt;bench_time\u0026gt; \u0026lt;bench_time\u0026gt; \u0026lt;bench_time\u0026gt; \u0026lt;bench_time\u0026gt; \u0026lt;num\u0026gt; \u0026lt;bench_bytes\u0026gt; \u0026lt;num\u0026gt; \u0026lt;int\u0026gt; \u0026lt;bench_time\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; # 1: r_for(n) 30.49ms 31.14ms 31.11ms 32.82ms 32.1107 3.89MB 0 17 529ms 14.39273 \u0026lt;Rprofmem\u0026gt; 1: 30.7ms,30.7ms,30.8ms,31.9ms,31.1ms,31.2ms,... \u0026lt;tbl_df\u0026gt; # 2: r_vec(n) 2.25ms 2.85ms 2.74ms 7.87ms 350.2935 11.44MB 35 81 231ms 14.39273 \u0026lt;Rprofmem\u0026gt; 2: 7.87ms,6.67ms,6.73ms,7.65ms,2.81ms,2.8ms,... \u0026lt;tbl_df\u0026gt; # 3: rcpp(n) 1ms 1.02ms 1ms 1.79ms 977.6923 2.49KB 0 489 500ms 14.39273 \u0026lt;Rprofmem\u0026gt; 3: 1.79ms,1.05ms,1.04ms,1.04ms,1.03ms,1.03ms,... \u0026lt;tbl_df\u0026gt; plot(df) ちょっとした比較には rbenchmark の表示がシンプルで見やすい:\nrbenchmark::benchmark(r_for(n), r_vec(n), rcpp(n))[,1:4] # test replications elapsed relative # 1 r_for(n) 100 3.968 29.835 # 2 r_vec(n) 100 0.473 3.556 # 3 rcpp(n) 100 0.133 1.000 もうちょっと詳しく見たい場合は microbenchmark の出力も扱いやすい:\ndf = microbenchmark::microbenchmark(r_for(n), r_vec(n), rcpp(n), times = 100L) df # Unit: milliseconds # expr min lq mean median uq max neval # r_for(n) 35.716909 36.131130 37.406180 36.791894 38.330989 42.605700 100 # r_vec(n) 2.630786 3.251540 4.285222 3.412421 5.573207 8.180405 100 # rcpp(n) 1.216385 1.222994 1.270488 1.231115 1.304536 1.515532 100 str(df) # Classes ‘microbenchmark’ and \u0026#39;data.frame\u0026#39;: 300 obs. of 2 variables: # $ expr: Factor w/ 3 levels \u0026#34;r_for(n)\u0026#34;,\u0026#34;r_vec(n)\u0026#34;,..: 2 1 1 1 2 2 1 1 1 2 ... # $ time: num 4722392 42474783 38625124 38819035 3888012 ... ggplot(df) + aes(expr, time) + stat_summary(fun = mean, geom = \u0026#34;bar\u0026#34;) + geom_jitter(height = 0, alpha = 0.5) 関連書籍 統計解析やグラフ描画ではなく、 プログラミング言語としてのRを学びたいときに:\nhttps://adv-r.hadley.nz/\n",
  "href": "/rstats/programming.html",
  "tags": [
   "r"
  ],
  "title": "RプログラミングTips",
  "type": "rstats"
 },
 {
  "content": "https://cran.r-project.org/manuals.html\nインストール R本体 RStudio (任意) Macなら Homebrew で brew install --cask r rstudio のように入れるのが楽チン。 Caskじゃない brew install r のほうだとバイナリ版パッケージが使えなくて毎回ソースからビルドさせられるので大変。\nhttps://cran.r-project.org/doc/manuals/R-admin.html\n起動オプション ワークスペースの自動保存や自動復帰は危険なので切っておく。 R.app や RStudio から使う場合はメニューから環境設定みたいなやつを開いて設定。 シェルから使う場合は例えば以下のようなエイリアスを設定する。\nalias r=\u0026#39;R --quiet --no-save --no-restore-data\u0026#39; 詳しくは R --help または https://cran.r-project.org/doc/manuals/R-intro.html#Invoking-R\nパッケージのサーチパス https://stat.ethz.ch/R-manual/R-patched/library/base/html/libPaths.html\n.libPaths() でパッケージのインストール先候補一覧を取得できる。 install.packages(pkgs, lib, ...) の lib = オプションを指定しない場合にこれらが参照される。 また library() によるパッケージ読み込みもこれらのパスから。\n.libPaths(\u0026quot;newpath\u0026quot;) のように任意のパスを追加することもできるが、 後述の .Renviron ファイルなどで環境変数 (R_LIBS, R_LIBS_USER, R_LIBS_SITE) を設定しておく方法がよさそう。 ファイルから設定が正しく読み込まれても、 当該ディレクトリが存在しないと認識されず自動生成もされないことに注意。\nここで設定するパスには %v といった記号でRのバージョン情報などを含めることも可能。 古いRでインストールしたパッケージを新しいRで使おうとすると package ‘***’ was installed before R x.y.0: please re-install it などと怒られるので、バージョン番号を入れておいたほうがいい。 path.expand() も適用されるので ~/.R/library のようなチルダも展開される。\n環境変数には優先順位があるので例えば次のように使い分けられる:\nR_LIBS: プロジェクトごとの一時的な設定 R_LIBS_USER: ユーザーが常に使いたい設定 macOS既定値: ~/Library/R/%v/library Linux既定値: ~/R/%p-library/%v R_LIBS_SITE: 管理者が全ユーザーに使わせたい設定 R内から .Library.site で参照可能 空の場合は $R_HOME/site-library になる Rと一緒についてくる標準パッケージのインストール先は .Library で参照可能。 何も設定しないで使うとほかのパッケージもそこに入ってしまう場合があってあんまりよろしくない。\n環境変数 https://cran.r-project.org/doc/manuals/R-admin.html#Environment-variable-index\nhttps://stat.ethz.ch/R-manual/R-patched/library/base/html/EnvVar.html\nSys.getenv() .Renviron https://stat.ethz.ch/R-manual/R-patched/library/base/html/Startup.html\nR起動時に読み込まれ、環境変数を設定するファイル。 Rスクリプトではなく、シェルスクリプトっぽい代入式で書く。 例 (https://github.com/heavywatal/dotfiles/blob/master/.R/.Renviron):\nR_USER=${HOME}/.R R_LIBS_USER=${R_USER}/library/%v R_ENVIRON_USER=${R_USER}/.Renviron R_PROFILE_USER=${R_USER}/.Rprofile R_HISTFILE=${R_USER}/.Rhistory R_HISTSIZE=65535 LANG=C LC_CTYPE=en_US.UTF-8 探される・読み込まれる順序はだいたい以下のとおり:\n$R_ENVIRON $R_HOME/etc/Renviron.site $R_ENVIRON_USER ./.Renviron ~/.Renviron 読み込ませたくないときは --no-environ オプション。\n.Rprofile https://cran.r-project.org/doc/manuals/R-intro.html#Customizing-the-environment\nR起動時に読み込まれるファイル。 中身はRスクリプトなので、パッケージの読み込みや関数の定義など、Rでできることは何でもできるはず。 例: https://github.com/heavywatal/dotfiles/blob/master/.R/.Rprofile\n.First() と .Last() はそれぞれ起動時と終了時に実行される関数。 これらが原因で R CMD やパッケージ関連の操作が失敗することもあるので、 普通の対話環境でのみ有効になるよう if (interactive()) などとしておいたほうが安心。\n読み込まれる順序はだいたい以下のとおり。 .Renviron のほうが先に読み込まれるので、 上記のように R_PROFILE_USER を定義しておいて、そこに置いとけば読み込まれる。\n$R_PROFILE $R_HOME/etc/Rprofile.site $R_PROFILE_USER ./.Rprofile ~/.Rprofile 読み込ませたくないときは --no-init-file オプション。\noptions() https://stat.ethz.ch/R-manual/R-patched/library/base/html/options.html\n?options で項目の一覧を見られる。\nスクリプトの結果が設定依存で変わっては困るので、 そういう本質的なものはいじらずに、 表示関連の項目だけ変えるに留めたほうがよい。 例えば options(stringsAsFactors=FALSE) とやってしまいたくなるが、 それを設定した環境とそうでない環境で read.table() の結果が変わるのは危険。 関数の引数として毎回指定するか、 readrの関数を使うべし。\nwarn=1 警告レベルの設定。 デフォルト(warn=0)では、警告があっても計算は滞り無く進行し、 最後に \u0026ldquo;There were 50 or more warnings (use warnings() to see the first 50)\u0026rdquo; などと軽く表示されるだけなので、 見落としたりして後々大変なバグ取り作業に発展する恐れがある。 警告が発生するごとに警告文を表示する(warn=1)か、 エラー扱いにして計算をストップするようにしておく(warn=2)ことでそれを回避すべし。 ちなみに負数だと警告無視。 warnPartialMatchAttr, warnPartialMatchDollar listやdata.frameなどの要素を抜き出すとき、 対象が一意に定まる範囲で変数名の省略が許されてしまう (e.g., mtcars$m)。 これは危険なので、せめて警告がでるように設定する。 tibble を使うほうがより安全。 warnPartialMatchArgs 関数の引数名の省略に関する警告。 自分のコーディングに関してはTRUEにしておきたいけど、 結構いろんなパッケージが警告を発してうるさいので仕方なくFALSE。 showWarnCalls=TRUE, showErrorCalls=TRUE 警告やエラーの出処をたどって表示する。 Pythonほどわかりやすくないが、ちょっとはマシになる。 defaultPackages 起動時(.First() 実行よりは後)に自動で読み込むパッケージを指定する。 環境変数 R_DEFAULT_PACKAGES からも変更可能。 デフォルトは datasets, utils, grDevices, graphics, stats, methods. conflicted と tidyverse を加えて横着したいところだが、 そうすると肝心の衝突チェックが機能しなくなる。 前者だけを加えた上で次のようにフックを設定すれば自動読み込みでチェック有効: setHook(packageEvent(\u0026#34;conflicted\u0026#34;, \u0026#34;attach\u0026#34;), \\(...) library(tidyverse)) ちなみに conflicted は knitr chunk 内や withr::local_package() では動かない。 \u0026ldquo;conflicted is designed specifically for use in interactive sessions\u0026rdquo; とのこと。\nライブラリの管理 https://cran.r-project.org/doc/manuals/R-admin.html#Add_002don-packages\n~/.R/Makevars パッケージをソースコードからビルドするときの設定。 大概はビルド済みのものを入れるので不要だけど、たまに必要になる。\n例えば、素のmacOSでOpenMP依存のパッケージをビルドしようとすると clang: error: unsupported option '-fopenmp' などと怒られる。 Homebrewで gcc か llvm をインストールし、 そっちを使ってビルドするように ~/.R/Makevars で指定する:\n# gcc CC=/usr/local/bin/gcc-8 CXX=/usr/local/bin/g++-8 # llvm CC=/usr/local/opt/llvm/bin/clang CXX=/usr/local/opt/llvm/bin/clang++ CPPFLAGS, CFLAGS, CXXFLAGS, LDFLAGS なども設定可能。\nhttps://cran.r-project.org/doc/manuals/r-release/R-exts.html#Using-Makevars\n",
  "href": "/rstats/config.html",
  "tags": [
   "r"
  ],
  "title": "R環境設定",
  "type": "rstats"
 },
 {
  "content": "初学者向け講義資料 Rを用いたデータ解析の基礎 2023-04 東北大学 (学部3年生向け進化学実習の一部) 導入: データ解析の全体像。Rを使うメリット。Rの基本。 データの可視化、レポート作成 データ前処理: 抽出、集約、結合、変形 データ解釈の基礎知識 統計モデリング: 確率分布、尤度、一般化線形モデル Rを用いたデータ解析の基礎と応用 (石川由希 2022 名古屋大学):\n初心者に寄り添ってさらに噛み砕いた説明。 実験生物学における「とにかくt検定」脱却を目指した統計解析パートも実践的で充実。 よくあるエラー集が特に重宝。 R環境のインストール R本体 コマンドを解釈して実行するコア部分 よく使われる関数なども標準パッケージとして同梱 https://cran.rstudio.com/ からダウンロードしてインストール RStudio Desktop Rをより快適に使うための総合開発環境(IDE) 必須ではないけど、結構みんな使ってるらしい https://rstudio.com/ からダウンロードしてインストール スクリプトを保存 R のコンソールにコマンドを打ち込むと、即座に結果が返ってくる。 一度きりで済む短い処理ならそのように対話的な方法が分かりやすいかもしれないが、 別のデータにも使い回すとか、タイプミスでやり直すとか、 同じような処理を何度か繰り返す場合には(大抵はそうなる)、 一連の処理をまとめてスクリプト(テキストファイル)に書き出しておくとよい。 ファイルの拡張子は .txt でも何でもいいが .R にすることが多い。\nR Markdown を使えばコードと解析結果の図表を同時に保存・プレゼンすることができる。\nパッケージ 便利な関数やサンプルデータなどをひとまとめにしたもの。 R開発チームが公式に作ってるものから、 ユーザーが自分用に作ってアップロードしたものまで、さまざまある。 自分でやろうとしてることは既に誰かがやってくれてる可能性が高いので、 車輪の再発明をする前に、まずは既存のパッケージを調べるべし。 プログラミングは、イチから「書く」のではなく、 既存のパーツを利用して「組む」感覚のほうが近い。\nStandard Packages https://stat.ethz.ch/R-manual/R-devel/doc/html/packages.html\nR の標準機能。 何もしなくても使用可能な状態になっているので、 パッケージであることはあまり意識しなくてもいい。\nbase c(), data.frame(), sum() などホントに基本的なもの graphics, grDevices, grid plot() などグラフ描画関連 stats anova(), glm(), t.test() など統計解析関連 utils help(), install.packages(), read.table() など ほかに compiler, datasets, methods, parallel, splines, stats4, tcltk, tools\nRecommended Packages R と一緒にインストールされるが、 使用する前に library() で呼び出しておく必要があるパッケージ。\n例えば MASS に入ってる stepAIC() を使うには\nstepAIC(model) ## Error: could not find function \u0026#34;stepAIC\u0026#34; library(MASS) stepAIC(model) # OK ほかに boot, class, cluster, codetools, foreign, KernSmooth, lattice, Matrix, mgcv, nlme, nnet, rpart, spatial, survival\nContributed Packages 数千ものパッケージが有志により開発され、CRANにまとめて公開されている。\n例えば rstudio.com/products/rpackages で紹介されているもの、特にHadley Wickhamらによる tidyverse パッケージ群 (ggplot2, dplyr, tidyr, purrr, readr, stringrなど) はどんな解析にも有用で、標準になってもいいくらい便利。 というより、tidyverseなき裸のRは使う気が起きない。 Rの中から下記のようなコマンドで一括インストール・読み込みできる。\ninstall.packages(\u0026#34;tidyverse\u0026#34;) # 一度やればOK library(conflicted) # 安全のおまじない library(tidyverse) # 読み込みはRを起動するたびに必要 update.packages() # たまには更新しよう そのほか https://cran.r-project.org/web/views/ で用途別に紹介されている。\nパッケージを作るには devtools を使う。\n作業ディレクトリ Rではどこかのディレクトリ(=フォルダ)に身をおいて作業する。 データファイルを読み込むときなどに Rがファイルを探すのはこの作業ディレクトリである。 No such file or directory と怒られる場合は、 作業ディレクトリとファイルの場所が合っていないかったり、 ファイル名のタイプミスだったりすることが多い。\n作業ディレクトリは解析の途中でホイホイ移動せず、 プロジェクトの最上階などに固定しておいて、 そこからの相対パスでファイルを読み書きするのが分かりやすくて安全。\ngetwd(), setwd() 現在地 working directory をget/setする関数。 getwd() ## [1] /Users/watal setwd(\u0026#34;~/Desktop\u0026#34;) getwd() ## [1] /Users/watal/Desktop list.files(path = \u0026quot;.\u0026quot;, ...) または dir(path = \u0026quot;.\u0026quot;, ...) ディレクトリ内のファイルを列挙する関数。 path を省略するとワーキングディレクトリが対象となる read.table(\u0026#34;mydata.txt\u0026#34;, header = TRUE) ## Warning in file(file, \u0026#34;rt\u0026#34;) : ## cannot open file \u0026#39;mydata.txt\u0026#39;: No such file or directory ## Error in file(file, \u0026#34;rt\u0026#34;) : cannot open the connection list.files() ## [1] mydate.txt ファイル名が微妙に違う!! R で調べる help(topic, package = NULL, ...) ヘルプを表示する。 知りたいオブジェクトの先頭にクエスチョンマークを付けるという方法もある。 以下の2つは等価 help(sum) ?sum help.start() 組み込みヘルプをブラウザで開く。 インストール済みパッケージのヘルプも Packages のリンクから見られる。 attributes(obj), str(obj) オブジェクトの属性や構造を調べる。 $names に列挙されてる属性はダラー $ を挟んで取り出せる x = rnorm(100) y = rnorm(100) mymodel = lm(y ~ x) attributes(mymodel) ## $names ## [1] \u0026#34;coefficients\u0026#34; \u0026#34;residuals\u0026#34; \u0026#34;effects\u0026#34; \u0026#34;rank\u0026#34; ## [5] \u0026#34;fitted.values\u0026#34; \u0026#34;assign\u0026#34; \u0026#34;qr\u0026#34; \u0026#34;df.residual\u0026#34; ## [9] \u0026#34;xlevels\u0026#34; \u0026#34;call\u0026#34; \u0026#34;terms\u0026#34; \u0026#34;model\u0026#34; ## $class ## [1] \u0026#34;lm\u0026#34; mymodel$coefficients ## (Intercept) x ## -0.1494812 0.1218096 str(mymodel) ## List of 12 ## $ coefficients : Named num [1:2] -0.171 -0.198 ## ..- attr(*, \u0026#34;names\u0026#34;)= chr [1:2] \u0026#34;(Intercept)\u0026#34; \u0026#34;x\u0026#34; ## $ residuals : Named num [1:100] -0.5143 -1.3148 0.1954 1.6039 -0.0875 ... ## ..- attr(*, \u0026#34;names\u0026#34;)= chr [1:100] \u0026#34;1\u0026#34; \u0026#34;2\u0026#34; \u0026#34;3\u0026#34; \u0026#34;4\u0026#34; ... ## $ effects : Named num [1:100] 1.2027 2.0571 0.1179 1.8384 -0.0946 ... ## ..- attr(*, \u0026#34;names\u0026#34;)= chr [1:100] \u0026#34;(Intercept)\u0026#34; \u0026#34;x\u0026#34; \u0026#34;\u0026#34; \u0026#34;\u0026#34; ... インターネットで調べる・尋ねる r-wakalang \u0026mdash; Slack上の日本語コミュニティ https://r4ds.hadley.nz/ \u0026mdash; R for Data Science (体系的に学びたい人はぜひ通読を) https://www.rdocumentation.org/ \u0026mdash; 各パッケージの公式ドキュメントを閲覧 Stack Overflow データ読み込み See readr.\nデータ処理・整形 See dplyr, tidyr, and purrr.\nグラフ作図 2D: See ggplot2.\n3D: See rgl.\n関連書籍 Rのモダンな使い方、考え方を学ぶにはr4ds。 まずは公開オンライン版を読んでみて。\n即戦力が欲しい場合は、新しく日本語で書かれた宇宙本が取っつきやすい:\n統計モデルの基礎は緑本で:\n",
  "href": "/rstats/intro.html",
  "tags": [
   "r"
  ],
  "title": "R自学自習の基礎知識",
  "type": "rstats"
 },
 {
  "content": "https://www.htslib.org/\n操作 https://www.htslib.org/doc/samtools.html\n閲覧・要約 view BAM/CRAMをプレーンテキストのSAMとして閲覧: samtools view -h --no-PG aln.bam | less ここで -h をつけないとヘッダーが削れてしまうし、 --no-PG をつけないと元ファイルに含まれていなかった情報 (@PG) が付加されてしまうので要注意。\nファイルに含まれるヘッダー情報を改変せずに閲覧する samtools view --header-only --no-PG のショートカットとして samtools head サブコマンドが追加された。 BAM/CRAMに変換するのも、リードをフィルターするのもこのコマンド。名前が悪い。 tview マッピングされた形でインタラクティブに閲覧: samtools tview aln.bam [ref.fa] 参照配列を与えると表示方法に選択肢が増える。 予めインデックスを作っておかなくても初回実行時に faidx が勝手に走って ref.fa.fai が作成される。\nヘルプ: shift/ ジャンプ: g または / して chr1:10000 のような形式 idxstats インデックス作成済みBAMの統計量を表示: chr1 249250621 6343976 0 chr10 135534747 2407204 0 chr11 135006516 3773511 0 chr12 133851895 3696141 0 ... seqnames, seqlengths, mapped reads, unmapped reads\nflagstat フラグの要約統計: 65182282 + 0 in total (QC-passed reads + QC-failed reads) 6895859 + 0 secondary 0 + 0 supplementary 0 + 0 duplicates 65182282 + 0 mapped (100.00%:nan%) 58286423 + 0 paired in sequencing 29382202 + 0 read1 28904221 + 0 read2 52470794 + 0 properly paired (90.02%:nan%) 55907370 + 0 with itself and mate mapped 2379053 + 0 singletons (4.08%:nan%) 456098 + 0 with mate mapped to a different chr 169500 + 0 with mate mapped to a different chr (mapQ\u0026gt;=5) 下ごしらえ PCR duplicatesを除去\nsamtools collate でリード名ごとに並べる。 順番は関係ないので sort -n より collate のほうが効率的。 アラインメント直後は大概こうなっていて省略可能。 samtools fixmate -m で MC, ms タグを付加。 samtools sort で位置順にソート。 samtools markdup -r で重複を除去。 適当な条件でフィルタリング:\nsamtools view -hb -f3 -q2 aln.bam -o filtered.bam bgzipでFASTAやGFFを圧縮。 インデックス(.gzi)を利用して部分的に展開して高速アクセスすることが可能。 普通の gzip としても展開可能。 拡張子はデフォルトで .gz だけど .bgz にすることもある。\nインデックス作成:\nsamtools index → BAMインデックス (.bam.bai) samtools faidx → 参照配列インデックス (.fa.fai) tabix → タブ区切りゲノムポジションインデックス (.bgz.tbi)\nいろんな形式を扱える(-p gff|bed|sam|vcf)。 位置順ソート且つbgzip圧縮されている必要がある。 bgzip -r → BGZFインデックス (.gz.gzi)\nbgzip済みfastaを faidx するとついでに作ってもらえるし、 tabix にはおそらく込み込みなので、明示的に作ることは少ない。 variant calling https://samtools.github.io/bcftools/howtos/variant-calling.html https://samtools.github.io/bcftools/bcftools.html VCF/BCFを書き出す:\nbcftools mpileup -f ref.fa aln.bam | bcftools call -mv -Ob -o calls.bcf 以前はsamtoolsの機能だったが、bcftoolsが担うことになった。\nSAM形式 https://www.htslib.org/doc/sam.html\nhttps://samtools.github.io/hts-specs/\nQNAME: リード名\nFLAG: マッピングの状況をビット表現:\n0x001 1 paired in sequencing 0x002 2 mapped in proper pair 0x004 4 unmapped 0x008 8 mate unmapped 0x010 16 reverse strand 0x020 32 mate reverse strand 0x040 64 first read in pair 0x080 128 second read in pair 0x100 256 not primary alignment 0x200 512 not passing platform/vendor quality checks 0x400 1024 PCR or optical duplicate 0x800 2048 supplementary alignment paired-end でだいたいちゃんと張り付いたものをとるには -f 3 (= 1 + 2)\nマップされたリード数を数えたいときなどは -F 256 で各リードが1度だけ登場するようにフィルタできるが、 0x100 が立っててもmultiple hitで同点優勝してる可能性があり、 このときどれがprimaryになるかはマッパー依存。 重複遺伝子などを考慮する場合はむやみに捨ててはいけない。\n数字とフラグを変換してくれる便利 web app: https://broadinstitute.github.io/picard/explain-flags.html\nstrandも場合分けして詳細に数え上げた例: https://ppotato.wordpress.com/2010/08/25/samtool-bitwise-flag-paired-reads/\nRNAME: 参照配列・染色体の名前\nPOS: 位置\nMAPQ: マッピングクオリティ = round($-10\\log_{10}\\text{Pr[mapping~is~wrong]}$)\nマッパーによって微妙に違うらしい。 例えばTopHatでは:\n50: unique 3: 2 locations 2: 3 locations 1: 4–9 locations 0: \u0026gt;10 locations CIGAR: マッピング状況とその長さ e.g., 101M, 18M200I83M, 65M36S:\nM: alignment match I: insertion to the reference D: deletion from the reference N: skipped (= intron for mRNA-to-genome; undefined otherwise) S: soft clipping = SEQ の中で無視された部分 H: hard clipping = SEQ の外で無視された部分。先頭と末尾にのみ存在。 P: padding =: sequence match X: sequence mismatch M|I|S|=|X の長さを足せば SEQ の長さになる。\nRNEXT: paired-endの他方が張り付いた染色体。 同じときは = で不明なときは *\nPNEXT: paired-endの他方が張り付いた位置\nTLEN: inferred Template LENgth。 左腕の左端から右腕の右端までの距離。 左腕なら正、右腕なら負、single-endなら0。\nSEQ: 塩基配列\nQUAL: 塩基クオリティ\nそれ以降はマッパー依存。 形式は TAG:VTYPE:VALUE\nCRAM https://www.htslib.org/workflow/cram.html\n参照配列からの差分だけを保持することで、BAMよりもコンパクトになりやすい。 裏を返せば、このCRAM単体では完結できない操作も出てくるので扱いに注意が必要。 BAMを置き換えて一般ユーザーの主流になるにはキャッシュの設計がイマイチな気がするけど、 種数あたりのサンプル数・リード数が多くなるほど恩恵も大きくなるからオッケー、なのかなぁ。\n参照配列を探しに行く優先順位 https://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES\nsamtoolsを呼ぶときの明示的なオプション, e.g., --reference. M5 タグのハッシュ値 → 環境変数 $REF_CACHE. 次の $REF_PATH 参照でダウンロードが生じた場合の保存先。 中身は無圧縮の生FASTAなので容量に注意。 ハッシュ値のプレースホルダ %s を含める。 %2s/%s のようにするとMD5先頭2文字を消費してディレクトリ構造を作れる。 これはファイル数が増えすぎて起こる問題を回避するため。 デフォルトは ${XDG_CACHE_HOME}/hts-ref/%2s/%2s/%s or ${HOME}/.cache/hts-ref/%2s/%2s/%s. 見えない場所で勝手に容量が膨らんでいくのも恐ろしいし、 すぐ消してしまいそうな場所にあるのも恐ろしいので変えたほうがいい。 M5 タグのハッシュ値 → 環境変数 $REF_PATH. 普通の PATH と同じようにコロン区切りで $REF_CACHE 同様 %s を含む。 デフォルトは http://www.ebi.ac.uk/ena/cram/md5/%s. つまり REF_PATH が空だと UR タグより先にインターネットに読みに行った挙げ句、 無圧縮の生FASTAを勝手に保存する凶悪仕様。 UR タグに書かれたファイル。ローカルのみ可、リモートは不可。 無理やり相対パスにすることも可能だが、 CRAMファイルからではなくコマンド実行時の $PWD からの相対なので実質使えない。 設定例 https://www.ebi.ac.uk/ena に参照配列があって扱う種数も多くなければ、特に設定せずともそれなりに使える。 新しい参照配列が必要になるたびに大きめのダウンロードが発生することと、 それがホーム以下の見えないところに溜まっていくことだけ我慢。\n参照配列を自分で管理するなら M5 を無効化して UR のみで運用すればトラフィックやキャッシュの心配が無くなり単純。\nexport REF_CACHE=/dev/null export REF_PATH=$REF_CACHE パスの変更などでタグを編集したい場合は samtools reheader が使える。\n勝手にEBIを見に行くのは止めたいけど M5 は使いたい場合、 何らかの文字を REF_PATH に入れておけばいい。 公式例に習って $REF_CACHE を入れておく:\nexport REF_CACHE=\u0026#34;${HOME}/db/hts-ref-cache/%2s/%2s/%s\u0026#34; export REF_PATH=\u0026#34;$REF_CACHE\u0026#34; 手元のファイルを REF_CACHE に配置するには付属のスクリプトが使える。 FASTAに複数の配列が入っていてもMD5は1本ずつ計算される。\nseq_cache_populate.pl -root ${REF_CACHE%%/\\%*} \u0026lt;(gunzip -c genome.fa.gz) 大元の参照配列置き場はテキトーに決めてもよくなるけど、 キャッシュを管理するのもそれはそれで難しそう。\n",
  "href": "/bio/samtools.html",
  "tags": [
   "genetics"
  ],
  "title": "SAMtools",
  "type": "bio"
 },
 {
  "content": "",
  "href": "/search.html",
  "tags": null,
  "title": "Search",
  "type": "page"
 },
 {
  "content": "https://www.openssh.com/\n公開鍵によるパスワード無しの認証 秘密鍵 ~/.ssh/id_ed25519 を持つホストから、 公開鍵 ~/.ssh/authorized_keys を持つホストにログインできるようにする。\n手元のコンピュータ1台につき1ペア生成して、公開鍵をリモートホストに登録するのが基本。 リモートホストごとに違う鍵を使うことも可能。\n手元のコンピュータで鍵ペア (秘密鍵 id_ed25519 と公開鍵 id_ed25519.pub) を作成:\nssh-keygen -t ed25519 # Generating public/private ed25519 key pair. # Enter file in which to save the key (~/.ssh/id_ed25519): \u0026lt;return\u0026gt; # Enter passphrase (empty for no passphrase): パスフレーズ\u0026lt;return\u0026gt; # Enter same passphrase again: パスフレーズ\u0026lt;return\u0026gt; # Your identification has been saved in ~/.ssh/id_ed25519 # Your public key has been saved in ~/.ssh/id_ed25519.pub Ed25519を使えない古い環境ではRSA (-t rsa -b 4096) やECDSA (-t ecdsa -b 521) を使う。 鍵ファイルの名前はデフォルトのままreturnが簡単。 特定のリモートサーバー用に別の鍵を用意する場合だけ変える。 使用中のものを上書きしてしまわないように注意。 コメントは「この鍵がどこで作られたか」の人間用メモ。 ホスト名がまともならデフォルト -C username@hostname の形でいい。 ここで入力するパスフレーズはログインパスワードではなく鍵の暗号化に使われる。 パスフレーズ無しのほうが断然楽ちんなのでそうされることが多いが、 ssh-agentやKeyChainを設定しておけば入力をほとんど省略できるので、 簡単なものでも設定しておいたほうが鍵ファイル流出に強くなる。 できあがった鍵ペアのうち公開鍵のほう ~/.ssh/id_ed25519.pub をリモートホストの ~/.ssh/authorized_keys に追加する。 やり方は場合によって異なる。\nウェブブラウザからアップロードする方法。 (e.g., GitHub \u0026gt; Settings)。 ターミナルからコマンドで送り込む方法 (パスワードなど別の認証でログインできる場合): ssh-copy-id -i ~/.ssh/id_ed25519.pub watal@example.com # or ssh watal@example.com \u0026#34;mkdir ~/.ssh; touch ~/.ssh/authorized_keys\u0026#34; cat id_ed25519.pub | ssh watal@example.com \u0026#34;cat \u0026gt;\u0026gt; ~/.ssh/authorized_keys\u0026#34; ~/.ssh/ ディレクトリ内のパーミッションを確認:\nls -al ~/.ssh # drwx------ . # -rw------- authorized_keys # -rw------- config # -rw------- id_ed25519 # -rw-r--r-- id_ed25519.pub # -rw------- known_hosts 外に出していい公開鍵 .pub 以外はユーザー本人だけが読み書きできるように。 必要があれば次のようなコマンドで修正:\nchmod 700 ~/.ssh chmod 600 ~/.ssh/id_ed25519 chmod 600 ~/.ssh/authorized_keys 設定 ~/.ssh/config ssh する側のユーザー毎の設定。 設定項目は ssh_config と同じ。 先に書いたものが優先されるので、一般設定は最後に:\nHost beast Hostname 192.168.6.66 User eddie Host *.ddbj.nig.ac.jp User heavywatal RequestTTY yes Host * PasswordAuthentication no KbdInteractiveAuthentication no GSSAPIAuthentication no StrictHostKeyChecking no VisualHostKey yes AddKeysToAgent yes UseKeychain yes IdentityFile ~/.ssh/id_ed25519 ユーザー名やアドレスをセットにしてニックネームをつけることで、入力を省略できる。 例えば上のような設定を ~/.ssh/config に書いておけば以下の２つは等価:\nssh eddie@192.168.6.66 ssh beast macOSで UseKeychain yes にしておけばパスフレーズ入力は最初の1回だけで済む。 パスフレーズ無しのような手軽さで秘密鍵の流出に強くなる。 AddKeysToAgent yes を設定せず ssh-add -l が The agent has no identities. を返す状態でも行ける。\n/etc/ssh_config man ssh_config\nssh する側のコンピュータ全体の設定。 普通は ~/.ssh/config で事足りるのでいじらない。 サーバー間の通信を調整する管理者向け。\n/etc/sshd_config man sshd_config\nssh される側のデーモンの設定。余計な入り口を塞ぐべし:\nPermitRootLogin no PasswordAuthentication no KbdInteractiveAuthentication no KerberosAuthentication no GSSAPIAuthentication no UsePAM no PermitRootLogin without-password は prohibit-password に取って代わられた。 KbdInteractiveAuthentication ChallengeResponseAuthentication は取って代わられた。 ファイル転送 ファイルひとつならscpでもいいけどそれ以上なら rsync を使ったほうがよい。 あるいはsshfsでマウントしてしまうのも楽ちん。\n環境変数 echo $SSH_CONNECTION [client IP] [client port] [server IP] [server port] どこから ssh したか、を取得するには echo $SSH_CONNECTION | awk '{print $1}'\n",
  "href": "/dev/ssh.html",
  "tags": [
   "shell",
   "communication"
  ],
  "title": "ssh",
  "type": "dev"
 },
 {
  "content": "Geometric distribution 幾何分布 確率 p のBernoulli試行で k 回目に初めて当たる確率。 まず外し続ける必要がある。\n\\[\\begin{aligned} \\text{Prob}[X = k] \u0026= p(1 - p)^{k - 1} \\\\ \\mathrm E[X] \u0026= \\frac 1 p \\\\ \\text{Var}[X] \u0026= \\frac {1 - p} p \\end{aligned}\\] Negative binomial distribution 負の二項分布 確率 p のBernoulli試行で r 回目の当たりが k 回目に出る確率。 幾何分布を r 回畳み込んだもの。\n\\[\\begin{aligned} \\text{Prob}[X = k] \u0026= {k - 1 \\choose r - 1} p^r (1 - p)^{k - r} \\\\ \\mathrm E[X] \u0026= \\frac r p \\\\ \\text{Var}[X] \u0026= \\frac {r(1 - p)} {p^2} \\end{aligned}\\] Exponential distribution 指数分布 幾何分布の連続時間バージョン。 時間あたり $\\lambda$ 回起こるPoisson過程で時間 $t$ まで起こらない確率 $Q(t)$\n\\[\\begin{aligned} Q(t) \u0026= \\lim_{\\mathrm dt \\to 0} (1 - \\lambda \\mathrm dt) ^ {t/ \\mathrm d t}\\\\ \u0026= \\lim_{n \\to \\infty} (1 - \\frac {\\lambda t} n) ^ n\\\\ \u0026= e ^ {-\\lambda t} \\end{aligned}\\] ネイピア数\n\\[\\begin{aligned} \\lim_{x \\to \\infty} \\left(1 + \\frac 1 x \\right)^x \u0026= e\\\\ \\lim_{x \\to \\infty} \\left(1 + \\frac a x \\right)^x \u0026= \\lim_{y \\to \\infty} \\left(1 + \\frac 1 y \\right)^{ay}\\\\ \u0026= e^a \\end{aligned}\\] 別の考え方\n\\[\\begin{aligned} Q(t + \\mathrm dt) \u0026= Q(t) (1 - \\lambda \\mathrm dt)\\\\ Q(t + \\mathrm dt) - Q(t) \u0026= -\\lambda Q(t) \\mathrm dt\\\\ \\frac {\\mathrm dQ(t)} {\\mathrm dt} \u0026= -\\lambda Q(t)\\\\ \\frac 1 {Q(t)} \\mathrm dQ(t) \u0026= -\\lambda \\mathrm dt\\\\ \\ln Q(t) \u0026= -\\lambda t\\\\ Q(t) \u0026= e ^ {-\\lambda t} \\end{aligned}\\] $t$ らへんで初めて起こる確率は、 $t$ まで起こらず次の瞬間に起こる確率 $Q(t) \\lambda \\mathrm dt = \\lambda e ^ {-\\lambda t} \\mathrm dt$ であり、それを全区間たすと1になる\n\\[\\begin{aligned} \\int _0^\\infty \\lambda e ^ {-\\lambda t} \\mathrm dt = \\left. -e ^ {\\lambda t} \\right\\rvert _0^\\infty = 1 \\end{aligned}\\] よって、最初の事象が起こるまでの待ち時間 $t$ の分布、 すなわち確率密度関数 (PDF: probability density function) は\n\\[\\begin{aligned} f(t; \\lambda) \u0026= \\lambda e ^ {-\\lambda t} \\\\ \\mathrm E[t] \u0026= \\frac 1 \\lambda \\\\ \\text{Var}[t] \u0026= \\frac 1 {\\lambda^2} \\end{aligned}\\] また、$t$ までに最低1回は起こる確率、 すなわち累積分布関数 (CDF: cumulative distribution function) は\n\\[\\begin{aligned} F(t; \\lambda) \u0026= \\int _0^t f(x, \\lambda) \\mathrm dx \u0026= \\int _0^t \\lambda e ^ {-\\lambda x} \\mathrm dx\\\\ \u0026= \\left. -e ^ {-\\lambda x} \\right\\rvert _0^t\\\\ \u0026= 1 - e ^ {-\\lambda t} \\end{aligned}\\] これは起こらない確率 $Q(t)$ を1から引いたものに等しい。\nGamma distribution パラメータの取り方によって書き方がいくつかある。 shape k (\u0026gt;0), scale θ (\u0026gt;0) を使った一般的な表し方では、 平均待ち時間 θ の事象が k 回起こるまでの待ち時間の分布と見なせる。\n\\[\\begin{aligned} f(x; k, \\theta) \u0026= \\frac {x^{k-1} \\exp[-\\frac x \\theta]} {\\theta^k \\Gamma(k)} \\\\ \\mathrm E[x] \u0026= k\\theta \\\\ \\text{Var}[x] \u0026= k\\theta^2 \\\\ \\end{aligned}\\] rate parameter $\\lambda = \\frac 1 \\theta$ を使った形だと導出もイメージしやすい。 時間当たり λ 回起こるPoisson過程で k 回起こるまでの待ち時間の分布、 すなわち指数分布を k 個畳み込んだ分布。 cf. /lectures/wakeley-2-2\n\\[\\begin{aligned} f(t; k, \\lambda) \u0026= \\lambda e^{-\\lambda t} \\frac {(\\lambda t)^{k-1}} {\\Gamma(k)} \\\\ \\mathrm E[t] \u0026= \\frac k \\lambda \\\\ \\text{Var}[t] \u0026= \\frac k {\\lambda^2} \\\\ \\end{aligned}\\] 内部的に k ステップからなる過程が時間 $\\mu = \\frac k \\lambda = k\\theta$ で完了する。\n\\[\\begin{aligned} f(t; k, \\mu) \u0026= e^{-\\left(\\frac k \\mu t \\right)} \\frac {k^k t^{k-1}} {\\mu^k \\Gamma(k)} \\\\ \\mathrm E[t] \u0026= \\mu \\\\ \\text{Var}[t] \u0026= \\frac {\\mu^2} k \\\\ \\end{aligned}\\] Weibull distribution 指数関数がmemorylessな待ち時間なのに対して、 こちらはある時間まで事象が起こらなかったという記憶ありの待ち時間\n\\[\\begin{aligned} f(t; k, \\lambda) \u0026= \\frac k \\lambda \\left(\\frac t \\lambda \\right)^{k - 1} \\exp[-\\left(\\frac t \\lambda \\right)^k] \\\\ F(t; k, \\lambda) \u0026= 1 - \\exp[-\\left(\\frac t \\lambda \\right)^k] \\end{aligned}\\] Scale parameter: $\\lambda$ Shape parameter: k ",
  "href": "/bio/stochastic_process.html",
  "tags": [
   "genetics",
   "math"
  ],
  "title": "Stochastic Process",
  "type": "bio"
 },
 {
  "content": " R標準のbaseパッケージが提供する関数でも文字列処理は可能だが、 stringrのほうが統一的なインターフェイスに合理的な挙動で使いやすい。\nfactor と character を同じように扱う 引数オブジェクトの各要素の名前や位置を保持する 長さゼロのオブジェクトを引数として与えた場合には長さゼロの結果を返す 引数オブジェクトに NA が含まれる場合はその部分の結果を NA とする 対象文字列が一貫して第一引数で、パターンが二番目 何をやる関数なのか名前から分かりやすい\n(標準が覚えにくすぎ: grep, grepl, regexpr, gregexpr, regexec) ICU4C (via stringi) を使って動くため高速 ICU正規表現 の仕様が明確 今や stringr は stringi のラッパーだし、 どちらもほぼ同じインターフェイスなので、 もし前者に不足があれば後者を直接使えばよいが、 普通に使う分にはそんな場面には出くわさない。 むしろ、機能がある程度絞られているほうが取っ付き易いし、 str_* のほうが stri_* よりも1文字短いので、 基本的には stringr を使っとけばよい。\ntidyverse に含まれているので、 install.packages(\u0026quot;tidyverse\u0026quot;) で一括インストール、 library(tidyverse) で一括ロード。\nhttps://r4ds.hadley.nz/strings.html base R関数との対応表 Functions Basic Operation str_length(string) 文字列の長さを数える。 base::nchar(x) と相同。 str_length(c(\u0026#34;NA\u0026#34;, NA)) [1] 2 NA str_sub(string, start = 1, end = -1) 文字列を部分的に参照・変更する。 base::substr() と相同だが、負数で末尾からの位置を指定できる。 ただしRのインデックスは1始まりで終端も含むの。 str_sub\u0026lt;- が定義されているので置換にも使える。 str_sub(\u0026#34;supercalifragilisticexpialidocious\u0026#34;, 10, -15) [1] \u0026#34;fragilistic\u0026#34; str_flatten(string, collapse = \u0026quot;\u0026quot;) 文字列vectorを1つの文字列に結合する。 base::paste0(string, collapse = \u0026quot;\u0026quot;) と同等だが NA を扱える。 str_flatten(c(\u0026#34;Dragon\u0026#34;, NA, \u0026#34;Force\u0026#34;)) [1] NA str_flatten(c(\u0026#34;Dragon\u0026#34;, NA, \u0026#34;Force\u0026#34;), na.rm = TRUE) [1] \u0026#34;DragonForce\u0026#34; paste0(c(\u0026#34;Dragon\u0026#34;, NA, \u0026#34;Force\u0026#34;), collapse = \u0026#34;\u0026#34;) [1] \u0026#34;DragonNAForce\u0026#34; str_c(..., sep = \u0026quot;\u0026quot;, collapse = NULL) 複数の引数で与えた文字列を結合する。 デフォルトの sep がスペースじゃないので base::paste0() に近い。 str_c(c(\u0026#34;Dragon\u0026#34;, \u0026#34;Hammer\u0026#34;), c(NA, \u0026#34;\u0026#34;), c(\u0026#34;Force\u0026#34;, \u0026#34;Fall\u0026#34;)) [1] NA \u0026#34;HammerFall\u0026#34; paste0(c(\u0026#34;Dragon\u0026#34;, \u0026#34;Hammer\u0026#34;), c(NA, \u0026#34;\u0026#34;), c(\u0026#34;Force\u0026#34;, \u0026#34;Fall\u0026#34;)) [1] \u0026#34;DragonNAForce\u0026#34; \u0026#34;HammerFall\u0026#34; str_split(string, pattern, n = Inf, simplify = FALSE) 文字列を分割してlistを返す base::strsplit(x, split) の改良版。 string と pattern の要素数が噛み合わないときにちゃんと警告が出る。 最大 n 個に分割するということを指定できる。 simplify = TRUE とするとmatrixで返す。 str_split(c(\u0026#34;DragonForce\u0026#34;, \u0026#34;HammerFall\u0026#34;), \u0026#34;(?\u0026lt;=[a-z])(?=[A-Z])\u0026#34;) [[1]] [1] \u0026#34;Dragon\u0026#34; \u0026#34;Force\u0026#34; [[2]] [1] \u0026#34;Hammer\u0026#34; \u0026#34;Fall\u0026#34; str_split(c(\u0026#34;DragonForce\u0026#34;, \u0026#34;HammerFall\u0026#34;), \u0026#34;(?\u0026lt;=[a-z])(?=[A-Z])\u0026#34;, simplify = TRUE) [,1] [,2] [1,] \u0026#34;Dragon\u0026#34; \u0026#34;Force\u0026#34; [2,] \u0026#34;Hammer\u0026#34; \u0026#34;Fall\u0026#34; str_split_1(\u0026#34;DragonForce\u0026#34;, \u0026#34;(?\u0026lt;=[a-z])(?=[A-Z])\u0026#34;) [1] \u0026#34;Dragon\u0026#34; \u0026#34;Force\u0026#34; str_split_i(c(\u0026#34;DragonForce\u0026#34;, \u0026#34;HammerFall\u0026#34;), \u0026#34;(?\u0026lt;=[a-z])(?=[A-Z])\u0026#34;, 1) [1] \u0026#34;Dragon\u0026#34; \u0026#34;Hammer\u0026#34; str_split_1(string, pattern) は1つの文字列を受け取ってvectorを返す簡略版。 str_split_i(string, pattern, i) は分割してできたi番目の要素だけをvectorで返す亜種。 str_split_fixed(string, pattern, n) は n 必須、 simplify = TRUE 固定でmatrixを返すショートカット。 data.frame内の文字列を分割したい場合は tidyr::separate*() 系の関数を使う。 str_dup(string, times) 指定した回数だけ文字列を繰り返して結合。 base::strrep() と同等。 str_dup(\u0026#34;pizza\u0026#34;, 10) [1] \u0026#34;pizzapizzapizzapizzapizzapizzapizzapizzapizzapizza\u0026#34; Pattern Matching str_count(string, pattern) マッチする箇所の数を返す。 str_detect(string, pattern, negate = FALSE) マッチするかどうか logical を返す。 nagate = TRUE で結果を反転。 base::grepl(pattern, x) と相同。 正規表現を覚えてなくても始まりと終わりだけ手軽にマッチできる str_starts(), str_ends() もある。 str_extract(string, pattern), str_extract_all(string, pattern) マッチした部分文字列を取り出す。しなかった要素には NA。 数値＋単位のような文字列から数値部分だけを抜き出すには readr::parse_number() が便利。 str_subset(string, pattern, negate = FALSE) x[str_detect(x, pattern)] のショートカット。 マッチする要素だけ元の形で返すので str_extract() より base::grep(pattern, x, value = TRUE) に近い。 str_which(string, pattern, negate = FALSE) マッチする要素のインデックスを整数で返す which(str_detect(x, pattern)) のショートカット。 base::grep(pattern, x) と相同。 str_locate(string, pattern) マッチする最初の箇所の start, end 位置を行列で返す。 str_match(string, pattern), str_match_all(string, pattern) マッチした部分文字列を取り出し、後方参照を含む行列を返す。 str_extract(string, pattern) と同じ結果全体 \\0 が1列目で、 カッコでマッチさせた \\1 以降の結果が2列目以降に入る。 str_replace(string, pattern, replacement) マッチしなかった部分をそのままに、マッチした部分を置換する。 base::sub(pattern, replacement, x) と相同。 base::gsub() のように全てのマッチを置換するには str_replace_all() 。 str_remove() はマッチした部分を消すためのショートカット。 上記関数のpattern引数は普通に文字列を渡すと正規表現として解釈してくれるが、 下記の関数を通して渡すことでその挙動を変更することができる。\nstringr::regex(pattern, ignore_case = FALSE, multiline = FALSE, comments = FALSE, dotall = FALSE, ...) デフォルトのICU正規表現。 複数行ファイルに対するマッチではこの関数を通して挙動をいじることになる。 stringr::fixed(pattern) 正規表現ではなくそのままの文字としてマッチさせる stringr::boundary(type = \u0026quot;character\u0026quot;, skip_word_none = NA, ...) 境界に対するマッチ。 typeの選択肢は character, line_break, sentence, word. stringr::coll(pattern, ignore_case = FALSE, locale = NULL, ...) よくわからないけど非ascii対策？ Formatting str_to_upper(), str_to_lower(), str_to_title(), str_to_sentence() 大文字・小文字の変換 str_glue(..., .sep = \u0026quot;\u0026quot;, .envir = parent.frame()) 渡された文字列の中の {R表現} を評価して埋め込む。 sprintf() よりも使い方が簡単。ライバルは paste0() とか。 str_interp() はこれに取って代わられた。 str_glue(\u0026#34;fruit[1] is {fruit[1]}.\u0026#34;) fruit[1] is apple. data.frameの流れるpipe上では str_glue_data() が便利: mtcars |\u0026gt; str_glue_data(\u0026#34;mean(disp) is {mean(disp)}.\u0026#34;) mean(disp) is 230.721875. 本家の library(glue) にはほかのオプションもある。 それでもPythonのf-stringのような簡易フォーマッタは無くてちょっと不便。 str_pad(string, width, side = c(\u0026quot;left\u0026quot;, \u0026quot;right\u0026quot;, \u0026quot;both\u0026quot;), pad = \u0026quot; \u0026quot;) 文字列の幅を width に伸ばして side 側を pad で埋める。 str_pad(c(\u0026#34;9\u0026#34;, \u0026#34;10\u0026#34;), 3L, pad = \u0026#34;0\u0026#34;) [1] \u0026#34;009\u0026#34; \u0026#34;010\u0026#34; str_trim(string, side = \u0026quot;both\u0026quot;) 端の空白文字を除去する。 Python でいうところの string.strip()。 str_squish() は両端trimしたうえに内部の連続する空白文字を1つに縮める亜種。 str_trim(\u0026#34; trim me \u0026#34;) [1] \u0026#34;trim me\u0026#34; str_squish(\u0026#34; trim me \u0026#34;) [1] \u0026#34;trim me\u0026#34; str_trunc(string, width, side = c(\u0026quot;right\u0026quot;, \u0026quot;left\u0026quot;, \u0026quot;center\u0026quot;), ellipsis = \u0026quot;...\u0026quot;) 一定の長さを超えたら捨てて ... にする。 str_wrap(string, width = 80, indent = 0, exdent = 0) 指定した幅で折り返す。 indent は先頭行の左余白。 exdent はそれ以外の行の左余白。 文字列と数値の型変換はstringrの管轄外なので、標準の as.character() や as.double() などを使うか、 readr::parse_*()系の関数 を使う。\nRの文字列と正規表現 ダブルクォーテーションで挟んで作る。 文字列の中に \u0026quot; を含む場合はシングルクォーテーションで挟む。\ns = \u0026#34;This is a string.\u0026#34; s = \u0026#39;This is a string with \u0026#34;double quotes\u0026#34;.\u0026#39; エスケープシーケンス バックスラッシュを使って改行 \\n やタブ \\t などの制御文字を表現できる。 バックスラッシュ自体を表すためには \\\\ のように重ねる必要がある。\nstring = \u0026#34;x\\ty\\n0\\t1\\n\u0026#34; print(string) [1] \u0026#34;x\\ty\\n0\\t1\\n\u0026#34; cat(string) x\ty 0\t1 readr::read_tsv(I(string)) x y 1 0 1 See ?Quotes\n正規表現 ICU正規表現からよく使うやつを抜粋。\nメタ文字 意味 \\d 数字 \\s 空白 \\w 英数字 . 何でも ^ 行頭 $ 行末 \\D, \\S, \\W のように大文字にすると反転してそれ以外にマッチ。\n演算子 意味 ? 0回か1回 * 0回以上繰り返し + 1回以上繰り返し {n,m} n回以上m回以下 XXX(?=YYY) YYYに先立つXXX (?\u0026lt;=YYY)XXX YYYに続くXXX 生文字列 数字にマッチする正規表現を書こうとして pattern = \u0026quot;\\d\u0026quot; とすると怒られる。 先述のようにバックスラッシュそのものを表すには二重にしておく必要があるため。\n\u0026#34;\\d\u0026#34; # Error: \u0026#39;\\d\u0026#39; is an unrecognized escape in character string starting (\u0026lt;input\u0026gt;:1:3) \u0026#34;\\\\d\u0026#34; # Good. エスケープシーケンスを無効にした生文字列(raw string)を用いることでバックスラッシュを重ねずに済む。 PythonやC++などでは前からあったけどRでもようやく4.0.0 から使えるようになった。\npattern = \u0026#34;\\\\d\u0026#34; pattern = r\u0026#34;(\\d)\u0026#34; pattern = R\u0026#34;(\\d)\u0026#34; pattern = r\u0026#34;---(\\d)---\u0026#34; pattern = r\u0026#34;---[\\d]---\u0026#34; pattern = r\u0026#34;---{\\d}---\u0026#34; stringr::str_count(\u0026#34;1q2w3e4r\u0026#34;, pattern) 関連書籍 ",
  "href": "/rstats/stringr.html",
  "tags": [
   "r",
   "tidyverse"
  ],
  "title": "stringr",
  "type": "rstats"
 },
 {
  "content": " data.frameを縦長・横広・入れ子に変形・整形するためのツール。 dplyr や purrr と一緒に使うとよい。 reshape2 を置き換えるべく再設計された改良版。\ntidyverse に含まれているので、 install.packages(\u0026quot;tidyverse\u0026quot;) で一括インストール、 library(tidyverse) で一括ロード。\nhttps://r4ds.hadley.nz/data-tidy.html https://github.com/tidyverse/tidyr vignette(\u0026quot;tidy-data\u0026quot;) demo(package = \u0026quot;tidyr\u0026quot;) https://speakerdeck.com/yutannihilation/tidyr-pivot パイプ演算子 |\u0026gt; についてはdplyrを参照。\nPivoting: 縦長 ↔ 横広 https://tidyr.tidyverse.org/articles/pivot.html\ntidyr::pivot_longer() で縦長にする 複数列のmatrix的にまたがっていた値を1列にまとめ、元の列名をその横に添えることで、 data.frameを横広(wide-format)から縦長(long-format)に変形する。 reshape2::melt(), tidyr::gather() の改良版。\ntidyr::pivot_longer(data, cols, names_to = \u0026quot;name\u0026quot;, ..., values_to = \u0026quot;value\u0026quot;, ...)\ncols 動かしたい値が含まれている列。 コロンで範囲指定、文字列、 selection helpersなども使える。 動かさない列を ! で反転指定するのほうが楽なことも多い。 names_to 元々列名だったものを入れる列の名前 values_to 値の移動先の列名 anscombe_long = anscombe |\u0026gt; tibble::rowid_to_column(\u0026#34;id\u0026#34;) |\u0026gt; print() |\u0026gt; pivot_longer(!id, names_to = \u0026#34;namae\u0026#34;, values_to = \u0026#34;atai\u0026#34;) |\u0026gt; print() id x1 x2 x3 x4 y1 y2 y3 y4 1 1 10 10 10 8 8.04 9.14 7.46 6.58 2 2 8 8 8 8 6.95 8.14 6.77 5.76 3 3 13 13 13 8 7.58 8.74 12.74 7.71 4 4 9 9 9 8 8.81 8.77 7.11 8.84 5 5 11 11 11 8 8.33 9.26 7.81 8.47 6 6 14 14 14 8 9.96 8.10 8.84 7.04 7 7 6 6 6 8 7.24 6.13 6.08 5.25 8 8 4 4 4 19 4.26 3.10 5.39 12.50 9 9 12 12 12 8 10.84 9.13 8.15 5.56 10 10 7 7 7 8 4.82 7.26 6.42 7.91 11 11 5 5 5 8 5.68 4.74 5.73 6.89 id namae atai 1 1 x1 10.00 2 1 x2 10.00 3 1 x3 10.00 4 1 x4 8.00 -- 85 11 y1 5.68 86 11 y2 4.74 87 11 y3 5.73 88 11 y4 6.89 tidyr::pivot_wider() で横広にする 1列にまとまっていた値を、別の変数に応じて複数の列に並べ直すことで、 data.frameを縦長(long-format)から横広(wide-format)に変形する。 reshape2::dcast(), tidyr::spread() の改良版。\ntidyr::pivot_wider(data, ..., id_cols = NULL, names_from = name, values_from = value, values_fill = NULL, values_fn = NULL)\n..., id_cols ここで指定した列のユニークな組み合わせが変形後にそれぞれ1行になる。 !で反転指定、:で範囲指定、文字列、tidyselect関数なども使える。 デフォルトでは names_from と values_from で指定されなかった列すべて。 names_from 新しく列名になる列。\u0026ldquo;name\u0026rdquo; という列名なら省略可能。 values_from 動かしたい値が入っている列。\u0026ldquo;value\u0026rdquo; という列名なら省略可能。 values_fill 存在しない組み合わせのセルを埋める値。 列によって値を変えたい場合は名前付きリストで渡す。 values_fn id_cols の組み合わせが一意に定まらず複数のvalueを1セルに詰め込む場合の処理関数。 デフォルトでは警告とともに list() が使われる。 anscombe_long |\u0026gt; pivot_wider(names_from = namae, values_from = atai) |\u0026gt; dplyr::select(!id) x1 x2 x3 x4 y1 y2 y3 y4 1 10 10 10 8 8.04 9.14 7.46 6.58 2 8 8 8 8 6.95 8.14 6.77 5.76 3 13 13 13 8 7.58 8.74 12.74 7.71 4 9 9 9 8 8.81 8.77 7.11 8.84 -- 8 4 4 4 19 4.26 3.10 5.39 12.50 9 12 12 12 8 10.84 9.13 8.15 5.56 10 7 7 7 8 4.82 7.26 6.42 7.91 11 5 5 5 8 5.68 4.74 5.73 6.89 カテゴリカル変数を指示変数(ダミー変数)に変換するのにも使える:\npg = PlantGrowth |\u0026gt; dplyr::slice(c(1, 2, 11, 12, 21, 22)) |\u0026gt; print() weight group 1 4.17 ctrl 2 5.58 ctrl 3 4.81 trt1 4 4.17 trt1 5 6.31 trt2 6 5.12 trt2 pg |\u0026gt; tibble::rowid_to_column(\u0026#34;id\u0026#34;) |\u0026gt; dplyr::mutate(name = group, value = 1L) |\u0026gt; tidyr::pivot_wider(values_fill = 0L) |\u0026gt; dplyr::select(!c(id, ctrl)) weight group trt1 trt2 1 4.17 ctrl 0 0 2 5.58 ctrl 0 0 3 4.81 trt1 1 0 4 4.17 trt1 1 0 5 6.31 trt2 0 1 6 5.12 trt2 0 1 tidyr::pivot_* 関数のもっと高度なオプション names_sep や names_pattern を指定して names_to, name_from に複数の値を渡すと tidyr::separate() / tidyr::unite() 的な操作も同時にやってしまえる:\nanscombe |\u0026gt; tibble::rowid_to_column(\u0026#34;id\u0026#34;) |\u0026gt; pivot_longer(!id, names_to = c(\u0026#34;axis\u0026#34;, \u0026#34;group\u0026#34;), names_sep = 1L) |\u0026gt; print() |\u0026gt; pivot_wider(id_cols = id, names_from = c(axis, group), names_sep = \u0026#34;_\u0026#34;) id axis group value 1 1 x 1 10.00 2 1 x 2 10.00 3 1 x 3 10.00 4 1 x 4 8.00 -- 85 11 y 1 5.68 86 11 y 2 4.74 87 11 y 3 5.73 88 11 y 4 6.89 id x_1 x_2 x_3 x_4 y_1 y_2 y_3 y_4 1 1 10 10 10 8 8.04 9.14 7.46 6.58 2 2 8 8 8 8 6.95 8.14 6.77 5.76 3 3 13 13 13 8 7.58 8.74 12.74 7.71 4 4 9 9 9 8 8.81 8.77 7.11 8.84 -- 8 8 4 4 4 19 4.26 3.10 5.39 12.50 9 9 12 12 12 8 10.84 9.13 8.15 5.56 10 10 7 7 7 8 4.82 7.26 6.42 7.91 11 11 5 5 5 8 5.68 4.74 5.73 6.89 VADeaths |\u0026gt; as.data.frame() |\u0026gt; print() |\u0026gt; rownames_to_column(\u0026#34;age\u0026#34;) |\u0026gt; pivot_longer(!age, names_to = c(\u0026#34;region\u0026#34;, \u0026#34;sex\u0026#34;), names_sep = \u0026#34; \u0026#34;, values_to = \u0026#34;death\u0026#34;) Rural Male Rural Female Urban Male Urban Female 50-54 11.7 8.7 15.4 8.4 55-59 18.1 11.7 24.3 13.6 60-64 26.9 20.3 37.0 19.3 65-69 41.0 30.9 54.6 35.1 70-74 66.0 54.3 71.1 50.0 age region sex death 1 50-54 Rural Male 11.7 2 50-54 Rural Female 8.7 3 50-54 Urban Male 15.4 4 50-54 Urban Female 8.4 -- 17 70-74 Rural Male 66.0 18 70-74 Rural Female 54.3 19 70-74 Urban Male 71.1 20 70-74 Urban Female 50.0 names_transform に関数を指定すると、 列名だったものに適用される。 例えば型変換に使える:\nanscombe |\u0026gt; tibble::rowid_to_column(\u0026#34;id\u0026#34;) |\u0026gt; tidyr::pivot_longer(!id, names_to = c(\u0026#34;axis\u0026#34;, \u0026#34;group\u0026#34;), names_sep = 1L, names_transform = list(group = as.integer)) |\u0026gt; tidyr::pivot_wider(id_cols = c(id, group), names_from = axis) |\u0026gt; dplyr::select(!id) |\u0026gt; dplyr::arrange(group) group x y 1 1 10 8.04 2 1 8 6.95 3 1 13 7.58 4 1 9 8.81 -- 41 4 19 12.50 42 4 8 5.56 43 4 8 7.91 44 4 8 6.89 names_prefix を使えば、列名の頭に共通して付いてた文字を消せる:\nanscombe |\u0026gt; dplyr::select(starts_with(\u0026#34;x\u0026#34;)) |\u0026gt; pivot_longer(everything(), names_prefix = \u0026#34;x\u0026#34;) name value 1 1 10 2 2 10 3 3 10 4 4 8 -- 41 1 5 42 2 5 43 3 5 44 4 8 names_to に \u0026quot;.value\u0026quot; という特殊な値を渡すことで、 旧列名から新しい列名が作られ、複数列への縦長変形を同時にできる。 また names_to = c(\u0026quot;name\u0026quot;, NA) のように不要な列を捨てることもできる。\ntidy_anscombe = anscombe |\u0026gt; pivot_longer( # 縦長に変形したい everything(), # すべての列について names_to = c(\u0026#34;.value\u0026#34;, \u0026#34;group\u0026#34;), # x, yを列名に、1, 2, 3をgroup列に names_sep = 1L, # 切る位置 names_transform = list(group = as.integer) # 型変換 ) |\u0026gt; dplyr::arrange(group) |\u0026gt; # グループごとに並べる print() # ggplotしたい形！ group x y 1 1 10 8.04 2 1 8 6.95 3 1 13 7.58 4 1 9 8.81 -- 41 4 19 12.50 42 4 8 5.56 43 4 8 7.91 44 4 8 6.89 See https://speakerdeck.com/yutannihilation/tidyr-pivot?slide=67 for details.\nNested data.frame \u0026mdash; 入れ子構造 https://tidyr.tidyverse.org/articles/nest.html\ntidyr::nest(data, ..., .by = NULL, .key = NULL, .names_sep = NULL) data.frameをネストして(入れ子にして)、list of data.frames のカラムを作る。 内側のdata.frameに押し込むカラムを ... に指定するか、 外側に残すカラムを ! で反転指定する。\ndiamonds |\u0026gt; nest(NEW_COLUMN = !cut) |\u0026gt; dplyr::arrange(cut) cut NEW_COLUMN 1 Fair \u0026lt;tbl_df [1610 x 9]\u0026gt; 2 Good \u0026lt;tbl_df [4906 x 9]\u0026gt; 3 Very Good \u0026lt;tbl_df [12082 x 9]\u0026gt; 4 Premium \u0026lt;tbl_df [13791 x 9]\u0026gt; 5 Ideal \u0026lt;tbl_df [21551 x 9]\u0026gt; # equivalent to diamonds |\u0026gt; nest(NEW_COLUMN = c(carat, color:z)) |\u0026gt; dplyr::arrange(cut) diamonds |\u0026gt; nest(.by = cut, .key = \u0026#34;NEW_COLUMN\u0026#34;) |\u0026gt; dplyr::arrange(cut) diamonds |\u0026gt; dplyr::group_nest(cut, .key = \u0026#34;NEW_COLUMN\u0026#34;) diamonds |\u0026gt; dplyr::nest_by(cut, .key = \u0026#34;NEW_COLUMN\u0026#34;) |\u0026gt; dplyr::ungroup() .key を使う方法でそれを省略すると data という名前の列になる。\ncf. Hadley Wickham: Managing many models with R (YouTube)\ntidyr::unnest(data, cols, ...) ネストされたdata.frameを展開してフラットにする。 list of data.framesだけでなく、list of vectorsとかでもよい。\nネストされた列が複数ある場合に曖昧なコードにならないよう cols を明示的に指定することが求められる。\ndiamonds |\u0026gt; nest(NEW_COLUMN = !cut) |\u0026gt; unnest(cols = NEW_COLUMN) 方向を明示する tidyr::unnest_longer(), tidyr::unnest_wider() もある。\nその他の便利関数 tidyr::separate() 文字列カラムを任意のセパレータで複数カラムに分割。 tidyr::unite() の逆。 reshape2::colsplit() に相当。\ntidyr::separate(data, col, into, sep = \u0026quot;[^[:alnum:]]\u0026quot;, remove = TRUE, convert = FALSE, extra = \u0026quot;warn\u0026quot;, fill = \u0026quot;warn\u0026quot;, ...)\ncol 切り分けたい列の名前 into 切り分けたあとの新しい列名を文字列ベクタで sep = \u0026quot;[^[:alnum:]]\u0026quot; セパレータを正規表現で。デフォルトはあらゆる非アルファベット。 整数を渡すと位置で切れる。例えば A4 を 1L で切ると A と 4 に。 remove = TRUE 切り分ける前の列を取り除くかどうか convert = FALSE 切り分け後の値の型変換を試みるか extra = \u0026quot;warn\u0026quot; 列数が揃わないときにどうするか: warn, drop, merge fill = \u0026quot;warn\u0026quot; 足りない場合にどっち側をNAで埋めるか: warn, right, left。 つまり、文字を左詰めにするにはrightが正解(紛らわしい)。 VADeaths |\u0026gt; as.data.frame() |\u0026gt; tibble::rownames_to_column(\u0026#34;class\u0026#34;) |\u0026gt; print() |\u0026gt; tidyr::separate(class, c(\u0026#34;lbound\u0026#34;, \u0026#34;ubound\u0026#34;), \u0026#34;-\u0026#34;, convert = TRUE) class Rural Male Rural Female Urban Male Urban Female 1 50-54 11.7 8.7 15.4 8.4 2 55-59 18.1 11.7 24.3 13.6 3 60-64 26.9 20.3 37.0 19.3 4 65-69 41.0 30.9 54.6 35.1 5 70-74 66.0 54.3 71.1 50.0 lbound ubound Rural Male Rural Female Urban Male Urban Female 1 50 54 11.7 8.7 15.4 8.4 2 55 59 18.1 11.7 24.3 13.6 3 60 64 26.9 20.3 37.0 19.3 4 65 69 41.0 30.9 54.6 35.1 5 70 74 66.0 54.3 71.1 50.0 行方向に分割する tidyr::separate_rows(data, ..., sep, convert) もある。\ntidyr::extract(data, col, into, regex, ...) を使えば正規表現でもっと細かく指定できる。\n名前の似てる tidyr::extract_numeric(x) は 文字列から数字部分をnumericとして抜き出す関数だったが今はdeprecatedなので、 新しいreadr::parse_number()を使うべし。\ntidyr::unite(data, col, ..., sep = \u0026quot;_\u0026quot;, remove = TRUE, na.rm = FALSE) 複数カラムを結合して1列にする。 tidyr::separate() の逆。\npaste() とか stringr::str_c() でも似たようなことができるけど na.rm = TRUE の挙動が欲しいときに便利。\ndf = tibble(x = c(\u0026#34;x\u0026#34;, \u0026#34;x\u0026#34;, NA), y = c(\u0026#34;y\u0026#34;, NA, \u0026#34;y\u0026#34;)) df |\u0026gt; tidyr::unite(z, c(x, y), sep = \u0026#34;_\u0026#34;, remove = FALSE) z x y 1 x_y x y 2 x_NA x \u0026lt;NA\u0026gt; 3 NA_y \u0026lt;NA\u0026gt; y df |\u0026gt; tidyr::unite(z, c(x, y), sep = \u0026#34;_\u0026#34;, remove = FALSE, na.rm = TRUE) z x y 1 x_y x y 2 x x \u0026lt;NA\u0026gt; 3 y \u0026lt;NA\u0026gt; y df |\u0026gt; dplyr::mutate(z = stringr::str_c(x, y, sep = \u0026#34;_\u0026#34;)) x y z 1 x y x_y 2 x \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 3 \u0026lt;NA\u0026gt; y \u0026lt;NA\u0026gt; df |\u0026gt; dplyr::mutate(z = dplyr::coalesce(x, y)) x y z 1 x y x 2 x \u0026lt;NA\u0026gt; x 3 \u0026lt;NA\u0026gt; y y tidyr::complete(data, ..., fill = list()) 指定した列の全ての組み合わせが登場するように、 指定しなかった列に欠損値NA(あるいは任意の値)を補完した行を挿入する。\ndf |\u0026gt; complete(key1, key2, fill = list(val1 = 0, val2 = \u0026#34;-\u0026#34;)) tidyr::expand(data, ...) 指定した列の全ての組み合わせが登場するような新しいdata.frameを作る。 全ての列を指定すればcomplete()と同じ効果だが、 指定しなかった列が消えるという点では異なる。\ncrossing(...)はvectorを引数に取る亜種で、 tibble版expand.grid(...)のようなもの。\nnesting(...)は存在するユニークな組み合わせのみ残す、 nest(data, ...) |\u0026gt; dplyr::select(!data)のショートカット。 この結果はexpand()やcomplete()の引数としても使える。\n数値vectorの補完にはfull_seq(x, period, tol = 1e-6)が便利。\ntidyr::drop_na(data, ...) complete()の逆。 指定した列にNAが含まれてる行を削除する。 何も指定しなければ標準の data[complete.cases(data),] と同じ。\ntidyr::replace_na() 欠損値 NA を好きな値で置き換える。 これまでは mutate(x = ifelse(is.na(x), 0, x)) のようにしてたところを\ndf |\u0026gt; replace_na(list(x = 0, y = \u0026#34;unknown\u0026#34;)) 逆に、特定の値をNAにしたい場合は dplyr::na_if()\ntidyr::fill() NA を、その列の直前の NA でない値で埋める。 えくせるでセルの結合とかやってしまって、 最初のセルにしか値が無いような場合に使うのかな？\n関連書籍 ",
  "href": "/rstats/tidyr.html",
  "tags": [
   "r",
   "tidyverse"
  ],
  "title": "tidyr",
  "type": "rstats"
 },
 {
  "content": "https://tmux.github.io/\nGNU screen の後を継ぐ端末多重化ソフト(terminal multiplexer)。\n1つの画面の中でウインドウを追加・分割して複数の端末を開く GUIアプリのタブ代わりに。 1つのsshセッションで複数の端末を持てる。 ssh切断後も端末丸ごと継続され、後でまた繋ぎ直せる 不意のssh切断でも作業が失われない。 別の端末から接続しても同じ作業を継続できる。 nohup とかバックグラウンド化とか考えるより楽チン。 Homebrew で一発インストール: brew install tmux\nキーバインド tmux 内で prefix key に続けて特定のキーを送信すると、 そのキーに応じたさまざまなコマンドを実行できる (e.g. prefix? でキーバインドを列挙)。 prefix keyはデフォルトで C-b (controlbの略記、^bと等価) だがそれはキャレット左移動に使われるべきなので後述のように変更する。\nkey command description ? list-keys : command-prompt d detach-client c new-window n next-window p previous-window l last-window , rename-window . move-window 0 1 2 3 select-window -t :=N \u0026quot; split-window 横長・縦並びに分割: 日 % split-window -h 縦長・横並びに分割: Φ ; last-pane 直前のペイン(往復) o select-pane -t:.+ 番号順にペインを巡回 ↑↓←→ select-pane -U C-↑C-↓ resize-pane -U ペインサイズ変更 C-o rotate-window レイアウトを維持してペインを回す space next-layout レイアウトを変更する ! break-pane ペインを独立したウィンドウにする [ copy-mode ] paste-buffer コピーモード 上に戻ってスクロールしたり、その内容をコピーしたいときはコピーモードを使う。 キーボードから prefix[ で入れるほか、 set -g mouse on を設定すれば上スクロールで自然に入れる。\nコピーモードでのキー操作はデフォルトだとemacs風で、 環境変数 EDITOR/VISUAL やオプション mode-keys からviに変更できる。 シェルを介さずに直接起動する場合も考えると明示的にオプション設定しておくのが無難。\ncommand vi emacs description cancel q esc コピーモード終了 begin-selection space C-space 選択開始点をマーク copy-pipe-and-cancel enter M-w 選択範囲内を copy-command に送って終了 コピーと同時に終了せずモードや選択状態を維持したい場合はキーバインドを copy-pipe や copy-pipe-no-clear に変更する。\ncopy-pipe* の宛先はデフォルトでtmux内のバッファになっており、 ペーストはtmux内で prefix] するしかない。 macOSで次のように設定しておけば、 ⌘commandcと同じところにコピーして、 アプリを超えて⌘commandvできるようになる:\nif \u0026#34;command -v pbcopy\u0026#34; \u0026#34;set -s copy-command pbcopy\u0026#34; 設定 設定ファイル： ~/.tmux.conf\nhttps://github.com/heavywatal/dotfiles/blob/master/.tmux.conf\nprefix \u0026lt;key\u0026gt; 使えるのは ^h や ^[ のようなASCIIキャレット記法が存在するもの。 シェルやエディタであまり使わず左手だけで完結できるキーがいい。 tmux の頭文字で覚えやすい C-t がよかったけど fzf と衝突。 同じキーを bind \u0026lt;key\u0026gt; send-prefix に設定しておけば、 2回押しのうち1回分がtmuxを貫通して伝わる。 使う頻度の低いキーとの衝突ならこれで乗り切れる。 aggressive-resize [on | off] サイズの異なる端末からattachしたときにウィンドウサイズを変更する。 update-environment \u0026lt;variables\u0026gt; attachするときに環境変数を親プロセスから持ち込んで既存sessionの値を上書きする。 DISPLAY や SSH_AUTH_SOCK などがデフォルトで含まれているので、 -a オプションで追加するのが無難。 TERM_PROGRAM は特殊で、指定しても強制的に tmux に上書きされる。 showenv TERM_PROGRAM で上書き前の情報がとれるようにはなるので、 それを使って環境変数を再上書きすることは可能。 ただしattachの度にそこまで実行できないことには注意。 See tmux#3468. デタッチ後しばらくしてシェルを起動すると残存セッションを忘れがちなので、 以下のようなものを .zshrc とかに書いておけば表示で気付ける。\nif [ -n \u0026#34;$TMUX\u0026#34; ]; then eval $(tmux showenv TERM_PROGRAM) else tmux has-session \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026amp; tmux list-sessions fi open や pbcopy などがうまく働かなくて reattach-to-user-namespace が必要になる問題は既に解消された。\n利用例 リモートサーバーに ssh ログインし、 tmux の新しいセッションを開始: ssh remote.sample.com tmux ウィンドウを左右に分割し prefix%、 右ペインでPythonインタプリタを起動 python: 左ペインにフォーカスを戻しprefixo、 ファイルを閲覧したり何だり less ~/.tmux.conf 新しいウィンドウを作って prefixc、 root 仕事をしたり何だり su - ウィンドウを切り替える prefixl, prefixn, prefixp このセッションをデタッチし prefixd、 ログアウトして家に帰る exit 家からサーバーに再び ssh ログインして、 さっきの tmux セッションをアタッチして作業を再開: ssh remote.sample.com tmux attach -d セッション内のすべてのペインで exit して終了。 ",
  "href": "/dev/tmux.html",
  "tags": [
   "job",
   "shell"
  ],
  "title": "tmux",
  "type": "dev"
 },
 {
  "content": "Gene Ontology を利用して、 ある遺伝子セットにどんな機能が多めに含まれているかを解析する。\nhttps://www.bioconductor.org/packages/release/bioc/html/topGO.html\nRの中からインストール\nBiocManager::install(\u0026#34;topGO\u0026#34;) 使い方 ヒト遺伝子にランダムなスコアをつけた架空データ\nlibrary(tidyverse) library(org.Hs.eg.db) entrez_ids = mappedkeys(org.Hs.egGO) scores = runif(length(entrez_ids), 0, 1) # p-value-like # scores = rnorm(length(entrez_ids), 0, 0.4) # log2FC-like named_scores = setNames(scores, entrez_ids) topGOdata を作る tg_data = new(\u0026#34;topGOdata\u0026#34;, ontology=\u0026#34;BP\u0026#34;, allGenes=named_scores, geneSelectionFun=function(x) {x \u0026lt; 0.01}, # geneSelectionFun=function(x) {abs(x) \u0026gt; 1}, nodeSize=10, annotationFun=annFUN.org, mapping=\u0026#34;org.Hs.eg.db\u0026#34;, ID=\u0026#34;entrez\u0026#34;) ontology BP, CC, MF のどれか description (省略可) 説明string allGenes 遺伝子名を名前とするnamed vector。 値はp-valueとかlog2FCとかなんとか、好きなスコア。 名前は\u0026quot;all\u0026quot;だけど常に全遺伝子である必要はなくて、 むしろ目的に応じて適切な\u0026quot;background\u0026quot;を選んで渡すべき。 geneSelectionFun allGenes の数値を引数として今回興味のある遺伝子に TRUE を返すような関数。 スコアが閾値より大きい・小さいとか、上位100までとか。 function(p) {p \u0026lt; 0.01}, function(log2FC) {abs(log2FC) \u0026gt; 1}, function(score) {rank(score) \u0026lt;= 100L} 必要なのは statistic=\u0026quot;fisher\u0026quot; のときだけで、なおかつ実際に適用されるのは runTest() 実行時なのにここで入力させる、という筋の悪いデザイン。 スコアをそのまま使うはずの statistic=\u0026quot;ks\u0026quot; などでも要求され、 Significant genes という謎の結果が計算されてしまう。 fisherを使わない場合は結果の誤解を防ぐためにも一律 FALSE を返す関数にしておくのが安全。 function(x) {logical(length(x))} nodeSize 紐付けられた遺伝子の数がこれより少ないGO termを除外。1以上の整数。 annotationFun 遺伝子IDとGO termを結びつける関数。 後述のように org.**.**.db パッケージがあるようなメジャー種の遺伝子IDなら annFUN.org 。 チップが登録されているマイクロアレイなら annFUN.db 。 自作マップで頑張るなら annFUN.gene2GO, annFUN.GO2genes, annFUN.file 。 ... 以降は annotationFun() に渡す引数。 e.g., annFUN.org(whichOnto, feasibleGenes, mapping, ID=\u0026quot;entrez\u0026quot;) mapping IDマッピング用のBioConductorパッケージ。 BioConductor AnnotationData Packages から探す。例えばヒトなら org.Hs.eg.db 。 ID allGenes に与えた名前の種類。 entrez, genbank, alias, ensembl, symbol, genename, unigene. (annFUN.org() の中に書いてある。case-insensitive) 検定 whichAlgorithms() whichTests() so=\u0026#34;increasing\u0026#34; resClassicFisher = runTest(tg_data, algorithm=\u0026#34;classic\u0026#34;, statistic=\u0026#34;fisher\u0026#34;, sortOrder=so) resElimFisher = runTest(tg_data, algorithm=\u0026#34;elim\u0026#34;, statistic=\u0026#34;fisher\u0026#34;, sortOrder=so) resWeightFisher = runTest(tg_data, algorithm=\u0026#34;weight\u0026#34;, statistic=\u0026#34;fisher\u0026#34;, sortOrder=so) resClassicKS = runTest(tg_data, algorithm=\u0026#34;classic\u0026#34;, statistic=\u0026#34;ks\u0026#34;, sortOrder=so) resElimKS = runTest(tg_data, algorithm=\u0026#34;elim\u0026#34;, statistic=\u0026#34;ks\u0026#34;, sortOrder=so) resWeightKS = runTest(tg_data, algorithm=\u0026#34;weight01\u0026#34;, statistic=\u0026#34;ks\u0026#34;, sortOrder=so) algorithm classic: GO termをそのまま使って計算。偽陽性多め。 elim (Alexa et al. 2006): DAG上での隣接関係を考慮して補正。 下位termから検定を始め、有意なものが見つかったらそこに含まれる遺伝子を祖先ノードから除外する。 classicに比べて上位termの偽陽性が減ってconservative。 cutOff オプションでこの判定基準を 0.01 から変更可能。 weight (Alexa et al. 2006): elimを一般化して少しマイルドにしたような感じ。 有意な子ノードを多く持つ親ノードは生き残る。 classicよりconservativeだがelimより取りこぼさない。 計算が複雑すぎるせいか検定は fisher しかサポートされていない。 weight01: \u0026ldquo;mixture between the elim and the weight algorithms\u0026rdquo; とのことだが詳細は不明。 topGOデフォルトに据えるくらい自信あるんだろうけど。検定は全て可能。 lea: ドキュメントでも論文でも言及無し。お蔵入りしたプロトタイプ？ parentChild (Grossmann et al. 2007): classicよりも下位termでの偽陽性が少ない。 親を複数持つ場合の扱い2つ(union or intercection)のうちどちらを採用してるかは不明。 検定は fisher のみ。 statistic fisher: 遺伝子の数に基づいて検定。セットに含まれているか否かの二値。 ks (Ackermann and Strimmer 2009): 遺伝子のスコアや順位に基づいて検定。 閾値で遺伝子セットを区切らずに済む。 (普通のKSだったら上位への偏りだけを見ているとは限らないけどそのあたりは調整済み？) globaltest (Goeman and Bühlmann 2007): 統計量を挟まず生データに基づいて検定。 (topGOdata にどうやってデータ渡すんだろう？) t, sum, ks.ties: 不明。 scoreOrder allGenes に与えた値が小さいほど良いP値とかなら increasing (デフォルト)。 興味のある遺伝子で値が高くなるlog2FCのようなスコアなら decreasing を指定。 ドキュメントにあんまりちゃんと載ってない？ 結局どれを使うか？ algorithm はとりあえず最もconservativeで検定も自由な elim 。 もしくは作者Alexaさんを信じて weight01 。デフォルト設定は論文にも書きやすい。 statistic は、遺伝子セットが既に区切ってあるなら fisher 、 DEG解析やら何やらで遺伝子がp値やスコアを持ってるなら ks 。 globaltest を使いたい場合は 本家 globaltest パッケージ を参照。 結果は topGOresult オブジェクト。 計算されるp値は多重検定の補正をしていない生の値。 \u0026ldquo;Significant genes\u0026rdquo; は geneSelectionFun(allGenes) で TRUE になったものの数で、 無関係なはずのKSでも計算されてしまって気持ち悪い。\nstr(resElimKS) geneData(resElimKS) score(resElimKS) |\u0026gt; head() 解釈・描画 GenTable() を使って GO term Over-representation ランキングを表示できる。 複数の runTest() 産物を好きな名前で複数並べたりすることもできる。 が、P値が桁の小さい文字列型になってたり勝手に行が削られたりして怖いので使わない。 GenTable() の実装を参考に似た形式のより良いテーブルを自分で作る。\n# num_nodes=length(tg_data@graph@nodes) # GO terms \u0026gt;nodeSize # tg_table = GenTable(tg_data, # classicFisher=resClassicFisher, # elimFisher=resElimFisher, # classicKS=resClassicKS, # elimKS=resElimKS, # topNodes=num_nodes) |\u0026gt; # tibble::as_tibble() |\u0026gt; # print() annoStat = termStat(tg_data, sort(tg_data@graph@nodes)) |\u0026gt; tibble::rownames_to_column(var = \u0026#34;GO.ID\u0026#34;) |\u0026gt; tibble::as_tibble() |\u0026gt; dplyr::mutate(Term=topGO:::.getTermsDefinition(terms, ontology(tg_data), 65535L)) |\u0026gt; dplyr::relocate(Term, .after=GO.ID) |\u0026gt; print() tg_table = annoStat |\u0026gt; dplyr::mutate( classicFisher=score(resClassicFisher, whichGO=GO.ID), elimFisher=score(resElimFisher, whichGO=GO.ID), weightFisher=score(resWeightFisher, whichGO=GO.ID), classicKS=score(resClassicKS, whichGO=GO.ID), elimKS=score(resElimKS, whichGO=GO.ID), weightKS=score(resWeightKS, whichGO=GO.ID), ) |\u0026gt; print() あとは dplyr, tidyr, ggplot2 などを使って自由に整形、可視化する。\ntg_table |\u0026gt; dplyr::arrange(elimKS) tg_table |\u0026gt; dplyr::select(matches(\u0026#34;Fisher$|KS$\u0026#34;)) |\u0026gt; pairs() p0 = ggplot(tg_table) + geom_point(shape = 16, alpha = 0.5) + coord_fixed() + theme_minimal() p0 + aes(classicFisher, classicKS) p0 + aes(elimFisher, elimKS) p0 + aes(classicKS, elimKS) p0 + aes(classicKS, weightKS) p0 + aes(elimKS, weightKS) Rgraphviz を使ってDAG描画\n# BiocManager::install(\u0026#34;Rgraphviz\u0026#34;) showSigOfNodes(tg_data, score(resElimKS), firstSigNodes=6, useInfo=\u0026#34;all\u0026#34;) printGraph(tg_data, resElimKS, firstSigNodes=6, fn.prefix=\u0026#34;filename\u0026#34;, pdfSW=TRUE) 前者はプロットだけ、後者はPDFに書き出し。 significant nodeが四角で、赤いほど低いP値。\nGO terms BPterms = ls(GOBPTerm) MFterms = ls(GOMFTerm) CCterms = ls(GOCCTerm) ",
  "href": "/rstats/topgo.html",
  "tags": [
   "r",
   "bioconductor"
  ],
  "title": "topGO",
  "type": "rstats"
 },
 {
  "content": "http://www.adaptivecomputing.com/products/open-source/torque/\n環境 Ubuntu 12.04 LTS Precise Pangolin Torque 2.4.16 Usage Commands Overview\nPythonのvirtualenvは計算ノードに引き継がれないので、 メインのコマンドの前に明示的にactivateする必要あり:\necho \u0026quot;. ~/.virtualenv/project/bin/activate \u0026amp;\u0026amp; ./a.out\u0026quot; | /usr/bin/qsub [options] Installation http://tech.ckme.co.jp/torque.shtml\nhttp://www.clusterresources.com/torquedocs/\nhttp://www.adaptivecomputing.com/resources/docs/torque/2-5-9/index.php\nCompute node torque-mom と torque-client をインストール:\nsudo apt-get install torque-mom torque-client /etc/hosts を確認。 ヘッドノードのホスト名が解決できるように:\n192.168.0.254 charles ヘッドノードが複数のインターフェイスを持つ場合は 計算ノードから見えるIPを記述した行が一番上になるようにする。 そうしないと qsub でキューを出しても実行されず、 以下のような警告が静かに書き出される:\nsudo momctl -d3 WARNING: invalid attempt to connect from server 192.168.0.254:1022 (server not authorized) less /var/spool/torque/mom_logs/2012xxxx pbs_mom;Job;process_request;request type QueueJob from host charles rejected (host not authorized) pbs_mom;Req;req_reject;Reject reply code=15008(Access from host not allowed, or unknown host MSG=request not authorized), aux=0, type=QueueJob, from PBS_Server@charles ヘッドノードのホスト名を /etc/torque/server_name に設定: charles\n/var/spool/torque/mom_priv/config に設定記述 (ref. MOM Configuration)\n/home を NFSマウントしてるので scp しないように:\n$usecp *:/home /home ログファイルの末尾に計算ノードの名前を追加:\n$log_file_suffix %h pbs_mom を再起動:\nsudo service torque-mom restart Head node torque-server をインストール:\nsudo apt-get install torque-server pbs_server が動いてしまっていたら kill:\nps aux | grep pbs /etc/hosts を確認。 計算ノードのホスト名が解決できるように:\n192.168.0.1 node01 192.168.0.2 node02 192.168.0.3 node03 ヘッドノードのホスト名を /etc/torque/server_name に設定: charles\n計算ノードの情報を /var/spool/torque/server_priv/nodes に書き込む:\nnode01 np=4 node02 np=4 node03 np=4 np はコア数\n/var/spool/torque/spool のパーミションを設定:\nsudo chmod 777 /var/spool/torque/spool/ sudo chmod o+t /var/spool/torque/spool/ pbs_server を新規作成:\nsudo pbs_server -t create pbs_server の設定 (ref. Server Parameters):\nsudo qmgr -c \u0026quot;create queue batch queue_type=execution\u0026quot; sudo qmgr -c \u0026quot;set queue batch priority=0\u0026quot; sudo qmgr -c \u0026quot;set queue batch resources_default.nodes=1\u0026quot; sudo qmgr -c \u0026quot;set queue batch resources_default.ncpus=1\u0026quot; sudo qmgr -c \u0026quot;set queue batch resources_default.walltime=96:00:00\u0026quot; sudo qmgr -c \u0026quot;set queue batch keep_completed=3600\u0026quot; sudo qmgr -c \u0026quot;set queue batch enabled=true\u0026quot; sudo qmgr -c \u0026quot;set queue batch started=true\u0026quot; sudo qmgr -c \u0026quot;create queue low queue_type=execution\u0026quot; sudo qmgr -c \u0026quot;set queue low priority=-255\u0026quot; sudo qmgr -c \u0026quot;set queue low resources_default.nodes=1\u0026quot; sudo qmgr -c \u0026quot;set queue low resources_default.ncpus=1\u0026quot; sudo qmgr -c \u0026quot;set queue low resources_default.walltime=96:00:00\u0026quot; sudo qmgr -c \u0026quot;set queue low keep_completed=3600\u0026quot; sudo qmgr -c \u0026quot;set queue low enabled=true\u0026quot; sudo qmgr -c \u0026quot;set queue low started=true\u0026quot; sudo qmgr -c \u0026quot;create queue high queue_type=execution\u0026quot; sudo qmgr -c \u0026quot;set queue high priority=255\u0026quot; sudo qmgr -c \u0026quot;set queue high resources_default.nodes=1\u0026quot; sudo qmgr -c \u0026quot;set queue high resources_default.ncpus=1\u0026quot; sudo qmgr -c \u0026quot;set queue high resources_default.walltime=96:00:00\u0026quot; sudo qmgr -c \u0026quot;set queue high keep_completed=3600\u0026quot; sudo qmgr -c \u0026quot;set queue high enabled=true\u0026quot; sudo qmgr -c \u0026quot;set queue high started=true\u0026quot; sudo qmgr -c \u0026quot;set server default_queue=batch\u0026quot; sudo qmgr -c \u0026quot;set server scheduling=true\u0026quot; sudo qmgr -c \u0026quot;set server allow_node_submit = True\u0026quot; pbs_server を再起動:\nsudo service torque-server restart sudo service torque-scheduler restart 計算ノードの認識を確認 (state = free):\npbsnodes -a 試しにジョブを投入:\necho \u0026quot;sleep 30\u0026quot; | qsub 実行されているか確認 (S の下が R なら走ってる):\nqstat Maintenance / Trouble Shooting Head node 設定確認:\nqmgr -c 'p s' 計算ノードの認識を確認:\npbsnodes -a 投入したジョブの状態を確認:\nqstat qstat -q ログを見る:\nless /var/spool/torque/server_logs/2012xxxx less /var/spool/torque/sched_logs/2012xxxx サービス再起動:\nsudo service torque-server restart sudo service torque-sched restart 計算ノードが落ちたあと qstat リストに残ってしまうジョブを消す:\nsudo qdel -p Compute node 状態確認:\nsudo momctl -d3 ログを見る:\nless /var/spool/torque/mom_logs/2012xxxx サービス再起動:\nsudo service torque-mom restart ",
  "href": "/dev/torque.html",
  "tags": [
   "job",
   "concurrent"
  ],
  "title": "TORQUE",
  "type": "dev"
 },
 {
  "content": "\nBook Coalescent Theory \u0026mdash; An Introduction Author John Wakeley Publisher Roberts \u0026amp; Company Errata PDF 輪読担当 岩嵜航 日程 2015-03-20 2. Probability Theory 2.1 Fundamentals 2.1.1 Events, Probabilities, and Random Variables 2.1.2 Probability Distributions 2.2 Poisson Processes The backbone of the neutral coalescent:\nPoisson distribution the number of events that occur over a fixed period of time Exponential distribution waiting time until a first event occurs The poisson process is a counting process.\n\\[\\begin{aligned} P[K(t) = 1] \u0026= \\lambda t + o(t) \\\\ P[K(t) \\ge 2] \u0026= o(t) \\end{aligned}\\] where $K(t)$ is the number of observed events before $t$. $K(0) = 0$\n$\\lambda$ is the rate of occurrence per unit time\n$o(t)$ goes to faster than $t$ These implies, within a sufficiently short period of time, $\\delta t$, $P[K(\\delta t) = 1] \\approx \\lambda \\delta t$\n$P[K(\\delta t) \\ge 2]$ is negligible (i.e., two events don\u0026rsquo;t occur at the same time) The number of events over time 0 to $t$ (or from arbitrary starting time $s$ to $s + t$) is Poisson distributed, for $k = 0, 1, 2, \u0026hellip;$,\n\\[\\begin{aligned} P[K(t) = k] \\;\u0026=\\; \\frac {(\\lambda t)^k} {k!} e^{-\\lambda t} \\\\ P[K(t+s) - K(s) = k] \\;\u0026=\\; P[K(t) = k] \\end{aligned}\\] (Eq. 2.53) Waiting time to the first event is exponentially distiributed,\n\\[ f_T(t) = \\lambda e^{-\\lambda t} \\] memoryless (無記憶性) The number of coin-tosses required to observe the next heads is independent of previous results.\nThe waiting times between successive events are i.i.d. (independent and identically distributed). (Eq. 2.54) The waiting time $W$ until the $n$ th event (= the sum of $n$ i.i.d. wating times) can be derived by $n - 1$ successive convolutions:\n\\[\\begin{aligned} f_{W,1}(t) \u0026= f_T(t) = \\lambda e^{-\\lambda t} \\\\ f_{W,2}(t) \u0026= \\int _0^t \\lambda e^{-\\lambda (t-t')} f _{W,1}(t') dt' \\\\ \u0026= \\lambda ^2 e^{-\\lambda t} \\left[t'\\right]_0^t \\\\ \u0026= t\\lambda^2 e^{-\\lambda t} \\\\ f_{W,3}(t) \u0026= \\int _0^t \\lambda e^{-\\lambda (t-t')} f _{W,2}(t') dt' \\\\ \u0026= \\frac {t^2 \\lambda^3 e^{-\\lambda t}} {2!} \\\\ \u0026\\;\\vdots \\\\ f_{W,n}(t) \u0026= \\lambda e^{-\\lambda t} \\frac{(\\lambda t)^{n-1}} {(n - 1)!} \\end{aligned}\\] This is the gamma distribution.\n(Eq. 2.55) The mean and the variance are\n\\[\\begin{aligned} \\mathrm{E}[W] \u0026= \\sum^n \\mathrm{E}[T] \\\\ \u0026= \\sum^n \\frac 1 \\lambda \\\\ \u0026= \\frac n \\lambda \\\\ \\text{Var}[W] \u0026= \\sum^n \\text{Var}[T] \\\\ \u0026= \\sum^n \\frac 1 {\\lambda^2} \\\\ \u0026= \\frac n {\\lambda^2} \\end{aligned}\\] These hold even when $n$ is not an integer if we replace the factorial $(n - 1)!$ with gamma function, $\\Gamma(n) = \\int^\\infty_0 x^{n-1} e^{-x} dx$. (Eq. 2.57)\nThe coalescent considers the events that have a very small probability of occurring in any single generation:\ncoalescence (common ancestor event) mutation migration They will each form a Poisson process.\n2.2.1 Poisson Process Results for the Coalescent The Sum of Independent Poissons Two independent Poisson random variables: $X_1$ with occurrence rate $\\lambda _1$\n$X_2$ with occurrence rate $\\lambda _2$ (Eq. 2.58) The distribution of $Y = X_1 + X_2$ can be obtained by convolution:\n\\[\\begin{aligned} P[Y=k] \\;\u0026=\\; \\sum _{i=0}^k P[X_1=i] \\; P[X_2=k-i] \\\\ \u0026=\\; \\sum _{i=0}^k \\frac {\\lambda _1^i} {i!} e^{-\\lambda _1} \\frac {\\lambda _2^{k-i}} {(k-i)!} e^{-\\lambda _2} \\\\ \u0026=\\; e^{-\\lambda _1} e^{-\\lambda _2} \\sum _{i=0}^k \\frac {\\lambda _1^i} {i!} \\frac {\\lambda _2^{k-i}} {(k-i)!} \\frac {k!}{k!} \\\\ \u0026=\\; \\frac {e^{-(\\lambda _1 + \\lambda _2)}} {k!} \\sum _{i=0}^k {k \\choose i} \\lambda _1^i \\lambda _2^{k-i} \\\\ \u0026=\\; \\frac {e^{-(\\lambda _1 + \\lambda _2)}} {k!} (\\lambda _1 + \\lambda _2)^k \\\\ \u0026=\\; \\text{Poisson distribution with occurrence rate } \\lambda _1 + \\lambda _2 \\end{aligned}\\] The sum of independent Poisson processes is another Poisson process.\nThe Probability that the First Event Is of a Particular Type (Eq 2.60) The probability that $X_1$ is observed before $X_2$ is given simply by the relative rate of the event (i.e., as a fraction of the total rate):\n\\[\\begin{aligned} P[T_1 \u003c T_2] \u0026= \\int _0^\\infty P[T_2\u003et]\\; f _{T_1}(t) dt \\\\ \u0026= \\int _0^\\infty \\left(e^{-\\lambda _2 t} \\right) _\\text{Eq. 2.59}\\; \\lambda _1 e^{-\\lambda _1 t} dt \\\\ \u0026= \\lambda _1 \\int _0^\\infty e^{-(\\lambda _1 + \\lambda _2) t} dt \\\\ \u0026= \\lambda _1 \\left[-\\frac {e^{-(\\lambda _1 + \\lambda _2)t}} {\\lambda _1 + \\lambda _2} \\right]_0^\\infty \\\\ \u0026= \\frac {\\lambda _1} {\\lambda _1 + \\lambda _2}, \\end{aligned}\\] using (Eq 2.59)\n\\[\\begin{aligned} P[T\u003et] \\;\u0026=\\; \\int _t^\\infty \\lambda e^{-\\lambda t} dt \\\\ \u0026=\\; \\left[-e^{-\\lambda t} \\right]_t^\\infty \\\\ \u0026=\\; e^{-\\lambda t}. \\end{aligned}\\] The Time to the First Event among Independent Poissons (Eq. 2.61) The distribution of $T = \\min(T_1, T_2)$\n\\[\\begin{aligned} P[T\u003et] \\;\u0026=\\; P[\\min(T_1, T_2) \u003e t] \\\\ \u0026=\\; P[T_1 \u003e t \\;\\cap\\; T_2 \u003e t] \\\\ \u0026=\\; P[T_1 \u003e t]\\; P[T_2 \u003e t] \\\\ \u0026=\\; e^{-\\lambda _1 t} e^{-\\lambda _2 t} \\\\ \u0026=\\; e^{-(\\lambda _1 + \\lambda _2) t} \\end{aligned}\\] Therefore, $f_ {\\min(T_1, T_2)}(t) = (\\lambda _1 + \\lambda _2) e^{-(\\lambda _2 + \\lambda _1)t}$\nThere is a one-to-one correspondence between cumulative distribution $P[T \\le t]$ and probability densities $f_T(t)$\nThe Number of Events Required to See a Particular Outcome $X_2, X_2, X_2, \u0026hellip;, X_2, \\boldsymbol{X_1}$, \u0026hellip;\n(Eq. 2.62) How many $X_2$ (e.g., mutation events) occur before $X_1$ (e.g., common ancector event)?\n\\[\\begin{aligned} P[K=k] \\;\u0026=\\; P[\\text{First }X_1\\text{ occurs at }K\\text{th trial}] \\\\ \u0026=\\; P[X_2\\text{ occurs }k - 1\\text{ times first, then }X_1\\text{ occurs}] \\\\ \u0026=\\; \\left(\\frac {\\lambda _2} {\\lambda _1 + \\lambda _2} \\right)^{k-1} \\frac {\\lambda _1} {\\lambda _1 + \\lambda _2} \\end{aligned}\\] This is the geometric distribution with the rate $p = \\frac {\\lambda _1} {\\lambda _1 + \\lambda _2}$.\nThe process is just like a series of Bernoulli trials with probability of success $p = \\frac {\\lambda _1} {\\lambda _1 + \\lambda _2}$.\nTying All This Together: A Filtered Poisson Process Reinterpret $f_T(t)$ with the sum rule and product rule (see Eq. 2.7, 2.8).\n\\[\\begin{aligned} f_T(t)\\; \u0026=\\; \\sum _{k=1}^\\infty f_T(t \\mid K=k)\\; P[K=k] \\\\ \u0026=\\; \\sum _{k=1}^\\infty (\\text{Eq. }2.54) (\\text{Eq. }2.62) \\\\ \u0026=\\; \\sum _{k=1}^\\infty (\\lambda _1 + \\lambda _2) e^{-(\\lambda _1 + \\lambda _2)t} \\frac {\\{(\\lambda _1 + \\lambda _2)t\\}^{k-1}} {(k-1)!}\\; \\left(\\frac {\\lambda _2} {\\lambda _1 + \\lambda _2} \\right)^{k-1} \\frac {\\lambda _1} {\\lambda _1 + \\lambda _2} \\\\ \u0026=\\; \\lambda _1 e^{-(\\lambda _1 + \\lambda _2)t}\\; \\sum _{k=1}^\\infty \\frac {(\\lambda _2 t)^{k-1}} {(k-1)!} \\\\ \u0026=\\; \\lambda _1 e^{-(\\lambda _1 + \\lambda _2)t}\\; \\sum _{k=0}^\\infty \\frac {(\\lambda _2 t)^k} {k!} \\\\ \u0026=\\; \\lambda _1 e^{-(\\lambda _1 + \\lambda _2)t}\\; e^{\\lambda _2 t} \\\\ \u0026=\\; \\lambda _1 e^{-\\lambda _1 t} \\end{aligned}\\] Taylor series of $e^x$\n\\[\\begin{aligned} e^x \\;\u0026=\\; 1 + \\frac x 1 + \\frac {x^2} {2!} + \\frac {x^3} {3!} + \\cdots \\\\ \u0026=\\; \\sum _{k=0}^\\infty \\frac {x^k} {k!} \\end{aligned}\\] Filtered Poisson process = Poisson process with rate $\\lambda p; (=\\lambda _1)$ total occurrence rate $\\lambda = \\lambda _1 + \\lambda _2$ acceptance rate (proportion of the focal event) $p = \\frac {\\lambda _1} {\\lambda _1 + \\lambda _2}$ 2.2.2 Convolutions of Exponential Distributions The sum of the waiting times of $n$ events:\nIf the rate does not change with each event ($\\lambda _i = \\lambda \\text{ for all } i$), → gamma-distributed (Eq. 2.54) If the rate changes with each event ($\\lambda _i \\neq \\lambda _j \\text{ for } i \\neq j$), (e.g., in the study of genealogies, the rate of coalescence changes every time a coalescent event occurs)\n→ obtained by successive convolution of exponential distribution (Eq. 2.63, 2.64) \\[\\begin{aligned} f_{T_1 + T_2}(t) \u0026= \\int _0^t f_{T_1}(s)\\; f_{T_2}(t-s)ds \\\\ \u0026= \\int _0^t \\lambda _1 e^{-\\lambda _1 s}\\; \\lambda _2 e^{-\\lambda _2 (t-s)}ds \\\\ \u0026= \\lambda _1 \\lambda _2 e^{-\\lambda _2 t} \\int _0^t e^{-(\\lambda _1 -\\lambda _2)s}ds \\\\ \u0026= \\lambda _1 \\lambda _2 e^{-\\lambda _2 t} \\left[-\\frac 1{\\lambda _1 - \\lambda _2} e^{-(\\lambda _1 -\\lambda _2)s} \\right]_0^t \\\\ \u0026= \\frac {\\lambda _1} {\\lambda _1 - \\lambda _2} \\lambda _2 e^{-\\lambda _2 t} \\left(1 - e^{-(\\lambda _1 -\\lambda _2)t} \\right) \\\\ \u0026= \\frac {\\lambda _1} {\\lambda _1 - \\lambda _2} \\lambda _2 e^{-\\lambda _2 t} + \\frac {\\lambda _2} {\\lambda _2 - \\lambda _1} \\lambda _1 e^{-\\lambda _1 t} \\\\ \u0026= \\frac {\\lambda _1} {\\lambda _1 - \\lambda _2} f _{T_2}(t) + \\frac {\\lambda _2} {\\lambda _2 - \\lambda _1} f _{T_1}(t) \\\\ (\u0026= \\text{weighted sum of the original distributions})\\\\[1ex] f_{T_1 + T_2 + T_3}(t) \u0026= \\int _0^t f_{T_1 + T_2}(s)\\; f_{T_3}(t-s)ds \\\\ \u0026= \\frac {\\lambda _2} {\\lambda _2 - \\lambda _1} \\frac {\\lambda _3} {\\lambda _3 - \\lambda _1} \\lambda _1 e^{-\\lambda _1 t} + \\frac {\\lambda _3} {\\lambda _3 - \\lambda _2} \\frac {\\lambda _1} {\\lambda _1 - \\lambda _2} \\lambda _2 e^{-\\lambda _2 t} + \\frac {\\lambda _1} {\\lambda _1 - \\lambda _3} \\frac {\\lambda _2} {\\lambda _2 - \\lambda _3} \\lambda _3 e^{-\\lambda _3 t} \\\\[1ex] f_{T_1 + T_2 + T_3 + T_4}(t) \u0026= \\cdots \\\\ \u0026\\;\\vdots \\\\ f_{\\sum _{i=1}^n T_i}(t) \u0026= \\sum_{i=1}^n \\lambda _i e^{-\\lambda _i t} \\prod _{j=1,\\;j \\neq i} \\frac {\\lambda _j} {\\lambda _j - \\lambda _i} \\end{aligned}\\] We will use this to obtain the distribution of the total waiting time to the MRCA of the entire samples (Eq. 3.27)\n",
  "href": "/lectures/wakeley-2-2.html",
  "tags": [
   "genetics",
   "book"
  ],
  "title": "Wakeley輪読会 2章2節",
  "type": "lectures"
 },
 {
  "content": "run Windows apps on OS X Wine: http://www.winehq.org/ WineBottler: http://winebottler.kronenberg.org/ WineはLinuxなどのUNIX系OS上でWindowsアプリを実行できるようにするプログラム群。 WineBottlerはMac OS XでWineを使うための環境構築を手伝ってくれる。\nNote 仮想的なC:ドライブなどが含まれるひとつのWindows環境をPrefixと呼ぶ。 実行するWindowsアプリがPrefix内にある必要は無いが、 実行する環境としてひとつのPrefixを指定する必要がある。 Windowsアプリ.exeから単独のMacアプリ.appを作る方法と、 PrefixをひとつだけMacに作ってWindowsアプリをそこで実行する方法の2通りの使い方ができる。 前者の場合、アプリそれぞれに対してPrefixが作られるため Mac上に複数のWindows環境が作られることになるが、 いかにもMacネイティブっぽいアプリケーションができあがるので見栄えが良い。 後者の場合、一度Prefixを作ってしまえばあとは何も意識せず その環境で直接Windowsアプリを実行したりインストールしたりできる。 両者を使い分けても良い。 とりあえずWindowsアプリを実行できる環境としてPrefixをひとつ作っておくが、 よく使うアプリは*.appに固める、とか。 Wine や WineBottler を使うときはOSの言語設定を英語にする (要、一旦ログアウトして再ログイン)。 基本的にはもう日本語には戻さないつもりで。（と思ったけど、 Wineメニュー --\u0026gt; Configuration --\u0026gt; Desktop Integration --\u0026gt; Appearance からフォントを適切に設定すれば日本語のままでも大丈夫かも。知らんけど） Installation 作者のウェブサイト から WineBottlerCombo_*.dmg をダウンロード それを開いて、Wine.app と WineBottler.app を /Applications/ 以下にコピー make Mac Application.app from Windows Application.exe Windowsアプリ.exe またはその インストーラ.msi から 単独の Macアプリ.app を作る方法\nWineBottler.app を起動\n左サイドバーから Create Custom Prefixes を選択\nInstall File [____] に元となるWindowsアプリまたはそのインストーラを入力。 (Select File\u0026hellip;) から選んでもよい。 それがインストーラではなく単体で動くアプリケーションの場合は その下の [_] Copy only. にチェック prefix template は new prefix のままで Winetricks はここでは無視。ランタイム環境が必要な場合はここで指定しとくのかな？ Self-contained [_] にチェックすると、 Wine がインストールされてないMacでも動くアプリケーションを作れる。 が、200MBくらい大きくなるので、自分のMacで使うだけならチェックしないほうがいい。 その下の3つはとりあえず無視。 Runtime Version [____] に元のWindowsアプリのバージョンを入力。 分からなきゃそのまま1.0で。 Install ボタンを押す。 アプリケーションの名前と置く場所を聞かれるので、 /Applications などの適当な場所に 適当な名前.app を指定してSave\nWindowsっぽいインストーラが立ち上がったら基本的に全部Yesで進む。\nSelect Startfile と言われたら、 Program Files/ 以下にインストールされた目的のアプリを指定してOK\nできあがったアプリケーションをダブルクリックして実行\nこの方法で作ったアプリケーションを一度起動すると、 ~/Library/Application\\ Support/Wine/prefixes/ 以下にアプリケーションと同名のPrefixが作られ、 以後そのアプリケーションはそのPrefix環境で実行される。 (e.g. Notepad.app を立ち上げると ~/Library/Application\\ Support/Wine/prefixes/Notepad というPrefixが作られ、その環境で実行される)\nexecute Application.exe on One Prefix 実行環境としてPrefixをひとつ作っておき、Windowsアプリ.exe を直接実行する方法。 Prefixは複数作れるが、ひとつだけ ~/.wine というPrefixを作って 常にそれを使うのが分かりやすい。（LinuxにおけるWineの動作）\nPrefix Wine.app を起動\n画面右上にある ワイングラスのメニュー --\u0026gt; Change Prefix...\nAdd\u0026hellip; ホーム以下の適当な場所に適当な名前で作る (ホーム直下に .wine という名前で作ることを推奨) 間違って作ったPrefixなどはマイナスアイコンで消せる 虫眼鏡アイコンをクリックするとそのPrefixがどこにできたかFinderで確認できる 設定したいPrefixを選択してOK 右上ワイングラスのメニューで current prefix がちゃんと設定されてるか確認\nExecution 上記の手順で目的のPrefixに設定されてることを確認。\nFinderなどからWindowsアプリ (notepad.exe とか setup.msi とか) を実行\n普通はダブルクリックで行ける ダメなら右クリックから Wine.app を指定 Wineダイアログが出てきたら、 Run directly in [\u0026hellip;] でPrefixを指定してGo\nDon’t show this dialog again にチェックすれば次回からこのステップを飛ばせる そのアプリがインストーラなら、手順に従ってインストールする\n全部Yesで進むと大概 C:\\\\Program Files\\\\ 以下にインストールすることになるが、 それらはMac上では \u0026lt;Prefix\u0026gt;/drive_c/Program Files に相当する。 misc OSの言語設定を日本語にした状態でWineを使うと文字化けしたりPrefix内の設定が壊れたりするらしい。\nX11 (または XQuartz) というウィンドウシステムを使って Windowsアプリを表示するので、Wineが動いてるうちはそれらが起動した状態になる。 （何か変なの起動したとか思わないように）\nPrefixはLinuxに倣って ~/.wine とするのがおすすめだけど、 これだと不可視ファイルになってしまうので混乱する人も出てきそう。 Finderで開くには2つの方法がある\n右上の ワイングラス --\u0026gt; Change Prefix... --\u0026gt; 右端の虫眼鏡アイコン Terminalを開いて open ~/.wine を実行 インストールされたWindowsアプリを立ち上げる度にPrefixを開いて Program Files まで行くというのは面倒なので、 /Applications/ 以下にエイリアスを作るなどする。 Finder で command + alt + drag-and-drop するか、 コマンドで以下のようにする。:\nln -s ~/.wine/drive_c/Program\\ Files /Applications/ wine コマンドは /Applications/Wine.app/Contents/Resources/bin/wine にあるが、 パスなどをうまく設定するため /Applications/Wine.app/Contents/MacOS/startwine というスクリプトから動かしたほうがいい。 .zshrc などに以下のようなエイリアスを記述するとよいだろう:\nif [ $(uname) = Darwin ]; then alias wine=/Applications/Wine.app/Contents/MacOS/startwine fi これでWindowsアプリをコマンドラインから簡単に起動できる。 ちなみに \u0026lt;Prefix\u0026gt;/drive_c/windows/system32/ 以下にある 実行ファイルにはパスが通っており、拡張子.exeも省略可能:\nwine notepad シェル変数 WINEPREFIX によって wine コマンドが実行されるPrefixを指定できる。 相対パスではなく絶対パスで。デフォルトは ~/.wine:\nWINEPREFIX=${HOME}/Wine wine notepad ",
  "href": "/mac/winebottler.html",
  "tags": [
   "mac",
   "windows"
  ],
  "title": "WineBottler",
  "type": "mac"
 },
 {
  "content": "The most powerful shell https://www.zsh.org/ https://zsh.sourceforge.io/ Installation 基本的にはOSに入ってる /bin/zsh を使う。 新しいのを入れるならHomebrewを使うのが楽:\nbrew install zsh brew install zsh-completions Environment variables ZDOTDIR 設定ファイルを読み込むディレクトリ。デフォルトは ${HOME} HISTFILE コマンド履歴を保存するファイル。 デフォルトは ${ZDOTDIR}/.zsh_history っぽいけど明示的に設定しといたほうが安心。 fpath zsh関数や補完関数のパス Configuration files $ZDOTDIR 以下の個人設定ファイルが場合に応じて下記の順で読まれる。 システム全体の設定ファイルとして /etc/z* が個人設定ファイルの前に読み込まれる。\n/etc/zshenv スクリプトの実行時も含めてあらゆる場合に読み込まれ、オプションでも外せない。 Ubuntuはここで基本的な PATH を設定。 .zshenv スクリプトの実行時も含めてほぼあらゆる場合に読み込まれる。 インタラクティブ用の設定などはせず、最低限の記述に留める。 例えば ZDOTDIR, unsetopt NOMATCH など。 ここで PATH を設定したい気もするけど、 OSによっては次の /etc/zprofile で上書きされてしまう。 unsetopt GLOBAL_RCS としてそれを防ぐ手もあるけどやや危険。 /etc/zprofile Macでは /usr/libexec/path_helper が /usr/bin などの基本的な PATH を設定する。 ここで /etc/profile を source して設定するLinuxもある。 .zprofile ログインシェルとして立ち上げるときのみ読まれる。 export する環境変数(PATH とか)を設定するのに適している。 .bash_profile に対応するので共通設定を .profile に書いておいて source するとか。 例えばローカル環境Mac + リモート環境Linux CUIで開発する場合、 ターミナルもtmuxもデフォルトでログインシェルを立ち上げるので、 .zshrc に一本化してしまっても構わない。 使い分けるのはLinux GUIを使う場合とか、 よほど重い初期化をログインシェル1回で済ませたい場合とか。 .zshrc ログイン・非ログイン問わず、インタラクティブシェルとして立ち上げるときに読まれる。 こだわりが無ければだいたいどの設定もこれに書いておけば問題ない。 .zprofile と使い分けるなら setopt や autoload など、親シェルから引き継がれないものはこちら。 alias などは別ファイルを読み込む形にして .bashrc と共有。 .zlogin .zshrc より後に読まれる以外は .zprofile と同じ。使わない。 .zlogout ログアウト時にしてほしいことが万が一あれば。 起動時間短縮 まずはプロファイリングしてボトルネックを知る:\n# head of .zshenv zmodload zsh/zprof # tail of .zshrc zprof compinit とかが遅かったり複数回呼ばれていたりするので順番やオプションを変えてみる。\nGlob Qualifiers ファイルの種類を限定したり順番を変えたりできる。 setopt EXTENDED_GLOB で有効になる。\n*(/): directories *(.): plain files *(@): symbolic links *(*): executable plain files *(On), *(^on): descending order by name.\n*(Om), *(^om): ascending order by modification time, oldest first.\n例えば file と file.backup を比較したいときにただ diff file* とするとbackupのほうが後に来てしまうのを解決。 名前順と時間順でデフォルトの方向が逆なのはいずいけど仕方ない。 ",
  "href": "/dev/zsh.html",
  "tags": [
   "shell"
  ],
  "title": "zsh",
  "type": "dev"
 },
 {
  "content": "https://www.gnu.org/software/bash/manual/html_node/\nif-then Conditional Constructs Bash Conditional Expressions [ はPOSIXコマンドで ] は最後の引数。 [[ expression ]] は便利な面もあるけどBash拡張なので注意。 # basic if test -e ~; then echo \u0026#39;test -e ~\u0026#39; fi # popular if [ -e ~ ]; then echo \u0026#39;[ -e ~ ]\u0026#39; fi # bash/zsh extension; not POSIX if [[ -e ~ ]]; then echo \u0026#39;[[ -e ~ ]]\u0026#39; fi # shortcut with exit status test -e ~ \u0026amp;\u0026amp; echo \u0026#39;test -e ~ \u0026amp;\u0026amp; echo\u0026#39; || echo \u0026#39;not printed\u0026#39; [ -e ~ ] \u0026amp;\u0026amp; echo \u0026#39;[ -e ~ ] \u0026amp;\u0026amp; echo\u0026#39; || echo \u0026#39;not printed\u0026#39; [[ -e ~ ]] \u0026amp;\u0026amp; echo \u0026#39;[[ -e ~ ]] \u0026amp;\u0026amp; echo\u0026#39; || echo \u0026#39;not printed\u0026#39; [[ ! -e ~ ]] \u0026amp;\u0026amp; echo \u0026#39;not printed\u0026#39; || echo \u0026#39;[[ ! -e ~ ]] || echo\u0026#39; AND/OR\n[ -e ~ ] \u0026amp;\u0026amp; [ -d ~ ] \u0026amp;\u0026amp; echo \u0026#39;[ -e ~ ] \u0026amp;\u0026amp; [ -d ~ ]\u0026#39; [ -e ~ ] || [ -f ~ ] \u0026amp;\u0026amp; echo \u0026#39;[ -e ~ ] || [ -f ~ ]\u0026#39; [[ -e ~ \u0026amp;\u0026amp; -d ~ ]] \u0026amp;\u0026amp; echo \u0026#39;[[ -e ~ \u0026amp;\u0026amp; -d ~ ]]\u0026#39; [[ -e ~ || -f ~ ]] \u0026amp;\u0026amp; echo \u0026#39;[[ -e ~ || -f ~ ]]\u0026#39; 文字列\nEMPTY=\u0026#34;\u0026#34; NOTEMPTY=\u0026#34;content\u0026#34; [ -z \u0026#34;$EMPTY\u0026#34; ] \u0026amp;\u0026amp; echo \u0026#39;-z \u0026#34;$EMPTY\u0026#34;\u0026#39; [ -n \u0026#34;$NOTEMPTY\u0026#34; ] \u0026amp;\u0026amp; echo \u0026#39;-n \u0026#34;$NOTEMPTY\u0026#34;\u0026#39; [ \u0026#34;$NOTEMPTY\u0026#34; != \u0026#34;$EMPTY\u0026#34; ] \u0026amp;\u0026amp; echo \u0026#39;\u0026#34;$NOTEMPTY\u0026#34; != \u0026#34;$EMPTY\u0026#34;\u0026#39; [ \u0026#34;$NOTEMPTY\u0026#34; = \u0026#34;content\u0026#34; ] \u0026amp;\u0026amp; echo \u0026#39;\u0026#34;$NOTEMPTY\u0026#34; = \u0026#34;content\u0026#34;\u0026#39; [[ \u0026#34;$NOTEMPTY\u0026#34; =~ \u0026#34;tent$\u0026#34; ]] \u0026amp;\u0026amp; echo \u0026#39;\u0026#34;$NOTEMPTY\u0026#34; =~ \u0026#34;tent$\u0026#34;\u0026#39; ファイル、ディレクトリ\nx=\u0026#34;${HOME}/.bashrc\u0026#34; [ -e $x ] \u0026amp;\u0026amp; echo \u0026#34;x exists\u0026#34; [ -d $x ] \u0026amp;\u0026amp; echo \u0026#34;x is a directory\u0026#34; [ -f $x ] \u0026amp;\u0026amp; echo \u0026#34;x is a regular file\u0026#34; [ -s $x ] \u0026amp;\u0026amp; echo \u0026#34;x is a regular file with size \u0026gt;0\u0026#34; [ -L $x ] \u0026amp;\u0026amp; echo \u0026#34;x is a symlink\u0026#34; [ -r $x ] \u0026amp;\u0026amp; echo \u0026#34;x is readable\u0026#34; [ -w $x ] \u0026amp;\u0026amp; echo \u0026#34;x is writable\u0026#34; [ -x $x ] \u0026amp;\u0026amp; echo \u0026#34;x is executable\u0026#34; 数値比較\nx=1 y=2 [ $x -eq $y ] \u0026amp;\u0026amp; echo \u0026#34;x == y\u0026#34; [ $x -ne $y ] \u0026amp;\u0026amp; echo \u0026#34;x != y\u0026#34; [ $x -lt $y ] \u0026amp;\u0026amp; echo \u0026#34;x \u0026lt; y\u0026#34; [ $x -le $y ] \u0026amp;\u0026amp; echo \u0026#34;x \u0026lt;= y\u0026#34; [ $x -gt $y ] \u0026amp;\u0026amp; echo \u0026#34;x \u0026gt; y\u0026#34; [ $x -ge $y ] \u0026amp;\u0026amp; echo \u0026#34;x \u0026gt;= y\u0026#34; Looping Constructs https://www.gnu.org/software/bash/manual/html_node/Looping-Constructs.html\n基本形。クオートもカッコも不要:\nfor x in 1 2 3; do echo $x done # 1 # 2 # 3 for x in \u0026quot;1 2 3\u0026quot; のようにクオートすれば当然1要素扱いになるが、 一旦変数に代入したあとの挙動はシェルによって異なるので要注意: zshでは1要素扱い、dashやbashではスペース区切りで分割。\nSTRING=\u0026#34;1 2 3\u0026#34; for x in $STRING; do echo $x done Special Parameters https://www.gnu.org/software/bash/manual/html_node/Special-Parameters.html\n$0 # The full path of the script $1 # First argument $# # The number of arguments ($0 is not included) $* # 全ての引数。\u0026#34;$*\u0026#34; はひと括り: \u0026#34;$1 $2 $3\u0026#34; $@ # 全ての引数。\u0026#34;$@\u0026#34; は個別括り: \u0026#34;$1\u0026#34; \u0026#34;$2\u0026#34; \u0026#34;$3\u0026#34; $- # シェルの起動時のフラグ、setコマンドを使って設定したフラグの一覧 $$ # シェル自身のプロセスID $! # シェルが最後に起動したバックグラウンドプロセスのプロセスID $? # 最後に実行したコマンドのexit値 Parameter Expansion https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html\nstring=abcdef echo ${string} # abcdef echo ${#string} # 6 うっかりアンダースコアで変数名をつなげてしまいがちなので、 常に{カッコ}つける癖をつける:\nfor CITY in sendai yokosuka; do echo hello_$CITY_people # hello_ echo hello_${CITY}_people # hello_sendai_people done 変数が未定義or空だったり、空白や特殊文字を含んでいたりして事故りがちなので、 基本的にダブルクォートを付ける癖をつける:\nEMPTY=\u0026#34;\u0026#34; [ -n $EMPTY ] \u0026amp;\u0026amp; echo \u0026#34;Bug! this is printed\u0026#34; [ -n \u0026#34;$EMPTY\u0026#34; ] \u0026amp;\u0026amp; echo \u0026#34;OK, this is not printed.\u0026#34; DIR=${HOME}/directory with space # with: command not found echo $DIR # /Users/watal/directory DIR=\u0026#34;${HOME}/directory with space\u0026#34; echo $DIR # /Users/watal/directory with space cd $DIR # cd: /Users/watal/directory: No such file or directory cd \u0026#34;$DIR\u0026#34; # cd: /Users/watal/directory with space: No such file or directory シングルクォートの中の変数は展開されずそのまま渡される:\necho \u0026#34;$HOME\u0026#34; # /Users/watal echo \u0026#39;$HOME\u0026#39; # $HOME 部分列 # ${parameter:offset} # ${parameter:offset:length} echo ${string:0:3} # abc echo ${string:1:3} # bcd echo ${string:2} # cdef echo ${string: -2} # ef 置換 文字列の置換には正規表現を使える sed が便利。 でもパスや拡張子のちょっとした操作なら、 わざわざ外部コマンドやパイプを使わずにシェル(bash)の機能だけで実現できる。\n* グロブの最短マッチか最長マッチが使える。（正規表現は使えない） $@ や $* に対しては、個々の要素に対して実行されてリストが返される。 # ${STR#pattern} # sed -e \u0026#34;s/^pattern//\u0026#34;) shortest # ${STR##pattern} # sed -e \u0026#34;s/^pattern//\u0026#34;) longest # ${STR%pattern} # sed -e \u0026#34;s/pattern$//\u0026#34;) shortest # ${STR%%pattern} # sed -e \u0026#34;s/pattern$//\u0026#34;) longest # ${STR/pattern/repl} # sed -e \u0026#34;s/pattern/repl/\u0026#34;) longest # ${STR//pattern/repl} # sed -e \u0026#34;s/pattern/repl/g\u0026#34;) longest # ${STR/#pattern/repl} # sed -e \u0026#34;s/^pattern/repl/\u0026#34;) longest # ${STR/%pattern/repl} # sed -e \u0026#34;s/pattern$/repl/\u0026#34;) longest FILEPATH=/root/dir/file.tar.gz echo ${FILEPATH##*/} # file.tar.gz echo ${FILEPATH##*.} # gz echo ${FILEPATH#*.} # tar.gz echo ${FILEPATH%/*} # /root/dir echo ${FILEPATH%.*} # /root/dir/file.tar echo ${FILEPATH%%.*} # /root/dir/file echo ${FILEPATH/%.*/.zip} # /root/dir/file.zip 変数が設定されていない場合にどうするか\n# if VAR is null, then ${VAR:-WORD} # return WORD; VAR remains null ${VAR:=WORD} # return WORD; WORD is assigned to VAR ${VAR:?WORD} # display error and exit ${VAR:+WORD} # nothing occurs; otherwise return WORD Misc. Command Substitution https://www.gnu.org/software/bash/manual/html_node/Command-Substitution.html\nコマンドの結果を文字列として受け取る方法は `command` と $(command) の2つあるが、 後者のほうが入れ子など柔軟に使えるのでより好ましい。\nArithmetic Expansion https://www.gnu.org/software/bash/manual/html_node/Arithmetic-Expansion.html\n簡単な数値計算は $((expression)) でできる。\nShell Arithmetic\nProcess Substitution https://www.gnu.org/software/bash/manual/html_node/Process-Substitution.html\n1つのコマンドから出力を受け取るだけならパイプ command1 | command2 で足りるけど、 2つ以上から受け取りたいときはプロセス置換 command2 \u0026lt;(command1a) \u0026lt;(command1b) を使う。\n標準入力や標準出力を受け付けないコマンドでも、 ファイル名を引数で指定できればプロセス置換で対応できる。 例えばgzip圧縮・展開の一時ファイルを作りたくない場合とか:\nsomecommand infile outfile gunzip -c infile.gz | somecommand /dev/stdin /dev/stdout | gzip -c \u0026gt;outfile.gz somecommand \u0026lt;(gunzip -c infile.gz) \u0026gt;(gzip -c \u0026gt;outfile.gz) Arrays https://www.gnu.org/software/bash/manual/html_node/Arrays.html\nPOSIXでは未定義の拡張機能であり、bashとzshでも挙動が異なる。\narray=(a b c d e f) # bash | zsh echo ${array} # a | a b c d e f echo ${array[0]} # a | echo ${array[1]} # b | a echo ${array[-1]} # (error) | f echo ${array[@]} # a b c d e f | a b c d e f echo ${#array} # 1 | 6 echo ${#array[@]} # 6 | 6 文字列と同様にコロンを使う方法ならzshでもゼロ基準なので安全。\necho ${array[@]:0:3} # a b c echo ${array[@]:1:3} # b c d echo ${array[@]:2} # c d e f echo ${array[@]: -2} # e f array[*] とarray[@] はクオートで括られる挙動が異なる。 前者はひと括り、後者はそれぞれ括られる。\nfor x in \u0026#34;${array[*]:3}\u0026#34;; do echo $x done # d e f for x in \u0026#34;${array[@]:3}\u0026#34;; do echo $x done # d # e # f オプション シェル起動時に bash -u script.sh とするか、 スクリプト内で set -u とする。 途中で set +u として戻したりもできる。\n後述のように注意は必要だけどとりあえず走りすぎない設定で書き始めたい:\nset -eux -o pipefail -e エラーが起きたらすぐ終了する。 これが無ければスクリプトの最後まで走ろうとする。 挙動が変わる要因が多すぎて (サブシェルか、条件文がついてるか、POSIXモードか、bashバージョンなど) 逆に難しくなるので使わないほうがいいという見方もある。 それが気になるほど込み入ってきたらもはやシェルスクリプトの出番ではなく、 Pythonとかでかっちり書いたほうがよいのでは。 source している中で exit するとシェルごと落ちることにも注意。 -u 定義されていない変数を展開しようとするとエラー扱い。 -v シェルに入力されるコマンドを実行前にそのまま表示する。 よくあるverboseオプションなので、 スクリプトに書いてしまうより必要に応じてコマンドに足すほうが自然か。 -x 変数を展開してからコマンドを表示する。 -v との併用も可能。 -o pipefail パイプの左側でエラーになったら終了する。 読み手側が早めに切り上げて SIGPIPE が発生しても止まるので注意。 POSIXではないがbashでもzshでも利用可能。 -o posix bash/zsh拡張を切ってPOSIX準拠モードで動く。 sh として呼び出されるとこれになる。 関連書籍 ",
  "href": "/dev/sh.html",
  "tags": [
   "shell"
  ],
  "title": "シェルスクリプト",
  "type": "dev"
 },
 {
  "content": "https://github.com/heavywatal/scribble/blob/master/cxx/bitwise.cpp\n基本 型 きっちりサイズが定義されているものと、 下限だけが定義されてて環境依存のものがある。\nhttps://github.com/heavywatal/scribble/blob/master/cxx/sizeof.cpp\n1byte == 8bits sizeof(char) == 1 sizeof(bool) \u0026gt;= 1 sizeof(int) \u0026gt;= 4 sizeof(uint32_t) == 4 \u0026lt;cstdint\u0026gt; sizeof(uint128_t) == 16 \u0026lt;boost/multiprecision/cpp_int.hpp\u0026gt; 符号あり(signed)型の場合、左端のビットが符号を司る。\n// uint8_t 0b00000000 // 0 0b10000000 // 128 0b11111111 // 255 // int8_t 0b01111111 // 127 0b10000000 // -128 0b10000001 // -127 0b10000010 // -126 // 0b11111101 // -3 0b11111110 // -2 0b11111111 // -1 0b は binary literal (C++14) の接頭辞\n演算子 operator // AND, OR, XOR x \u0026amp; y x | y x ^ y // NOT, complement of 1 ~ x // negation, complement of 2, (~x | 1) - x // shift x \u0026lt;\u0026lt; n x \u0026gt;\u0026gt; n // assuming 8-bit x = 5 // 0b00000101 x \u0026amp; 1 // 0b00000001 = 1 x | 2 // 0b00000111 = 7 x ^ 3 // 0b00000110 = 6 ~ x // 0b11111010 = -6 or 250u - x // 0b11111011 = -5 or 251u x \u0026lt;\u0026lt; 1 // 0b00001010 = 10 x \u0026gt;\u0026gt; 1 // 0b00000010 = 2 \u0026lt;\u0026lt; は右から0を詰めて左端を捨てる unsigned に対する \u0026gt;\u0026gt; は左から0を詰めて右端を捨てる signed の負数に対する \u0026gt;\u0026gt; は未定義だが、だいたい左端をコピーして右端を捨てる(i.e., 符号を維持しつつ2で割る) 8-bit型に対する操作が必ずしも8-bitで返ってくるとは限らないし、 暗黙の整数型は大概32-bitとかなので、両辺とも明示的に型を指定するほうが安心。 特に上位ビットも変化する ~ や - を使って比較するときは要注意。 応用 符号反転 -x は、全ビット反転して1加えることに相当する (~x | 1) 。 つまり、符号反転して全ビット反転すると、1加えるのと同じ。\nx = x + 1 x += 1 ++x x = -~x std::vector\u0026lt;bool\u0026gt; 特殊化されているため普通のSTL vectorではない 省メモリ イテレータあり e.g., std::next_permutation() data() メンバが無い。つまりナマのビット列としてアクセスできない？ std::bitset\u0026lt;N\u0026gt; http://en.cppreference.com/w/cpp/utility/bitset http://www.cplusplus.com/reference/bitset/bitset/ https://cpprefjp.github.io/reference/bitset.html ビット数がコンパイル時定数\nboost::dynamic_bitset\u0026lt;\u0026gt; http://www.boost.org/doc/libs/release/libs/dynamic_bitset/dynamic_bitset.html\n可変サイズ版bitset\nfind_first(), find_next(), is_subset_of() など便利な補助関数もあるが、速度的な最適化はされてないっぽいので、 実行速度がシビアな場面では普通に operator[] で自前ループを書いたほうがいいかも。 operator\u0026amp; とかも意外と遅い。\n",
  "href": "/cxx/bitwise.html",
  "tags": [
   "c++"
  ],
  "title": "ビット演算",
  "type": "cxx"
 },
 {
  "content": "実行中プロセスを知る /Applications/Utilities/Activity Monitor.app を見るのが楽チン。\nps OSによって挙動がかなり異なるのでややこしいが、 LinuxのほうがいくらかBSDに歩み寄ってくれているらしい。 とりあえず全プロセスを表示するオプション ax とカラムをたくさん表示するオプション u をつけて less や grep に流すのが基本。\nソートの仕方は共通ではない。 出力形式の調整は共通の o オプション。\nps aux | less ## Sort on Mac ps auxm | head # by Memory ps auxr | head # by CPU ## Sort on Linux ps auxk -pmem | head # by Memory ps auxk -pcpu | head # by CPU ## Output format ps axo user,pid,pcpu,pmem,command top 一度表示して終わりではなく、q で閉じるまで一定間隔で更新される。 起動してからソート項目を切り替えられる。 プラスで昇順、マイナスで降順。\nby Linux Mac PID N o -pid CPU P o -cpu Memory M o -mem Time T o -time jobs システム全体のプロセスが見える上記コマンドとは違い、 これで見えるのはそのシェルから実行したジョブだけ。 末尾に\u0026amp;をつけてバックグラウンドで走らせたジョブや、 ctrl-z でsuspendしたジョブを眺めるのに使う（下記）。\nジョブコントロール 何らかのプログラムを実行:\ntop ここで ctrl-z を押すと、プロセスはバックグラウンドで一時停止する:\n[1] + 19310 suspended top バックグラウンドのプロセスを確認するには jobs コマンド。 左からジョブ番号、カレントジョブか否か、状態、コマンド内容:\njobs [1] + suspended top フォアグラウンドで再開するには fg コマンド。 引数としてジョブ番号かプログラム名をパーセントに続けて指定する (%1 とか %a.out とか)。 zsh なら補完もしてくれる。 引数を省略すると、jobs で + がついてるカレントジョブが選択される。 :\nfg %1 [1] + 19310 continued top 再び一時停止して、今度はバックグラウンドで再開する。コマンドは bg:\n[1] + 19310 suspended top bg %top 始めからバックグラウンドで走らせるなら末尾にアンド:\ntop \u0026amp; 中止させるには kill コマンドで SIGTERM シグナルを送る。 指定するのはジョブ番号でも生のプロセスIDでもよい。 それでも応答しないやつを強制終了するには -9 オプションをつけて SIGKILL を送るのが最後の手段。 killall は指定した文字列と前方一致するプロセスを すべて kill するショートカット。\nkill %1 kill -9 %a.out kill 19310 killall a.out ログアウト後も継続 バックグラウンドで実行中のプロセスも、 ログアウトするとhangup(HUP)シグナルによって終了してしまう。 これを無視して実行し続けるようにプログラムを起動するのが nohup。 バックグランド化までは面倒見てくれないので末尾の \u0026amp; を忘れずに:\nnohup COMMAND [ARGUMENTS] \u0026amp; 標準出力と標準エラー出力は指定しなければ nohup.out または ~/nohup.out に追記モードでリダイレクトされる。\n標準出力の書き出し先を指定するには \u0026gt;{OUTFILE} 標準エラー出力の書き出し先を指定するには 2\u0026gt;{OUTFILE} 標準エラー出力を標準出力と同じところに流すには \u0026gt;{OUTFILE} 2\u0026gt;\u0026amp;1 nohup COMMAND \u0026gt;out.log 2\u0026gt;err.log \u0026amp; ssh 接続先のサーバーで nohup ジョブを走らせるときは 標準入出力をすべて切っておかないと期待通りにsshを抜けられない:\nnohup COMMAND \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026lt;/dev/null \u0026amp; うっかり普通に開始してしまったプロセスを後から nohup 状態にするには、 一時停止して、バックグラウンドで再開して、disown に渡す:\n./a.out # control + z [1] + 19310 suspended a.out bg %1 [1] + 19310 continued a.out disown %1 See tmux.\nnohup, disown がプロセス単位で切り離すのに対して、 tmux は端末セッション丸ごと切り離し＆復帰することができる。\n関連書籍 ",
  "href": "/dev/nohup.html",
  "tags": [
   "job"
  ],
  "title": "プロセス管理",
  "type": "dev"
 },
 {
  "content": " \u0026lt;cstdlib\u0026gt; の std::rand() は乱数の質も悪く、速度も遅いので非推奨。 C++11 から標準ライブラリに追加された \u0026lt;random\u0026gt; を使う。 \u0026lt;algorithm\u0026gt; の std::random_shuffle() は引数省略で std::rand() が使われてしまうので非推奨。 C++11 で追加された std::shuffle() に生成器を明示的に渡して使う。 非標準の生成器としてはSFMTやdSFMTが高速で高品質。Xorshift系とPCGの動向にも注目。 \u0026lt;random\u0026gt; http://www.cplusplus.com/reference/random/ http://en.cppreference.com/w/cpp/numeric/random https://cpprefjp.github.io/reference/random.html C++11 ではまともに使える乱数ライブラリが追加された。 乱数生成エンジンと分布関数オブジェクトを組み合わせて使う。\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;random\u0026gt; int main() { // seed std::random_device rd; const auto seed = rd(); // engine std::mt19937 rng(seed); // probability density distribution const double mean = 0.0; const double sd = 1.0; std::normal_distribution\u0026lt;double\u0026gt; dist(mean, sd); // generate! for (size_t i=0; i\u0026lt;8; ++i) { std::cout \u0026lt;\u0026lt; dist(rng) \u0026lt;\u0026lt; std::endl; } return 0; } Mersenne Twister http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/mt.html\n松本眞と西村拓士によって開発された高速・高品質な擬似乱数生成器。 標準の \u0026lt;random\u0026gt; でも利用可能になっており、 パラメータ定義済みの std::mt19937 がよく使われる。\nSFMT http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/SFMT/\nMersenne Twisterを松本眞と斎藤睦夫がさらに改良したもの。 SIMD命令を利用して、速度も品質も向上したらしい。 標準には含まれず、ソースからのビルドが必要。\ndSFMT http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/SFMT/#dSFMT\nSFMTの double 版。 整数乱数から除算で変換するよりも良質で高速。 v2.1からは整数も出力可能。 標準には含まれず、ソースからのビルドが必要。\nインストール方法、使い方 https://github.com/heavywatal/sfmt-class/\nSFMTやdSFMTを簡単に導入するためのインストーラを作って公開した。 C++標準 \u0026lt;random\u0026gt; の std::mt19937 と同じように使えるようにしたラッパークラス (wtl::sfmt19937) も書いた。\nXorshift Marsaglia (2003) から始まって、いくつかの改良版が派生している。 省メモリで高速。 周期は $2^{128}$ ほどでMTに比べれば短いけど、大概はこれで十分。 最新情報は xoroshiro.di.unimi.it で追えば良さそう。\nPCG 線形合同法(LCG)の出力をpermutationしたPCGも良さそう。 作者O\u0026rsquo;Neillのブログは読み応えがあっておもしろい。\nSeed /dev/urandom /dev/random は擬似乱数を生成するデバイスで、 環境ノイズから乱数を生成するため自然乱数に近い。 /dev/urandom は十分なノイズが蓄積していなくても 内部プールの再利用によってすぐに生成してくれるが、 そのために /dev/random ほど安全ではない。 長期に渡って使われる暗号鍵の生成以外の目的では /dev/urandom の利用が推奨されている。\n#include \u0026lt;fstream\u0026gt; unsigned int dev_urandom() { unsigned int x; try { std::ifstream fin(\u0026#34;/dev/urandom\u0026#34;, std::ios::binary | std::ios::in); fin.exceptions(std::ios::failbit); fin.read(reinterpret_cast\u0026lt;char*\u0026gt;(\u0026amp;x), sizeof(unsigned int)); } catch (std::ios::failure\u0026amp; e) {throw std::ios::failure(\u0026#34;/dev/urandom\u0026#34;);} return x; } std::random_device C++11ではそのための関数が \u0026lt;random\u0026gt; に用意されている。 これも基本的には /dev/urandom から生成するらしい。\nstd::random_device rd; const std::random_device::result_type seed = rd(); 大量のシミュレーションを回すときなど、 エントロピーがもっと必要な場合は std::seed_seq を利用する。\n",
  "href": "/cxx/random.html",
  "tags": [
   "c++"
  ],
  "title": "擬似乱数生成器",
  "type": "cxx"
 },
 {
  "content": "用語 正方行列 (square) 行と列の数が等しい行列。 上三角行列 (upper triangular), 下三角行列(lower triangular) 対角線の左下、右上が0となる正方行列。 対角成分の積が行列式になる。 対角行列 (diagonal) 対角成分以外が0の正方行列。上三角かつ下三角。 軸方向の伸縮だけで歪まない写像。 べき乗が対角成分それぞれのべき乗だけで計算できる。 単位行列 (identity) $I, E$ 対角成分が全て1の対角行列。 正則行列 (invertible, non-singular, non-degenerate, regular) 逆行列を持つ。行列式が0じゃない。固有値0を持たない。 行列式 (determinant) $\\text{det}, A, |A|$ 正方行列による変換の体積拡大率。 行列式が0 ⇔ 写像がぺちゃんこになる ⇔ 逆行列が存在しない。 固有値の積と等しい。 跡 (trace) $\\text{tr}, A$ 正方行列の対角成分の和。 固有値の和と等しい。 核 (kernel) $\\text{Ker}, A$ $A\\mathbf{x} = \\mathbf{o}$ で原点に移るような $\\mathbf{x}$ の集合。 核が原点だけ(0次元) ⇔ ランク＝元の次元数 ⇔ 写像は 単射。 像 (image) $\\text{Im}, A$ $\\mathbf{x}$ を目一杯いろいろ動かしたときの $\\mathbf{y} = A\\mathbf{x}$ の集合。 像が行き先の全空間 ⇔ ランク＝行き先の次元数 ⇔ 写像は 全射。 ランク (rank) $\\text{rank}, A$ 像の次元数。 行列のべき乗 行列は写像。行列のべき乗は写像の繰り返し。\nベクトル $\\mathbf{x}$ に正方行列 $A$ を $t$ 回かけたらどうなるか知りたい。\n\\[ \\mathbf{x}(t) = A\\mathbf{x}(t-1) = A^t\\mathbf{x}(0) \\] そのまま行列計算をするのではなく、適当な正則行列で $\\mathbf{x}(t) = P\\mathbf{y}(t)$ という変数変換をしてみると\n\\[\\begin{aligned} \\mathbf{y}(t) \u0026= P^{-1}\\mathbf{x}(t) \\\\ \u0026= P^{-1}A\\mathbf{x}(t-1) \\\\ \u0026= P^{-1}AP\\mathbf{y}(t-1) \\\\ \u0026= (P^{-1}AP)^t\\mathbf{y}(0) \\\\ \u0026= \\Lambda^t\\mathbf{y}(0) \\\\ \\mathbf{x}(t) \u0026= P\\mathbf{y}(t) \\\\ \u0026= P\\Lambda^t\\mathbf{y}(0) \\\\ \u0026= P\\Lambda^tP^{-1}\\mathbf{x}(0) \\end{aligned}\\] このとき $\\Lambda = P^{-1}AP$ が対角行列になってくれてると $t$ 乗する計算がすごく楽チン。\n\\[\\begin{aligned} \\Lambda^t \u0026= \\text{diag}(\\lambda _1, ..., \\lambda _n)^t \\\\ \u0026= \\text{diag}(\\lambda _1^t, ..., \\lambda _n^t) \\end{aligned}\\] この 対角化 (diagonalization) をもたらす変換行列 $P$ とはどういうものか\n\\[\\begin{aligned} P^{-1}AP \u0026= \\text{diag}(\\lambda _1, ..., \\lambda _n) \\\\ AP \u0026= P \\text{diag}(\\lambda _1, ..., \\lambda _n) \\end{aligned}\\] $P = (\\mathbf{p}_1, \u0026hellip;, \\mathbf{p}_n)$ として列ごとに見ると\n\\[\\begin{aligned} A\\mathbf{p}_1 \u0026= \\lambda_1 \\mathbf{p}_1 \\\\ \\vdots \\\\ A\\mathbf{p}_n \u0026= \\lambda_n \\mathbf{p}_n \\end{aligned}\\] $A$ をかけても長さが変わるだけで方向は変わらない。 この伸縮率 $\\lambda$ が 固有値 (eigenvalue) で、それぞれに対応する $\\mathbf{o}$ でない $\\mathbf{p}$ が 固有ベクトル (eigenvector)。\nつまり変換行列 $P$ は $A$ の固有ベクトルを並べたもので、 $\\Lambda$ は対角成分に $A$ の固有値を並べたもの。 そうするとさっきの $\\mathbf{x} = P\\mathbf{y}$ は、 $\\mathbf{x}$ を $A$ の固有ベクトルの線形結合として表し、 $A$ をかけても方向が変わらないように変数変換しておくということに相当する。\n\\[\\begin{aligned} A^t\\mathbf{x} \u0026= A^t P \\mathbf{y} \\\\ \u0026= A^t (y_1\\mathbf{p_1} + ... + y_n\\mathbf{p_n}) \\\\ \u0026= y_1 A^t \\mathbf{p_1} + ... + y_n A^t \\mathbf{p_n} \\\\ \u0026= y_1 \\lambda _1^t \\mathbf{p_1} + ... + y_n \\lambda _n^t \\mathbf{p_n} \\\\ \u0026= \\lambda_k ^t ( y_1 \\left(\\frac {\\lambda _1} {\\lambda _k}\\right) ^t \\mathbf{p_1} + ... + y_k \\mathbf{p_k} + ... + y_n \\left(\\frac {\\lambda _n} {\\lambda _k}\\right) ^t \\mathbf{p_n}) \\\\ \u0026\\sim y_k \\lambda_k ^t \\mathbf{p_k} \\end{aligned}\\] $t$ が大きくなるにつれて最大の固有値 $\\lambda_k$ に対応する固有ベクトル $\\mathbf{p_k}$ の向きに近づいていく。 その極限には行かないにしても、固有値の大きな固有ベクトルの方向に寄っていく傾向があるってこと。\n固有値・固有ベクトルの求め方 行列 $A$ に対して次のような関係を満たす非自明解として固有ベクトル $\\mathbf{p}$ を求めたい。\n\\[\\begin{aligned} A\\mathbf{p} \u0026= \\lambda\\mathbf{p} \\\\ (A - \\lambda\\mathbf{I}) \\mathbf{p} \u0026= \\mathbf{o} \\end{aligned}\\] それが存在するための条件は、左にかかってる部分の行列式がゼロであること。\n\\[\\begin{aligned} | A - \\lambda\\mathbf{I} | = 0 \\end{aligned}\\] この 固有多項式 (characteristic equation) を解いて固有値 $\\lambda$ を求め、 それぞれの解に対して元の連立方程式を解き、 固有ベクトルを求める。 実数解が見つかる場合、方向の変わらない固有ベクトルが存在し、線形変換 $A$ は原点からの伸び縮みを表す。 そうでない場合、線形変換 $A$ は回転を含み、どのベクトルも方向が変わる。\n固有値が重解を含んでいてもその重複と同じ分だけ固有ベクトルが取れれば (代数的重複度＝幾何的重複度ならば) 対角化可能。 対角化できない正方行列でも $P^{-1}AP = J$ となる $P$ を見つけて Jordan標準形 まで持っていくことは可能。 $J^t$ の計算は $\\Lambda^t$ ほどじゃないにせよそれなりに楽。\n参考文献 ",
  "href": "/bio/linear_algebra.html",
  "tags": [
   "math"
  ],
  "title": "線形代数",
  "type": "bio"
 },
 {
  "content": "用語 Node / Vertex ノード、頂点。 GRNでは遺伝子。 Edge / Link エッジ、枝。ノードとノードを結ぶ線。 方向性を持つ場合と持たない場合がある。 GRNでは転写促進・抑制の制御関係なので有向。 ネットワークの特徴量 Order ネットワーク中のノードの数 Size ネットワーク中のエッジの数 Degree 次数。ひとつのノードがもつエッジの数 Average Degree 平均次数。 次数をネットワーク中のノードで平均したもの。 Density / Connectance 密度あるいは結合度。 ネットワーク中に存在しているエッジの数を、最大可能エッジ数で割ったもの。 最大可能エッジ数は、ノード数と自己制御の有無によって決まる。 自己制御あり有向グラフの最大可能エッジ数: $V ^ 2$ 自己制御なし有向グラフの最大可能エッジ数: $V (V - 1)$ 自己制御なし無向グラフの最大可能エッジ数: $V (V - 1) / 2$ Clustering Coefficient クラスター係数。 あるノードから見て、隣接する2つのノード同士もエッジで繋がっていると三角形ができる。 この三角形が多いほどクラスター係数が大きくなる。 ネットワーク中のノードについて平均したのが平均クラスタ係数。 The number of selfloops 自己制御数。 有向グラフにおいて、両端が同じノードに接続しているエッジの数。 Degree assortativity 次数相関。 隣接する2つのノードの次数が似ているほど高くなる。 次数の高いハブ的なノードが同じようにハブ的なノードと接続しがちな場合、assortative。 逆に、ハブに対して次数の低いノードが接続しがちな場合、disassortative。 ",
  "href": "/bio/complexnetwork.html",
  "tags": [
   "math"
  ],
  "title": "複雑ネットワーク",
  "type": "bio"
 },
 {
  "content": "日本生態学会 第60回大会(静岡) 自由集会\n概要 企画 嶋田正和、三浦徹 日時 2013年3月5日(火)15時30分—17時30分 場所 グランシップ静岡 20世紀に体系立てられた「進化の総合説」や「分子進化の中立説」は、遺伝子頻度とDNA配列情報に基づく一側面からの進化の理解でしかなく、生物の多様な生きる姿・形（適応形態）を理解する上では無力だった。これに対して、20世紀の終わりから発展し始めた生態と進化と発生を連結する新しい研究分野（エコ・エボ・デボ）は大きな流れとなり、21世紀の進化学の中心課題となりつつある。これに即応したかのように、新しいエコ・エボ・デボ（進化発生生物学）の本、Gilbert S. F. and Epel D.(2009) 邦訳『生態進化発生学：エコ-エボ-デボの夜明け』（正木・竹田・田中共訳、2012）が出版された。エピジェネティクスを解明する分子細胞生物学が、表現型可塑性の生態的・進化的・発生的側面の知見と連携して、新しい「適応した姿・形」を解き明かすだろう。今回は、発生プログラムの改変による適応形態にテーマを絞って、4名の若手研究者に講演をお願いした。\n演題 はじめに（趣旨説明）: 形態形成の変異と環境適応 : 嶋田正和（東大・総合文化） 運動と可塑性が導く発生過程の適応的改変 : 梶智就（北大・環境） 可塑的かつ堅牢な昆虫肢形成を可能にするメカニズム : 小嶋徹也（東大・新領域） 環境と発生システムの相互作用が制限または増進する進化可能性 : 岩嵜航 1 ・津田真樹 2 ・河田雅圭 1 （1 東北大・生命科学、2 理研・基幹研） アリの新奇カーストにみられる発生過程と環境応答 : 宮崎智史 1 ・三浦徹 2 ・前川清人 1 （1 富山大・理、2 北大・環境） 総合討論（司会：三浦徹） ",
  "href": "/esj60w.html",
  "tags": null,
  "title": "迅速な適応性(第5回)—発生プログラムの適応的な改変",
  "type": "page"
 },
 {
  "content": "第11回日本進化学会大会（札幌大会） ワークショップ W3C\n概要 企画 岩嵜航（東北大・院・生命） 企画提出 ver. 生物が新しい形態や機能を獲得するメカニズムの解明は、進化生物学における主要な課題のひとつである。 本企画では、表現型として現れないまま集団中・集団間に蓄積していく中立変異と、 化学反応の確率性に由来する細胞内のゆらぎに着目する。 新奇形質は突然変異によって直接的に現れるのではなく、 外部環境の変化や内部環境のゆらぎによって生じる表現型多型の一部として現れ、 突然変異と自然選択によってその表現型を生じさせやすい遺伝的基盤が 後から徐々に固定していくのではないだろうか？ 若手研究者を中心に話題提供を行い、このシナリオの可能性について議論したい。\n要旨集掲載 ver. 生物は新しい機能や形態をどのように獲得してきたのか？ 本企画では、表現型として現れないまま集団中・集団間に蓄積していく中立変異と、 化学反応の確率性に由来する細胞内のゆらぎに着目し、 それらを通した可塑的な適応が先に起こり後から遺伝的基盤が進化してくるという 新奇形質獲得のシナリオについて議論したい。\n日時と場所 平成21年9月4日(金) 13:30-15:00 C会場 北海道大学高等教育機能開発総合センター 演題と要旨 企画 岩嵜航（東北大・院・生命） 座長 松橋彩衣子（東北大・院・生命） コメンテータ 三浦徹（北大・地球環境） [W3C-1] 遺伝子制御ネットワークの隠れた変異がもたらす表現型可塑性の多型 岩嵜航、津田真樹、河田雅圭 （東北大・院・生命）\n転写制御などを介した遺伝子間の相互作用は複雑なネットワーク（遺伝子制御ネットワーク：GRN）を形成し、 細胞分化や環境応答など生物のさまざま機能を生み出している。 シス制御領域の突然変異などによるGRNの構造変化は、 時間的・空間的に新しい遺伝子発現パターンをもたらし、 不連続な表現型変化を可能にする。 その一方で、GRNは相互作用の変化に対して頑健であり、 表現型を維持したまま突然変異によって変化していくことができるという理論研究がある。 これらのことから、ある環境では表現型に現れないGRNの中立多型が集団中に維持されていること、 またそれが環境変化を受けることで顕在化し、 多様な表現型応答を生み出して新奇形質の登場に寄与することが示唆される。 突然変異が個体ごとにしか起こらないのに対して、 環境変化は一度に集団全体に影響を与えるため表現型探索の効率を高めると考えられる。 本研究では、GRNを組み込んだ個体ベースモデルによる進化シミュレーションを用いて、 GRNの多型に対する環境変化・遺伝的変化が集団の表現型変異に与える影響を評価した。\n[W3C-2] 栄養枯渇に対する適応応答における遺伝子発現の多様性の役割 津留三良 ^1^ 、インベイウェン ^1^ 、森光太郎 ^1^ 、潮田純弥 ^1^ 、柏木明子 ^2^ 、四方哲也 ^1^ （1 阪大　院情報科学、2 弘前大・農学生命科学）\n生物は多様な遺伝子発現プログラム（転写制御機構）を適切に駆使することによって、 対応する様々な外部環境変化に対して適応的な遺伝子発現パターンへ変化し、対応している。 このように、転写制御が環境に対する特異性・専門性を持つことで、 適応的な遺伝子発現が補償されているが、細胞内環境はエラープローンであるため、 変異や挿入によって本来の制御・被制御の関係が遺伝的に改変・再編されやすい。 このような再編を受けた遺伝子発現プログラムは、新規機能を獲得するチャンスがあるが、 その代償として、既に獲得していた環境に対する特異性を失うことになり、 何らかの補償効果がない限り、進化能の障壁となりうる。 本研究では、この補償効果の有無とそのシナリオを明らかにするため、 アミノ酸合成に関わる転写制御を人工的に改変した大腸菌を用い、 本来対応可能であったアミノ酸枯渇に対する応答（遺伝子発現、増殖速度）を調べた。 予想に反した補償効果として、本来の転写制御を用いない適応的な遺伝子発現パターンへの変化が観察され、 それを可能とする、遺伝子発現の確率的性質に基づくシナリオを紹介する。\n[W3C-3] 植物の野外集団における隠蔽変異 山口正樹、工藤洋 （京大・生態研）\n隠蔽変異とは、熱ショックなどのストレス環境等特殊な条件下でのみ表現型に現れる遺伝的変異である。 その性質から、隠蔽変異は通常の環境下では集団中に中立的に蓄積する。 隠蔽変異の蓄積機構は、適応度に影響しうる遺伝的変異を蓄積することを介して、 進化に貢献する可能性がある。 隠蔽変異の蓄積機構の一つとして知られているのが、 分子シャペロンの一種であるヒートショックプロテイン90（HSP90）の働きである。 HSP90の働きを阻害した場合に表現型に現れる遺伝的変異が増加することが報告されている。 また、熱ショックによってもHSP90を阻害した場合と同様の変異の表出が見られる。 このことから、HSP90は軽度な遺伝的変異を隠蔽する働きを持ち、 熱ショックが加わると、変異を隠蔽する働きが低下すると考えられる。 野外においてもタンパク質の変性を誘発する様々な環境要因の違いが 集団の隠蔽変異の蓄積量を決定している可能性がある。 シロイヌナズナ属のタチスズシロソウとミヤマハタザオは、 同種の亜種でありながら砂浜と高山という温度環境が大きく異なる場所に生育する。 この2亜種を材料とし、野外集団における隠蔽変異の蓄積量を調べた。\n打ち上げ たくさんのご参加ありがとうございました。\n連絡先\n",
  "href": "/sesj11w3c.html",
  "tags": null,
  "title": "進化学会(2009) WS \"新奇形質獲得への裏口：ランダムな前適応と遺伝的同化\"",
  "type": "page"
 },
 {
  "content": "https://sc.ddbj.nig.ac.jp/\n利用開始 https://sc.ddbj.nig.ac.jp/start_the_service\n利用規定等を熟読。 手元のコンピュータでSSH鍵ペアを生成しておく。既にある場合は作り直す必要なし。 公式ドキュメント に従ってRSA 3072にするのが無難だが、Ed25519やEDCSAを登録することも可能。 利用登録申請のフォームを埋める。 申請者 所属機関 アカウント: 作っておいたSSH公開鍵をここでコピペ。 責任者 申請者と責任者にメールが届くので、それに従って誓約書PDFを管理者に送信。 アカウント登録証が手元に届く。 手元の ~/.ssh/config に設定を追加: Host *.ddbj.nig.ac.jp RequestTTY yes User heavywatal IdentityFile ~/.ssh/id_ed25519 ユーザ名と鍵ファイル名は適宜調整。 これでsshコマンドを短く済ませられる。 ゲートウェイノードにSSH接続し、ログインノードに qlogin: ssh gw.ddbj.nig.ac.jp qlogin # which is equivalent to (thanks to ~/.ssh/config) ssh -t -f ~/.ssh/id_ed25519 heavywatal@gw.ddbj.nig.ac.jp qlogin ファイルの送受信 公式「システムへのファイル転送方法」 にはsftpかAsperaを使えと書かれてるけど、 rsync を使うのが簡単。\n# send rsync -auv ~/input/ gw.ddbj.nig.ac.jp:~/input/ # receive rsync -auv gw.ddbj.nig.ac.jp:~/output/ ~/output/ ソースコードは当然Gitで管理。\n環境整備 ハードウェア構成 ソフトウェア構成 (Phase 3: Red Hat Enterprise Linux 7.5) Singularity ジョブ投入、管理 https://sc.ddbj.nig.ac.jp/general_analysis_division/ga_introduction https://sc.ddbj.nig.ac.jp/software/univa_grid_engine Univa Grid Engine (UGE) http://gridengine.eu/grid-engine-documentation qsub 遺伝研ウェブサイトにはスクリプトを書いてから渡す方法しか書いてないが、 簡単なタスクならコマンド引数として直接渡すのもあり。\n## スクリプトを書いてから渡す qsub test.sh ## コマンドライン引数として直接渡す qsub -l short -b y -shell n -cwd -N test \u0026#34;pwd; sleep 5; ls \u0026gt;ls.txt\u0026#34; -l *** 実行時間や計算ノードなどの要求を伝える。 管理者が定義したキューから選んで指定する。 例えば、1時間以内に終わる軽いものなら -l short、 2か月かかるものなら -l epyc、 メモリが多めに必要なら -l medium、など。 qstat -g c でキューの一覧とそれぞれの負荷を確認できるので空いてるところを探す。 1コアあたりのRAM上限(デフォルト8GB)もこのオプションから -l s_vmem=16G -l mem_req=16G のように変更できる。 -pe def_slot 8 parallel environment: 並列処理で使用するCPUコア数を指定。 MPIによる並列化の場合はまた違うオプションがある。 -cwd カレントディレクトリでジョブ実行。 デフォルトでは ${HOME}。 -N *** ジョブに名前をつける。 デフォルトではスクリプト名が採用される。 -o ***, -e *** 標準出力・標準エラー出力の書き出し先。 デフォルトではワーキングディレクトリ以下に {JOBNAME}.o{JOBID}, {JOBNAME}.e{JOBID} という名前で書き出される(空っぽでさえ)。 不要な出力は /dev/null に流し込むべし。 -S /bin/sh インタープリタを指定。 指定しないと csh が利用されるせいか、 標準出力で Warning: no access to tty と怒られる。 -t 1-M M個のタスクを持つアレイジョブとして投入する。 タスクごとにパラメータを変えてプログラムを走らせたい場合は、 スクリプトの中で環境変数 SGE_TASK_ID を拾ってどうにかする。 qsub コマンドを生成して何回も呼ぶほうが圧倒的に楽だけど、 アレイジョブのほうがクラスタ側の負荷が小さいらしい。ほんとかな。 -tc MAX_RUNNING_TASKS アレイジョブで同時実行するタスク数の上限を指定。 システムからユーザーに与えられた上限は qquota コマンドで確認できる。 いまのところ500らしい。 -v VARIABLE=value 環境変数を定義してジョブに引き継ぐ。 大文字 -V で全ての環境変数が渡される。 -b y 計算ノードにバイナリがあるものとしてジョブを投げる。 これを指定しない場合はスクリプト扱いになり、 投入ノードから計算ノードへのコピーなど余計なプロセスが挟まるらしい。 -shell n 環境変数の解決など、 プログラムの呼び出しにシェルを介す必要がない場合は これを指定することで多少コスト削減できる。 当然 -b y のときのみ有効。 qsub スクリプト #$ で始まる行は qsub へのオプションと見なされる。\nジョブスクリプト内で参照可能な特殊環境変数をプリントしてみるジョブの例:\n#!/bin/sh #$ -S /bin/sh #$ -l short #$ -cwd #$ -t 1-2 echo HOME: $HOME echo USER: $USER echo JOB_ID: $JOB_ID echo JOB_NAME: $JOB_NAME echo HOSTNAME: $HOSTNAME echo SGE_TASK_ID: $SGE_TASK_ID echo SGE_TASK_FIRST:$SGE_TASK_FIRST echo SGE_TASK_LAST: $SGE_TASK_LAST echo SGE_TASK_STEPSIZE: $SGE_TASK_STEPSIZE pwd ls スクリプトはPythonでもいい。 インタープリタを -S /usr/bin/env python で指定できないのは残念。\n#!/usr/bin/env python #$ -S $HOME/.pyenv/shims/python #$ -l short #$ -cwd #$ -t 1-2 import os print(\u0026#34;SGE_TASK_ID: \u0026#34; + os.environ[\u0026#34;SGE_TASK_ID\u0026#34;]) 補助コマンド qstat 現在実行中のジョブ一覧。 ステータスは1文字に省略されて E(rror), r(unning), R(estarted), s(uspended) のように表示される。 詳しくは man qstat を参照。 qstat -g c クラスタで定義されているキューの一覧と、それぞれの負荷を表示 qstat -f | less 全ノードの状況をfullに表示 qstat -u '*' | less 全ユーザのジョブを表示。 -s p でpending中のみに絞ったり、 -l medium でキューの種類を絞ったりできる。 qstat -j JOBID ジョブの詳細表示 qacct -j JOBID ジョブ実行後にリソース消費を確認 qdel JOBID ジョブ削除 qquota ユーザーに与えられたリソースを表示 Apptainer (Singularity) https://sc.ddbj.nig.ac.jp/software/Apptainer/\n/usr/local/biotools/ 以下に各種ソフトウェアが用意されている。 BioContainers のものをほぼそのまま置いているらしい。\n利用可能なソフトウェアとバージョンを探す:\nfind /usr/local/biotools/ -name \u0026#39;blast*\u0026#39; | sort イメージとプログラム名を指定して実行:\nsingularity exec -e /usr/local/biotools/f/fastp:0.20.0--hdbcaa40_0 fastp --help そこらに落ちてるイメージを拾ってきて使うこともできる。 例えばTrinityの公式最新版を使いたい場合:\nfind /usr/local/biotools/ -name \u0026#39;trinity*\u0026#39; | sort mkdir -p ~/image wget -P ~/image/ https://data.broadinstitute.org/Trinity/TRINITY_SINGULARITY/trinityrnaseq.v2.11.0.simg singularity exec -e ~/image/trinityrnaseq.v2.11.0.simg Trinity --help R Homebrew R Homebrew on Linux における標準GCCがついに新しくなったので brew install r で使えるRが入るようになった。\nライブラリが古かったりして途中でエラーになることもあるので brew install --force-bottle gcc のような感じでやり過ごす。\nただし古い gcc@5 まわりでまだ問題が残っているようなので要注意。\nsingularity R find /usr/local/biotools/ -name \u0026#39;r-base:*\u0026#39; | sort 最新が3.5.1と古い上にエラーで起動しない:\nsingularity exec -e /usr/local/biotools/r/r-base:3.5.1 R WARNING: Skipping mount /opt/pkg/singularity/3.7.1/var/singularity/mnt/session/etc/resolv.conf [files]: /etc/resolv.conf doesn\u0026#39;t exist in container [1] 100279 segmentation fault singularity exec -e /usr/local/biotools/r/r-base:3.5.1 R module R https://sc.ddbj.nig.ac.jp/ja/guide/software/r\nmodule load r/3.5.2 そのものが古い上に、 古いコンパイラ(おそらく /usr/bin/gcc 4.8.5)でビルドされているため RcppでC++11までしか使えない。 また、各パッケージも同じく古いコンパイラでビルドしなければならない。 module load gcc などで新しいgcc/g++がPATH上に乗っていると、 Rcppインストール時などに /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found と怒られる。\nSource R /opt/pkg/r/*/lib64/R/etc/Makeconf を参考に新しいコンパイラで自前ビルドを試みる:\nwget -O- https://cran.r-project.org/src/base/R-3/R-3.5.2.tar.gz | tar xz cd R-3.5.2/ ./configure -h | less ./configure --prefix=${HOME}/R --disable-openmp --disable-java \u0026#39;--enable-R-shlib\u0026#39; \u0026#39;--enable-shared\u0026#39; \u0026#39;--with-tcl-config=/usr/lib64/tclConfig.sh\u0026#39; \u0026#39;--with-tk-config=/usr/lib64/tkConfig.sh\u0026#39; \u0026#39;PKG_CONFIG_PATH=/usr/local/lib64/pkgconfig:/usr/local/lib/pkgconfig:/usr/local/share/pkgconfig:/cm/local/apps/curl/lib/pkgconfig:/usr/lib64/pkgconfig:/usr/lib/pkgconfig:/usr/share/pkgconfig\u0026#39; \u0026#39;CFLAGS=-I/usr/local/include:/usr/include/X11:/cm/local/apps/curl/include\u0026#39; \u0026#39;CPPFLAGS=-I/usr/local/include:/usr/include/X11:/cm/local/apps/curl/include\u0026#39; \u0026#39;CXXFLAGS=-I/usr/local/include:/usr/include/X11:/cm/local/apps/curl/include\u0026#39; \u0026#39;FFLAGS=-I/usr/local/include:/usr/include/X11:/cm/local/apps/curl/include\u0026#39; \u0026#39;FCFLAGS=-I/usr/local/include:/usr/include/X11:/cm/local/apps/curl/include\u0026#39; \u0026#39;LDFLAGS=-L/usr/local/lib64 -L/usr/lib64 -L/usr/lib -L/cm/local/apps/curl/lib\u0026#39; make -j2 make install configure: error: libcurl \u0026gt;= 7.22.0 library and headers are required with support for https 古いcurlは入ってないように見えるのに、なぜ？\n",
  "href": "/bio/nig.html",
  "tags": [
   "job"
  ],
  "title": "遺伝研スパコン",
  "type": "bio"
 },
 {
  "content": "文献\nTompa et al. 2005 Nature Biotechnology Phylogenetic footprinting、発現パターン、ChIP結果などの補助的情報を使わず、 配列のみを使って解析する13のツールを fly, human, mouse, yeast, 人工データでテストした。 ただし、エキスパートがパラメータをファインチューニングし、 ベストヒットした1つのモチーフを取る、という条件での比較。 どのツールもyeastで特にハイスコア。flyはやや低め。 人工データよりもrealのスコアが低いのは、 今回正解としている以外のモチーフも配列に含まれてて そいつをトップヒットとしてしまったことにペナルティがかかってしまったせい。\nHu et al. 2005 BMC Bioinformatics あまりチューニングせずほぼデフォルト設定でベンチマーク。 そうすると、どのツールも単体では意外とショボい。 複数のツールを組み合わせて使うと良い。 GuhaThakurta 2006 Nucleic Acids Res モチーフ探索の基本的な流れがわかりやすい Das and Dai 2007 BMC Bioinformatics 実験はしていないが網羅的なレビュー Zambelli et al. 2013 Briefings in Bioinformatics ChIP時代のツールも含めてざっくり 基本 配列モチーフとは 一般的に だいたい 5–20bp くらいの生物学的に意味のある塩基配列パターン。 e.g. 転写因子結合サイト、タンパク質の機能ドメイン palindoromic motifs 相補鎖同士を5\u0026rsquo;から3\u0026rsquo;に読んで同じになってるような配列。 e.g. CACGTG, GCATGC spaced dyad (gapped) motifs 3–5bpくらいの保存的な結合サイトの間に非保存的なspacer/gapがある。 結合タンパクがdimerな場合とか。 e.g. Yeast Gal4: 5\u0026rsquo;-CGGnnnnnnnnnnnCCG-3' Shine-Dalgarno sequence 原核生物mRNAの開始コドン上流にあるリボソーム呼び込みモチーフ。 16SリボソームにアンチSD配列がある。 e.g. E. coli AGGAGGA Kozak sequence 真核生物版SD配列。 見つけ方 Signature\n共発現 coexpression/coregulation 高頻度 overrepresented 保存性 conserved -\u0026gt; phylogenetic footprinting したがって、戦略としては\n1ゲノムの中で共発現してる遺伝子のプロモーターを集めて高頻度な部分配列を探す 複数種のorthologousな遺伝子のプロモーターを集めて特に保存的な部分配列を探す 上記2つの合わせ技 表し方 Consensus 1つの文字列でバシッと表現。 正規表現の文字集合 [] を使ってポジション毎の複数候補を表現したり、 IUPAC命名法に従って degenerate symbol で表すこともある。 http://www.bioinformatics.org/sms/iupac.html e.g TATAAT, TATA[AT]A[AT], TATAWAW\nPSPM: Position-Specific Probability Matrix ポジションごとの塩基・アミノ酸の相対的な出現頻度をそのままの整数、 あるいは合計1になるような実数[0, 1]の行列で表示。 Profile とも呼ばれる。 (Bucher 1990) A C G T 0.625 0.125 0.250 0.000 0.375 0.375 0.000 0.250 0.125 0.375 0.500 0.000 0.250 0.375 0.250 0.125 0.000 0.250 0.250 0.500 0.375 0.375 0.125 0.125 0.250 0.125 0.375 0.250 0.250 0.125 0.250 0.375 PSSM: Position-Specific Scoring Matrix 頻度からいろんなスコアに変換して行列にする。 ツールによって異なるのでよく分からん。 PWM: Position Weight Matrix 一番良く見る呼び方だが、PSPMと同義だったり、PSSMと同義だったり。。。 Sequence Logo ポジションごとの保存性・確実さと各塩基・アミノ酸の寄与を アルファベットの大きさで視覚的に表現。 (Schneider and Stephens 1990) ポジション i における塩基・アミノ酸 a の高さは、 相対頻度 f と情報量 R の積:\n$\\text{Height}(a, i) = f(a, i) \\times R(i)$\nポジション i における情報量 (information content, IC) は、 定数ひく不確実性ひく補正項:\n$R(i) = \\log_2(s) - H(i) - e(n)$\nただし s = 4 [DNA] or 20 [Protein]\nポジション i における不確実性 (Shannon entropy):\n$H_i = - \\sum _{k}{f(k, i)} \\times \\log_2 f(k, i)$\n配列の数 n が少ない時のための補正項:\n$e(n) = \\frac{1}{\\ln 2} \\times \\frac{s - 1}{2n}$\nただし s = 4 [DNA] or 20 [Protein]\nhttp://schneider.ncifcrf.gov/paper/logopaper/\nConsensus Logo コンセンサス配列の重み付け表示版、あるいはSequence Logoのトップヒット限定版。 1行の文字列なので図を使わず書式付きリッチテキストとして扱える。 アルゴリズムの分類 一覧 Das and Dai 2007 表を改変\nMEME Suite MEME, DREME, MEME-ChIP, MAST MoD Tools Weeder, WeederH, Pscan, PscanChIP RSAT (Regulatory Sequence Analysis Tools) Oligo-Analysis, Dyad-Analysis, Consensus, peak-motifs; ローカル使用には作者へのメールが必要。 Brutlag Bioinformatics Group BioProspector, MDscan SCOPE BEAM, PRISM, SPACER を組み合わせる。 ローカル使用には作者へのメールが必要。 word-based (string-based) オリゴヌクレオチドの数え上げと頻度比較 = exhaustive enumeration\n全通りやるので global optimality にたどり着く\n短いモチーフ配列(=真核生物)向き\nOligo-Analysis, Dyad-Analysis\nWeeder\nMITRA,\nYMF\nPSM: probabilistic sequence model 長いモチーフ配列(=原核生物)向き\n探索の仕方によっては local optimum に陥りがち\nEM (expectation maximization) algorithm w 行のPWM $\\theta$ を適当に作る (位置 j が塩基 a である確率は $\\theta_{ja}$)\n配列が n 本あり、それらの部分配列 S が $\\theta$ から生成される確率(すなわち尤度)は\n\\[ L(\\theta) = P(S_1, ..., S_n|\\theta) = \\prod_i^n \\prod_j^w \\theta_{i, S_{i j}} \\] 部分配列がモチーフ $\\theta$ から生成された場合と、 そうじゃないただのバックグラウンド $\\theta_0$ から生成された場合を\n\\[\\begin{aligned} z_i \u0026= 1 [\\text{if} S_i \\text{is generated by} \\theta]\\\\ z_i \u0026= 0 [\\text{if} S_i \\text{is generated by} \\theta_0] \\end{aligned}\\] のように missing parameter として表すと、尤度は\n\\[ L(z, \\theta, \\theta^0) = \\prod_i^n [z_i P(S_i|\\theta) + (1 - z_i)P(S_i|\\theta^0)] \\] EMアルゴリズムでは以下の平均対数尤度を使う。\n\\[ \\log \\tilde{L}(z, \\theta, \\theta^0) = \\sum_i^n [q(z_i = 1) \\log P(S_i|\\theta) + q(z_i = 0) \\log P(S_i|\\theta^0)] \\] ただし $q(z_i)$ はラベル変数の事後分布で、\n\\[\\begin{aligned} q(z_i = 1) \u0026\\sim P(z_i = 1) P(S_i|\\theta)\\\\ q(z_i = 0) \u0026\\sim P(z_i = 0) P(S_i|\\theta_0) \\end{aligned}\\] E (Expectation) ステップ: $\\theta$ を使って $q(z_i)$ を更新\nM (Maximization) ステップ: $q(z_i)$ を使って、尤度最大となるよう $\\theta$ を更新\nEMステップを繰り返す\nMEME では各配列が持つモチーフの数が1以外の場合も扱えるのと、 最初のPWMの作り方を工夫して大域最適解に行きやすくなっているという点で ただのEMより改善されているらしい。 Gibbs sampling AlignACE (Linux実行形式のみ配布), MotifSampler N 本の各配列のモチーフ位置の初期値をランダムに与える ランダムに配列をひとつ選び、それ以外の配列のモチーフを並べてPWMを計算 先に選んだ配列の上でPWMをスライドさせつつスコア (l-merがPWMから生成される確率など) を計算 スコアに比例した確率で新しいモチーフ位置を決める 各配列のモチーフ位置が安定するまで2–4を繰り返す スコアには生の生成確率ではなく、 バックグラウンドスコア(塩基出現頻度から予測される出現頻度)との比を考慮した 相対エントロピーを用いることがある。\n\\[ \\sum_i^l \\sum_a^{ACGT} p_{ai} \\log_2 \\frac{p_{ai}}{b_a} \\] Ensemble 基本的にはどのアルゴリズムもsensitivity, accuracy共に低い。 既存のアルゴリズムを複数ensemble的に組み合わせるべし。\nEMD (Hu et al. 2006) 配列 N 本のデータセットに対し M 個の異なるアルゴリズムをそれぞれ R 回ずつ走らせて K 個の予測サイトを得る 配列毎・アルゴリズム毎に予測サイトを集めてグループ化する 配列毎に全アルゴリズムの予測スコアを足しあわせて voting, smoothing, extracting PF: Phylogenetic Footprinting 古くは Tagle et al. 1988 がこれを用いてグロブリンの調節領域を予測した。\nClustalW などによるアラインメントでは、 近すぎると情報量が無く、遠すぎるとメチャクチャ。\n複数種の配列で保存的な部分配列を探すだけなら 上記のような1 genome用のツールを使ってもある程度はいけるが、 そのまま使うのではなくて系統関係による重み付けが必要。\nBackgroundの取り方 統計的仮説検証のためにランダム生成DNA配列を使ってしまうと、 繰り返しが多く含まれていたりする現実のDNA配列と比べて \u0026ldquo;too null\u0026rdquo; になり、 検出力を overestimate することにつながる。 かといって、そういう配列を予めマスクしてしまっていいかというとそれも微妙 (Simcha et al. 2012)\nランダムではなくバックグラウンドと比べて多いかどうかを明示的に言いたい場合は discriminatory motif と呼んだりする。\nMarkov process 各塩基の頻度を考慮して配列をランダム生成するのが0階(単純)マルコフ過程。 1つ前の塩基を考慮して次の塩基の出方が影響を受ける (つまり塩基ペアの頻度情報をつかう)のが1階マルコフ過程。 て感じで配列を生成するマルコフ過程のオーダーを高くしていく。 Thijs et al. 2001 BioProspector, MotifSampler, YMF, MEME バックグラウンド配列セットにおけるモチーフ出現頻度を考慮 Co-Bind 2001, WordSpy 2005, ANN-Spec 2000 見つけた後の評価 STAMP, TOMTOM, MotIV\nツールの性能評価 ツールを作るのも難しいが、性能を正しく評価するのも難しい。\nツール開発論文の多くは「MEMEよりxx%も精度が向上した」などと報告しているが、 それはデータ・評価項目・コマンドオプションなどに依存している。\nテストデータ 現実の配列を使う 真の正解が分からない。 つまり、未知のモチーフを発見すると不当なペナルティを食らう。 人工的に生成した配列を使う 自然の配列ができあがる真の確率過程が分からないので、 特定のアルゴリズムに有利になったりするような偏りが生じるかも。 評価項目 Raw Score (塩基単位) 予測されたモチーフを正解モチーフと塩基単位で比較し、 正解 (True Positive, True Negative) と間違い (False Positive, False Negative) をカウントする。\nnTP, nTN, nFP, nFN: Raw Score (サイト単位) 予測されたモチーフが正解モチーフの1/4以上オーバーラップしてたら当たりとし、 モチーフ(サイト)単位で TP, FP, FN をカウントする。 (TN は数えられない)\nsTP, sFP, sFN Sensitivity (塩基 or サイト単位) 正解モチーフ配列のうち、どれくらい拾えたか\n$Sn = \\frac{TP}{TP + FN}$ Specificity (塩基 or サイト単位) 本当はモチーフじゃない配列に、どれくらいノーと言えたか\n$Sp = \\frac{TN}{TN + FP}$ Positive Predictive Value (塩基 or サイト単位) モチーフだと予測されたサイトに、どれくらい正解が含まれていたか (これをSpecificityと呼ぶことも多い)\n$PPV = \\frac{TP}{TP + FP}$ Performance Coefficient (塩基単位) 正解率 (Pevzner and Sze 2000)\n$nPC = \\frac{TP}{TP + FN + FP}$ Correlation Coefficient (塩基単位) 予測と正解のピアソン相関係数 (Burset, M. and Guigó 1996)\n$nCC = \\frac{TP \\times TN - FN \\times FP}{\\sqrt{(TP + FN)(TN + FP)(TP + FP)(TN + FN)}}$ Average Site Performance (塩基単位) 感度と精度の平均 (Burset, M. and Guigó 1996)\n$nASP = \\frac{Sn + PPV}{2}$ モチーフを含まないネガコンをデータに含めた場合は常に $TP = FN = 0$、 アルゴリズムがモチーフ無しと予測した場合は常に $TP = FP = 0$ となり、いろんな尺度が undefined/noninformative になってしまうのが悩みどころ\n評価の要約方法 ツールによってデータや尺度の得意・不得意があるので、 複数のデータセットでいろんな尺度を計算することになる。 それをどのように要約するか。\nAveraged 普通に算術平均 Normalized 全アルゴリズムの平均からの残差を標準偏差で割り データセットに対する平均を取る Combined それぞれのデータで尺度を計算してしまうのではなく、 7つの Raw Score をそれぞれ全データセットで足しあわせて、 それから各尺度の計算をする。 ゼロ除算を避けられる場合が多いので有用。 Database DNA TRANSFAC (Matys et al. 2003) 真核生物。 情報量は多いが1つのTFが複数のプロファイルを持ってる。 Professional版と機能限定Public版がある。 JASPAR (Sandelin et al. 2004) 真核生物。 情報量は少ないが1つのTFは1つのプロファイルだけ持ってる。 無制限。 SCPD: The Promoter Database of Saccharomyces cerevisiae\nDBTBS: Database of Transcriptional Regulation in Bacillus subtilis\nRegulonDB: Escherichia coli K-12 Transcriptional Regulatory Network Protein Pfam\nPROSITE\nProDom\nPRINTS\nReferences ",
  "href": "/bio/motif.html",
  "tags": [
   "genetics"
  ],
  "title": "配列モチーフ探索",
  "type": "bio"
 },
 {
  "content": "https://github.com/heavywatal/dotfiles\nLinux ユーザ追加 ユーザーを作って管理権限を与える:\n# adduser USERNAME # gpassed -a USERNAME sudo すぐ入れるパッケージ sudo apt install:\nbuild-essential zsh tmux git これらシステム標準のものが古すぎたり、 管理者権限がなくて自由にインストールできない場合は Homebrew を利用してユーザのホームに入れる。\nMac Command Line Tools コンパイラや make などはOSに付いてこないので別途インストールが必要。 https://developer.apple.com/downloads/ からダウンロードするか、 ターミナルから以下のコマンドを実行:\nxcode-select -p xcode-select --install インストールされているバージョンなどを確認するには:\npkgutil --pkg-info=com.apple.pkg.CLTools_Executables clang -v 総合開発環境 Xcode をインストールしたければ、App Store から Xcode を選択。\nパッケージ管理ツール /mac/homebrew その他のプログラム MenuMeters QuickLook plugins defaultsコマンドで各種設定 共通 Python install pip C++ boost SFMT R /rstats/config\nエディタ VSCode emacs vim Trash rm はゴミ箱を経由せず削除してしまうので、 間違って消してしまっても基本的には元に戻せない。 以下のような対策によりその危険が少しは減るかも。\nalias rmi='rm -i' ホントに消していいかどうか確認してくれるようなオプションつきのエイリアスを .zshrc に設定しておく。 rmi .DS_Store rm: remove regular file '.DS_Store'? エイリアス名を rm そのものにしてしまうと、 結局ろくすっぽ確認せず y を押す癖や、 いちいち確認されないように rm -rf する癖がつくので逆に危険。 普段は rmi を使う癖をつけ、必要なときたまに rm を使い、 -f はよほどのことが無い限り使わないようにする。\ntrash-cli Python製なので pip で pip install trash-cli して入れる。 ゴミ箱の場所は ~/.local/share/Trash macos-trash Macでも trash-cli を使えないことはないが、 ゴミ箱のパスがMac標準の ~/.Trash ではなく Linuxのものになってしまうので Homebrew で macos-trash を入れる: brew install macos-trash ",
  "href": "/dev/devenv.html",
  "tags": [
   "package"
  ],
  "title": "開発環境",
  "type": "dev"
 }
]
